diff --git a/code/environment.yml b/code/environment.yml
index 31a3487..47a996c 100644
--- a/code/environment.yml
+++ b/code/environment.yml
@@ -8,9 +8,9 @@ dependencies:
 - patchelf
 - pip:
     - -f https://download.pytorch.org/whl/torch_stable.html
-    - numpy
+    - numpy==1.23.0
     - gym==0.18.0
-    - mujoco-py==2.0.2.13
+    - mujoco-py
     - matplotlib==3.3.4
     - torch==1.9.1+cu111
     - typed-argument-parserdiff --git a/code/config/locomotion_config.py b/code/config/locomotion_config.py
index 90173b3..b79b7e6 100644
--- a/code/config/locomotion_config.py
+++ b/code/config/locomotion_config.py
@@ -6,7 +6,7 @@ class Config(ParamsProto):
     # misc
     seed = 100
     device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
-    bucket = '/home/aajay/weights/'
+    bucket = '/home/yanyangji/research/RL/final/decision-diffuser/code/output'
     dataset = 'hopper-medium-expert-v2'
 
     ## model
diff --git a/code/environment.yml b/code/environment.yml
index 31a3487..47a996c 100644
--- a/code/environment.yml
+++ b/code/environment.yml
@@ -8,9 +8,9 @@ dependencies:
 - patchelf
 - pip:
     - -f https://download.pytorch.org/whl/torch_stable.html
-    - numpy
+    - numpy==1.23.0
     - gym==0.18.0
-    - mujoco-py==2.0.2.13
+    - mujoco-py
     - matplotlib==3.3.4
     - torch==1.9.1+cu111
     - typed-argument-parserdiff --git a/code/config/locomotion_config.py b/code/config/locomotion_config.py
index 90173b3..b79b7e6 100644
--- a/code/config/locomotion_config.py
+++ b/code/config/locomotion_config.py
@@ -6,7 +6,7 @@ class Config(ParamsProto):
     # misc
     seed = 100
     device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
-    bucket = '/home/aajay/weights/'
+    bucket = '/home/yanyangji/research/RL/final/decision-diffuser/code/output'
     dataset = 'hopper-medium-expert-v2'
 
     ## model
diff --git a/code/environment.yml b/code/environment.yml
index 31a3487..47a996c 100644
--- a/code/environment.yml
+++ b/code/environment.yml
@@ -8,9 +8,9 @@ dependencies:
 - patchelf
 - pip:
     - -f https://download.pytorch.org/whl/torch_stable.html
-    - numpy
+    - numpy==1.23.0
     - gym==0.18.0
-    - mujoco-py==2.0.2.13
+    - mujoco-py
     - matplotlib==3.3.4
     - torch==1.9.1+cu111
     - typed-argument-parserdiff --git a/code/config/locomotion_config.py b/code/config/locomotion_config.py
index 90173b3..b79b7e6 100644
--- a/code/config/locomotion_config.py
+++ b/code/config/locomotion_config.py
@@ -6,7 +6,7 @@ class Config(ParamsProto):
     # misc
     seed = 100
     device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
-    bucket = '/home/aajay/weights/'
+    bucket = '/home/yanyangji/research/RL/final/decision-diffuser/code/output'
     dataset = 'hopper-medium-expert-v2'
 
     ## model
diff --git a/code/environment.yml b/code/environment.yml
index 31a3487..47a996c 100644
--- a/code/environment.yml
+++ b/code/environment.yml
@@ -8,9 +8,9 @@ dependencies:
 - patchelf
 - pip:
     - -f https://download.pytorch.org/whl/torch_stable.html
-    - numpy
+    - numpy==1.23.0
     - gym==0.18.0
-    - mujoco-py==2.0.2.13
+    - mujoco-py
     - matplotlib==3.3.4
     - torch==1.9.1+cu111
     - typed-argument-parserdiff --git a/code/config/locomotion_config.py b/code/config/locomotion_config.py
index 90173b3..b79b7e6 100644
--- a/code/config/locomotion_config.py
+++ b/code/config/locomotion_config.py
@@ -6,7 +6,7 @@ class Config(ParamsProto):
     # misc
     seed = 100
     device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
-    bucket = '/home/aajay/weights/'
+    bucket = '/home/yanyangji/research/RL/final/decision-diffuser/code/output'
     dataset = 'hopper-medium-expert-v2'
 
     ## model
diff --git a/code/environment.yml b/code/environment.yml
index 31a3487..47a996c 100644
--- a/code/environment.yml
+++ b/code/environment.yml
@@ -8,9 +8,9 @@ dependencies:
 - patchelf
 - pip:
     - -f https://download.pytorch.org/whl/torch_stable.html
-    - numpy
+    - numpy==1.23.0
     - gym==0.18.0
-    - mujoco-py==2.0.2.13
+    - mujoco-py
     - matplotlib==3.3.4
     - torch==1.9.1+cu111
     - typed-argument-parserdiff --git a/code/config/locomotion_config.py b/code/config/locomotion_config.py
index 90173b3..b79b7e6 100644
--- a/code/config/locomotion_config.py
+++ b/code/config/locomotion_config.py
@@ -6,7 +6,7 @@ class Config(ParamsProto):
     # misc
     seed = 100
     device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
-    bucket = '/home/aajay/weights/'
+    bucket = '/home/yanyangji/research/RL/final/decision-diffuser/code/output'
     dataset = 'hopper-medium-expert-v2'
 
     ## model
diff --git a/code/environment.yml b/code/environment.yml
index 31a3487..47a996c 100644
--- a/code/environment.yml
+++ b/code/environment.yml
@@ -8,9 +8,9 @@ dependencies:
 - patchelf
 - pip:
     - -f https://download.pytorch.org/whl/torch_stable.html
-    - numpy
+    - numpy==1.23.0
     - gym==0.18.0
-    - mujoco-py==2.0.2.13
+    - mujoco-py
     - matplotlib==3.3.4
     - torch==1.9.1+cu111
     - typed-argument-parserdiff --git a/code/config/locomotion_config.py b/code/config/locomotion_config.py
index 90173b3..b79b7e6 100644
--- a/code/config/locomotion_config.py
+++ b/code/config/locomotion_config.py
@@ -6,7 +6,7 @@ class Config(ParamsProto):
     # misc
     seed = 100
     device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
-    bucket = '/home/aajay/weights/'
+    bucket = '/home/yanyangji/research/RL/final/decision-diffuser/code/output'
     dataset = 'hopper-medium-expert-v2'
 
     ## model
diff --git a/code/diffuser/models/diffusion.py b/code/diffuser/models/diffusion.py
index a1f74ea..f6d87b3 100644
--- a/code/diffuser/models/diffusion.py
+++ b/code/diffuser/models/diffusion.py
@@ -301,6 +301,10 @@ class GaussianInvDynDiffusion(nn.Module):
         self.model = model
         self.ar_inv = ar_inv
         self.train_only_inv = train_only_inv
+
+        print(self.observation_dim, self.action_dim)
+        breakpoint()
+
         if self.ar_inv:
             self.inv_model = ARInvModel(hidden_dim=hidden_dim, observation_dim=observation_dim, action_dim=action_dim)
         else:
diff --git a/code/environment.yml b/code/environment.yml
index 31a3487..47a996c 100644
--- a/code/environment.yml
+++ b/code/environment.yml
@@ -8,9 +8,9 @@ dependencies:
 - patchelf
 - pip:
     - -f https://download.pytorch.org/whl/torch_stable.html
-    - numpy
+    - numpy==1.23.0
     - gym==0.18.0
-    - mujoco-py==2.0.2.13
+    - mujoco-py
     - matplotlib==3.3.4
     - torch==1.9.1+cu111
     - typed-argument-parserdiff --git a/code/config/locomotion_config.py b/code/config/locomotion_config.py
index 90173b3..64a8820 100644
--- a/code/config/locomotion_config.py
+++ b/code/config/locomotion_config.py
@@ -6,11 +6,12 @@ class Config(ParamsProto):
     # misc
     seed = 100
     device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
-    bucket = '/home/aajay/weights/'
+    bucket = '/home/yanyangji/research/RL/final/decision-diffuser/code/output'
     dataset = 'hopper-medium-expert-v2'
 
     ## model
-    model = 'models.TemporalUnet'
+    # model = 'models.TemporalUnet'
+    model = 'models.MLPnet'
     diffusion = 'models.GaussianInvDynDiffusion'
     horizon = 100
     n_diffusion_steps = 200
diff --git a/code/diffuser/models/diffusion.py b/code/diffuser/models/diffusion.py
index a1f74ea..ebfca59 100644
--- a/code/diffuser/models/diffusion.py
+++ b/code/diffuser/models/diffusion.py
@@ -295,12 +295,13 @@ class GaussianInvDynDiffusion(nn.Module):
         condition_guidance_w=0.1, ar_inv=False, train_only_inv=False):
         super().__init__()
         self.horizon = horizon
-        self.observation_dim = observation_dim
-        self.action_dim = action_dim
+        self.observation_dim = observation_dim # 11
+        self.action_dim = action_dim # 3
         self.transition_dim = observation_dim + action_dim
         self.model = model
         self.ar_inv = ar_inv
         self.train_only_inv = train_only_inv
+
         if self.ar_inv:
             self.inv_model = ARInvModel(hidden_dim=hidden_dim, observation_dim=observation_dim, action_dim=action_dim)
         else:
diff --git a/code/environment.yml b/code/environment.yml
index 31a3487..47a996c 100644
--- a/code/environment.yml
+++ b/code/environment.yml
@@ -8,9 +8,9 @@ dependencies:
 - patchelf
 - pip:
     - -f https://download.pytorch.org/whl/torch_stable.html
-    - numpy
+    - numpy==1.23.0
     - gym==0.18.0
-    - mujoco-py==2.0.2.13
+    - mujoco-py
     - matplotlib==3.3.4
     - torch==1.9.1+cu111
     - typed-argument-parserdiff --git a/code/config/locomotion_config.py b/code/config/locomotion_config.py
index 90173b3..64a8820 100644
--- a/code/config/locomotion_config.py
+++ b/code/config/locomotion_config.py
@@ -6,11 +6,12 @@ class Config(ParamsProto):
     # misc
     seed = 100
     device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
-    bucket = '/home/aajay/weights/'
+    bucket = '/home/yanyangji/research/RL/final/decision-diffuser/code/output'
     dataset = 'hopper-medium-expert-v2'
 
     ## model
-    model = 'models.TemporalUnet'
+    # model = 'models.TemporalUnet'
+    model = 'models.MLPnet'
     diffusion = 'models.GaussianInvDynDiffusion'
     horizon = 100
     n_diffusion_steps = 200
diff --git a/code/diffuser/models/diffusion.py b/code/diffuser/models/diffusion.py
index a1f74ea..a91c2b0 100644
--- a/code/diffuser/models/diffusion.py
+++ b/code/diffuser/models/diffusion.py
@@ -3,6 +3,7 @@ import torch
 from torch import nn
 import torch.nn.functional as F
 import pdb
+import clip
 
 import diffuser.utils as utils
 from .helpers import (
@@ -295,12 +296,22 @@ class GaussianInvDynDiffusion(nn.Module):
         condition_guidance_w=0.1, ar_inv=False, train_only_inv=False):
         super().__init__()
         self.horizon = horizon
-        self.observation_dim = observation_dim
-        self.action_dim = action_dim
+        self.observation_dim = observation_dim # 11
+        self.action_dim = action_dim # 3
         self.transition_dim = observation_dim + action_dim
         self.model = model
         self.ar_inv = ar_inv
         self.train_only_inv = train_only_inv
+
+        # Text embedding
+        device = self.model.device
+        self.clip_model, _ = clip.load("ViT-B/32", device=device)
+        for p in self.clip_model.parameters():
+            p.requires_grad = False
+        with torch.no_grad():
+            self.text_cond = self.clip_model.encode_text(clip.tokenize('the man is hopping').to(device))
+        self.proj_layer = nn.Linear(512, self.observation_dim)
+
         if self.ar_inv:
             self.inv_model = ARInvModel(hidden_dim=hidden_dim, observation_dim=observation_dim, action_dim=action_dim)
         else:
@@ -538,7 +549,9 @@ class GaussianInvDynDiffusion(nn.Module):
         return loss, info
 
     def forward(self, cond, *args, **kwargs):
-        return self.conditional_sample(cond=cond, *args, **kwargs)
+        self.forward_feats = self.proj_layer(self.text_cond)
+        bs = len(cond)
+        return self.conditional_sample(cond=self.forward_feats.expand(bs, -1), *args, **kwargs)
 
 
 class ARInvModel(nn.Module):
diff --git a/code/environment.yml b/code/environment.yml
index 31a3487..47a996c 100644
--- a/code/environment.yml
+++ b/code/environment.yml
@@ -8,9 +8,9 @@ dependencies:
 - patchelf
 - pip:
     - -f https://download.pytorch.org/whl/torch_stable.html
-    - numpy
+    - numpy==1.23.0
     - gym==0.18.0
-    - mujoco-py==2.0.2.13
+    - mujoco-py
     - matplotlib==3.3.4
     - torch==1.9.1+cu111
     - typed-argument-parserdiff --git a/code/config/locomotion_config.py b/code/config/locomotion_config.py
index 90173b3..64a8820 100644
--- a/code/config/locomotion_config.py
+++ b/code/config/locomotion_config.py
@@ -6,11 +6,12 @@ class Config(ParamsProto):
     # misc
     seed = 100
     device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
-    bucket = '/home/aajay/weights/'
+    bucket = '/home/yanyangji/research/RL/final/decision-diffuser/code/output'
     dataset = 'hopper-medium-expert-v2'
 
     ## model
-    model = 'models.TemporalUnet'
+    # model = 'models.TemporalUnet'
+    model = 'models.MLPnet'
     diffusion = 'models.GaussianInvDynDiffusion'
     horizon = 100
     n_diffusion_steps = 200
diff --git a/code/diffuser/models/diffusion.py b/code/diffuser/models/diffusion.py
index a1f74ea..bcaa9a4 100644
--- a/code/diffuser/models/diffusion.py
+++ b/code/diffuser/models/diffusion.py
@@ -3,6 +3,7 @@ import torch
 from torch import nn
 import torch.nn.functional as F
 import pdb
+import clip
 
 import diffuser.utils as utils
 from .helpers import (
@@ -295,12 +296,21 @@ class GaussianInvDynDiffusion(nn.Module):
         condition_guidance_w=0.1, ar_inv=False, train_only_inv=False):
         super().__init__()
         self.horizon = horizon
-        self.observation_dim = observation_dim
-        self.action_dim = action_dim
+        self.observation_dim = observation_dim # 11
+        self.action_dim = action_dim # 3
         self.transition_dim = observation_dim + action_dim
         self.model = model
         self.ar_inv = ar_inv
         self.train_only_inv = train_only_inv
+
+        # Text embedding
+        self.clip_model, _ = clip.load("ViT-B/32")
+        for p in self.clip_model.parameters():
+            p.requires_grad = False
+        with torch.no_grad():
+            self.text_cond = self.clip_model.encode_text(clip.tokenize('the man is hopping'))
+        self.proj_layer = nn.Linear(512, self.observation_dim)
+
         if self.ar_inv:
             self.inv_model = ARInvModel(hidden_dim=hidden_dim, observation_dim=observation_dim, action_dim=action_dim)
         else:
@@ -538,7 +548,9 @@ class GaussianInvDynDiffusion(nn.Module):
         return loss, info
 
     def forward(self, cond, *args, **kwargs):
-        return self.conditional_sample(cond=cond, *args, **kwargs)
+        forward_feats = self.proj_layer(self.text_cond.to(cond.device))
+        bs = len(cond)
+        return self.conditional_sample(cond=forward_feats.expand(bs, -1).to(cond.device), *args, **kwargs)
 
 
 class ARInvModel(nn.Module):
diff --git a/code/environment.yml b/code/environment.yml
index 31a3487..47a996c 100644
--- a/code/environment.yml
+++ b/code/environment.yml
@@ -8,9 +8,9 @@ dependencies:
 - patchelf
 - pip:
     - -f https://download.pytorch.org/whl/torch_stable.html
-    - numpy
+    - numpy==1.23.0
     - gym==0.18.0
-    - mujoco-py==2.0.2.13
+    - mujoco-py
     - matplotlib==3.3.4
     - torch==1.9.1+cu111
     - typed-argument-parserdiff --git a/code/config/locomotion_config.py b/code/config/locomotion_config.py
index 90173b3..64a8820 100644
--- a/code/config/locomotion_config.py
+++ b/code/config/locomotion_config.py
@@ -6,11 +6,12 @@ class Config(ParamsProto):
     # misc
     seed = 100
     device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
-    bucket = '/home/aajay/weights/'
+    bucket = '/home/yanyangji/research/RL/final/decision-diffuser/code/output'
     dataset = 'hopper-medium-expert-v2'
 
     ## model
-    model = 'models.TemporalUnet'
+    # model = 'models.TemporalUnet'
+    model = 'models.MLPnet'
     diffusion = 'models.GaussianInvDynDiffusion'
     horizon = 100
     n_diffusion_steps = 200
diff --git a/code/diffuser/models/diffusion.py b/code/diffuser/models/diffusion.py
index a1f74ea..2b34ff6 100644
--- a/code/diffuser/models/diffusion.py
+++ b/code/diffuser/models/diffusion.py
@@ -3,6 +3,8 @@ import torch
 from torch import nn
 import torch.nn.functional as F
 import pdb
+import clip
+from config.locomotion_config import Config
 
 import diffuser.utils as utils
 from .helpers import (
@@ -295,12 +297,22 @@ class GaussianInvDynDiffusion(nn.Module):
         condition_guidance_w=0.1, ar_inv=False, train_only_inv=False):
         super().__init__()
         self.horizon = horizon
-        self.observation_dim = observation_dim
-        self.action_dim = action_dim
+        self.observation_dim = observation_dim # 11
+        self.action_dim = action_dim # 3
         self.transition_dim = observation_dim + action_dim
         self.model = model
         self.ar_inv = ar_inv
         self.train_only_inv = train_only_inv
+
+        # Text embedding
+        device = Config.device
+        self.clip_model, _ = clip.load("ViT-B/32", device=device)
+        for p in self.clip_model.parameters():
+            p.requires_grad = False
+        with torch.no_grad():
+            self.text_cond = self.clip_model.encode_text(clip.tokenize('the man is hopping').to(device))
+        self.proj_layer = nn.Linear(512, self.observation_dim)
+
         if self.ar_inv:
             self.inv_model = ARInvModel(hidden_dim=hidden_dim, observation_dim=observation_dim, action_dim=action_dim)
         else:
@@ -538,7 +550,9 @@ class GaussianInvDynDiffusion(nn.Module):
         return loss, info
 
     def forward(self, cond, *args, **kwargs):
-        return self.conditional_sample(cond=cond, *args, **kwargs)
+        bs = len(cond)
+        forward_feats = self.proj_layer(self.text_cond.expand(bs, -1))
+        return self.conditional_sample(cond=forward_feats, *args, **kwargs)
 
 
 class ARInvModel(nn.Module):
diff --git a/code/environment.yml b/code/environment.yml
index 31a3487..47a996c 100644
--- a/code/environment.yml
+++ b/code/environment.yml
@@ -8,9 +8,9 @@ dependencies:
 - patchelf
 - pip:
     - -f https://download.pytorch.org/whl/torch_stable.html
-    - numpy
+    - numpy==1.23.0
     - gym==0.18.0
-    - mujoco-py==2.0.2.13
+    - mujoco-py
     - matplotlib==3.3.4
     - torch==1.9.1+cu111
     - typed-argument-parserdiff --git a/code/config/locomotion_config.py b/code/config/locomotion_config.py
index 90173b3..64a8820 100644
--- a/code/config/locomotion_config.py
+++ b/code/config/locomotion_config.py
@@ -6,11 +6,12 @@ class Config(ParamsProto):
     # misc
     seed = 100
     device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
-    bucket = '/home/aajay/weights/'
+    bucket = '/home/yanyangji/research/RL/final/decision-diffuser/code/output'
     dataset = 'hopper-medium-expert-v2'
 
     ## model
-    model = 'models.TemporalUnet'
+    # model = 'models.TemporalUnet'
+    model = 'models.MLPnet'
     diffusion = 'models.GaussianInvDynDiffusion'
     horizon = 100
     n_diffusion_steps = 200
diff --git a/code/diffuser/models/diffusion.py b/code/diffuser/models/diffusion.py
index a1f74ea..f3b6b94 100644
--- a/code/diffuser/models/diffusion.py
+++ b/code/diffuser/models/diffusion.py
@@ -3,6 +3,8 @@ import torch
 from torch import nn
 import torch.nn.functional as F
 import pdb
+import clip
+from config.locomotion_config import Config
 
 import diffuser.utils as utils
 from .helpers import (
@@ -295,12 +297,22 @@ class GaussianInvDynDiffusion(nn.Module):
         condition_guidance_w=0.1, ar_inv=False, train_only_inv=False):
         super().__init__()
         self.horizon = horizon
-        self.observation_dim = observation_dim
-        self.action_dim = action_dim
+        self.observation_dim = observation_dim # 11
+        self.action_dim = action_dim # 3
         self.transition_dim = observation_dim + action_dim
         self.model = model
         self.ar_inv = ar_inv
         self.train_only_inv = train_only_inv
+
+        # Text embedding
+        device = Config.device
+        self.clip_model, _ = clip.load("ViT-B/32", device=device)
+        for p in self.clip_model.parameters():
+            p.requires_grad = False
+        with torch.no_grad():
+            self.text_cond = self.clip_model.encode_text(clip.tokenize('the man is hopping').to(device))
+        self.proj_layer = nn.Linear(512, self.observation_dim)
+
         if self.ar_inv:
             self.inv_model = ARInvModel(hidden_dim=hidden_dim, observation_dim=observation_dim, action_dim=action_dim)
         else:
@@ -538,6 +550,9 @@ class GaussianInvDynDiffusion(nn.Module):
         return loss, info
 
     def forward(self, cond, *args, **kwargs):
+        # bs = len(cond)
+        # forward_feats = self.proj_layer(self.text_cond.expand(bs, -1))
+        # return self.conditional_sample(cond=forward_feats, *args, **kwargs)
         return self.conditional_sample(cond=cond, *args, **kwargs)
 
 
diff --git a/code/environment.yml b/code/environment.yml
index 31a3487..47a996c 100644
--- a/code/environment.yml
+++ b/code/environment.yml
@@ -8,9 +8,9 @@ dependencies:
 - patchelf
 - pip:
     - -f https://download.pytorch.org/whl/torch_stable.html
-    - numpy
+    - numpy==1.23.0
     - gym==0.18.0
-    - mujoco-py==2.0.2.13
+    - mujoco-py
     - matplotlib==3.3.4
     - torch==1.9.1+cu111
     - typed-argument-parserdiff --git a/code/config/locomotion_config.py b/code/config/locomotion_config.py
index 90173b3..64a8820 100644
--- a/code/config/locomotion_config.py
+++ b/code/config/locomotion_config.py
@@ -6,11 +6,12 @@ class Config(ParamsProto):
     # misc
     seed = 100
     device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
-    bucket = '/home/aajay/weights/'
+    bucket = '/home/yanyangji/research/RL/final/decision-diffuser/code/output'
     dataset = 'hopper-medium-expert-v2'
 
     ## model
-    model = 'models.TemporalUnet'
+    # model = 'models.TemporalUnet'
+    model = 'models.MLPnet'
     diffusion = 'models.GaussianInvDynDiffusion'
     horizon = 100
     n_diffusion_steps = 200
diff --git a/code/diffuser/models/diffusion.py b/code/diffuser/models/diffusion.py
index a1f74ea..f3b6b94 100644
--- a/code/diffuser/models/diffusion.py
+++ b/code/diffuser/models/diffusion.py
@@ -3,6 +3,8 @@ import torch
 from torch import nn
 import torch.nn.functional as F
 import pdb
+import clip
+from config.locomotion_config import Config
 
 import diffuser.utils as utils
 from .helpers import (
@@ -295,12 +297,22 @@ class GaussianInvDynDiffusion(nn.Module):
         condition_guidance_w=0.1, ar_inv=False, train_only_inv=False):
         super().__init__()
         self.horizon = horizon
-        self.observation_dim = observation_dim
-        self.action_dim = action_dim
+        self.observation_dim = observation_dim # 11
+        self.action_dim = action_dim # 3
         self.transition_dim = observation_dim + action_dim
         self.model = model
         self.ar_inv = ar_inv
         self.train_only_inv = train_only_inv
+
+        # Text embedding
+        device = Config.device
+        self.clip_model, _ = clip.load("ViT-B/32", device=device)
+        for p in self.clip_model.parameters():
+            p.requires_grad = False
+        with torch.no_grad():
+            self.text_cond = self.clip_model.encode_text(clip.tokenize('the man is hopping').to(device))
+        self.proj_layer = nn.Linear(512, self.observation_dim)
+
         if self.ar_inv:
             self.inv_model = ARInvModel(hidden_dim=hidden_dim, observation_dim=observation_dim, action_dim=action_dim)
         else:
@@ -538,6 +550,9 @@ class GaussianInvDynDiffusion(nn.Module):
         return loss, info
 
     def forward(self, cond, *args, **kwargs):
+        # bs = len(cond)
+        # forward_feats = self.proj_layer(self.text_cond.expand(bs, -1))
+        # return self.conditional_sample(cond=forward_feats, *args, **kwargs)
         return self.conditional_sample(cond=cond, *args, **kwargs)
 
 
diff --git a/code/diffuser/models/temporal.py b/code/diffuser/models/temporal.py
index 63155c2..f461016 100644
--- a/code/diffuser/models/temporal.py
+++ b/code/diffuser/models/temporal.py
@@ -354,6 +354,7 @@ class MLPnet(nn.Module):
             returns : [batch x 1]
         '''
         # Assumes horizon = 1
+        print(cond.shape)
         t = self.time_mlp(time)
 
         if self.returns_condition:
diff --git a/code/environment.yml b/code/environment.yml
index 31a3487..47a996c 100644
--- a/code/environment.yml
+++ b/code/environment.yml
@@ -8,9 +8,9 @@ dependencies:
 - patchelf
 - pip:
     - -f https://download.pytorch.org/whl/torch_stable.html
-    - numpy
+    - numpy==1.23.0
     - gym==0.18.0
-    - mujoco-py==2.0.2.13
+    - mujoco-py
     - matplotlib==3.3.4
     - torch==1.9.1+cu111
     - typed-argument-parserdiff --git a/code/config/locomotion_config.py b/code/config/locomotion_config.py
index 90173b3..64a8820 100644
--- a/code/config/locomotion_config.py
+++ b/code/config/locomotion_config.py
@@ -6,11 +6,12 @@ class Config(ParamsProto):
     # misc
     seed = 100
     device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
-    bucket = '/home/aajay/weights/'
+    bucket = '/home/yanyangji/research/RL/final/decision-diffuser/code/output'
     dataset = 'hopper-medium-expert-v2'
 
     ## model
-    model = 'models.TemporalUnet'
+    # model = 'models.TemporalUnet'
+    model = 'models.MLPnet'
     diffusion = 'models.GaussianInvDynDiffusion'
     horizon = 100
     n_diffusion_steps = 200
diff --git a/code/diffuser/models/diffusion.py b/code/diffuser/models/diffusion.py
index a1f74ea..f3b6b94 100644
--- a/code/diffuser/models/diffusion.py
+++ b/code/diffuser/models/diffusion.py
@@ -3,6 +3,8 @@ import torch
 from torch import nn
 import torch.nn.functional as F
 import pdb
+import clip
+from config.locomotion_config import Config
 
 import diffuser.utils as utils
 from .helpers import (
@@ -295,12 +297,22 @@ class GaussianInvDynDiffusion(nn.Module):
         condition_guidance_w=0.1, ar_inv=False, train_only_inv=False):
         super().__init__()
         self.horizon = horizon
-        self.observation_dim = observation_dim
-        self.action_dim = action_dim
+        self.observation_dim = observation_dim # 11
+        self.action_dim = action_dim # 3
         self.transition_dim = observation_dim + action_dim
         self.model = model
         self.ar_inv = ar_inv
         self.train_only_inv = train_only_inv
+
+        # Text embedding
+        device = Config.device
+        self.clip_model, _ = clip.load("ViT-B/32", device=device)
+        for p in self.clip_model.parameters():
+            p.requires_grad = False
+        with torch.no_grad():
+            self.text_cond = self.clip_model.encode_text(clip.tokenize('the man is hopping').to(device))
+        self.proj_layer = nn.Linear(512, self.observation_dim)
+
         if self.ar_inv:
             self.inv_model = ARInvModel(hidden_dim=hidden_dim, observation_dim=observation_dim, action_dim=action_dim)
         else:
@@ -538,6 +550,9 @@ class GaussianInvDynDiffusion(nn.Module):
         return loss, info
 
     def forward(self, cond, *args, **kwargs):
+        # bs = len(cond)
+        # forward_feats = self.proj_layer(self.text_cond.expand(bs, -1))
+        # return self.conditional_sample(cond=forward_feats, *args, **kwargs)
         return self.conditional_sample(cond=cond, *args, **kwargs)
 
 
diff --git a/code/diffuser/models/temporal.py b/code/diffuser/models/temporal.py
index 63155c2..e827e49 100644
--- a/code/diffuser/models/temporal.py
+++ b/code/diffuser/models/temporal.py
@@ -354,6 +354,8 @@ class MLPnet(nn.Module):
             returns : [batch x 1]
         '''
         # Assumes horizon = 1
+        print(cond)
+        breakpoint()
         t = self.time_mlp(time)
 
         if self.returns_condition:
diff --git a/code/environment.yml b/code/environment.yml
index 31a3487..47a996c 100644
--- a/code/environment.yml
+++ b/code/environment.yml
@@ -8,9 +8,9 @@ dependencies:
 - patchelf
 - pip:
     - -f https://download.pytorch.org/whl/torch_stable.html
-    - numpy
+    - numpy==1.23.0
     - gym==0.18.0
-    - mujoco-py==2.0.2.13
+    - mujoco-py
     - matplotlib==3.3.4
     - torch==1.9.1+cu111
     - typed-argument-parserdiff --git a/code/config/locomotion_config.py b/code/config/locomotion_config.py
index 90173b3..64a8820 100644
--- a/code/config/locomotion_config.py
+++ b/code/config/locomotion_config.py
@@ -6,11 +6,12 @@ class Config(ParamsProto):
     # misc
     seed = 100
     device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
-    bucket = '/home/aajay/weights/'
+    bucket = '/home/yanyangji/research/RL/final/decision-diffuser/code/output'
     dataset = 'hopper-medium-expert-v2'
 
     ## model
-    model = 'models.TemporalUnet'
+    # model = 'models.TemporalUnet'
+    model = 'models.MLPnet'
     diffusion = 'models.GaussianInvDynDiffusion'
     horizon = 100
     n_diffusion_steps = 200
diff --git a/code/diffuser/models/diffusion.py b/code/diffuser/models/diffusion.py
index a1f74ea..f3b6b94 100644
--- a/code/diffuser/models/diffusion.py
+++ b/code/diffuser/models/diffusion.py
@@ -3,6 +3,8 @@ import torch
 from torch import nn
 import torch.nn.functional as F
 import pdb
+import clip
+from config.locomotion_config import Config
 
 import diffuser.utils as utils
 from .helpers import (
@@ -295,12 +297,22 @@ class GaussianInvDynDiffusion(nn.Module):
         condition_guidance_w=0.1, ar_inv=False, train_only_inv=False):
         super().__init__()
         self.horizon = horizon
-        self.observation_dim = observation_dim
-        self.action_dim = action_dim
+        self.observation_dim = observation_dim # 11
+        self.action_dim = action_dim # 3
         self.transition_dim = observation_dim + action_dim
         self.model = model
         self.ar_inv = ar_inv
         self.train_only_inv = train_only_inv
+
+        # Text embedding
+        device = Config.device
+        self.clip_model, _ = clip.load("ViT-B/32", device=device)
+        for p in self.clip_model.parameters():
+            p.requires_grad = False
+        with torch.no_grad():
+            self.text_cond = self.clip_model.encode_text(clip.tokenize('the man is hopping').to(device))
+        self.proj_layer = nn.Linear(512, self.observation_dim)
+
         if self.ar_inv:
             self.inv_model = ARInvModel(hidden_dim=hidden_dim, observation_dim=observation_dim, action_dim=action_dim)
         else:
@@ -538,6 +550,9 @@ class GaussianInvDynDiffusion(nn.Module):
         return loss, info
 
     def forward(self, cond, *args, **kwargs):
+        # bs = len(cond)
+        # forward_feats = self.proj_layer(self.text_cond.expand(bs, -1))
+        # return self.conditional_sample(cond=forward_feats, *args, **kwargs)
         return self.conditional_sample(cond=cond, *args, **kwargs)
 
 
diff --git a/code/diffuser/models/temporal.py b/code/diffuser/models/temporal.py
index 63155c2..e827e49 100644
--- a/code/diffuser/models/temporal.py
+++ b/code/diffuser/models/temporal.py
@@ -354,6 +354,8 @@ class MLPnet(nn.Module):
             returns : [batch x 1]
         '''
         # Assumes horizon = 1
+        print(cond)
+        breakpoint()
         t = self.time_mlp(time)
 
         if self.returns_condition:
diff --git a/code/environment.yml b/code/environment.yml
index 31a3487..47a996c 100644
--- a/code/environment.yml
+++ b/code/environment.yml
@@ -8,9 +8,9 @@ dependencies:
 - patchelf
 - pip:
     - -f https://download.pytorch.org/whl/torch_stable.html
-    - numpy
+    - numpy==1.23.0
     - gym==0.18.0
-    - mujoco-py==2.0.2.13
+    - mujoco-py
     - matplotlib==3.3.4
     - torch==1.9.1+cu111
     - typed-argument-parserdiff --git a/code/config/locomotion_config.py b/code/config/locomotion_config.py
index 90173b3..84d0757 100644
--- a/code/config/locomotion_config.py
+++ b/code/config/locomotion_config.py
@@ -6,11 +6,12 @@ class Config(ParamsProto):
     # misc
     seed = 100
     device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
-    bucket = '/home/aajay/weights/'
+    bucket = '/home/yanyangji/research/RL/final/decision-diffuser/code/output'
     dataset = 'hopper-medium-expert-v2'
 
     ## model
     model = 'models.TemporalUnet'
+    # model = 'models.MLPnet'
     diffusion = 'models.GaussianInvDynDiffusion'
     horizon = 100
     n_diffusion_steps = 200
diff --git a/code/diffuser/models/diffusion.py b/code/diffuser/models/diffusion.py
index a1f74ea..f3b6b94 100644
--- a/code/diffuser/models/diffusion.py
+++ b/code/diffuser/models/diffusion.py
@@ -3,6 +3,8 @@ import torch
 from torch import nn
 import torch.nn.functional as F
 import pdb
+import clip
+from config.locomotion_config import Config
 
 import diffuser.utils as utils
 from .helpers import (
@@ -295,12 +297,22 @@ class GaussianInvDynDiffusion(nn.Module):
         condition_guidance_w=0.1, ar_inv=False, train_only_inv=False):
         super().__init__()
         self.horizon = horizon
-        self.observation_dim = observation_dim
-        self.action_dim = action_dim
+        self.observation_dim = observation_dim # 11
+        self.action_dim = action_dim # 3
         self.transition_dim = observation_dim + action_dim
         self.model = model
         self.ar_inv = ar_inv
         self.train_only_inv = train_only_inv
+
+        # Text embedding
+        device = Config.device
+        self.clip_model, _ = clip.load("ViT-B/32", device=device)
+        for p in self.clip_model.parameters():
+            p.requires_grad = False
+        with torch.no_grad():
+            self.text_cond = self.clip_model.encode_text(clip.tokenize('the man is hopping').to(device))
+        self.proj_layer = nn.Linear(512, self.observation_dim)
+
         if self.ar_inv:
             self.inv_model = ARInvModel(hidden_dim=hidden_dim, observation_dim=observation_dim, action_dim=action_dim)
         else:
@@ -538,6 +550,9 @@ class GaussianInvDynDiffusion(nn.Module):
         return loss, info
 
     def forward(self, cond, *args, **kwargs):
+        # bs = len(cond)
+        # forward_feats = self.proj_layer(self.text_cond.expand(bs, -1))
+        # return self.conditional_sample(cond=forward_feats, *args, **kwargs)
         return self.conditional_sample(cond=cond, *args, **kwargs)
 
 
diff --git a/code/diffuser/models/temporal.py b/code/diffuser/models/temporal.py
index 63155c2..e827e49 100644
--- a/code/diffuser/models/temporal.py
+++ b/code/diffuser/models/temporal.py
@@ -354,6 +354,8 @@ class MLPnet(nn.Module):
             returns : [batch x 1]
         '''
         # Assumes horizon = 1
+        print(cond)
+        breakpoint()
         t = self.time_mlp(time)
 
         if self.returns_condition:
diff --git a/code/environment.yml b/code/environment.yml
index 31a3487..47a996c 100644
--- a/code/environment.yml
+++ b/code/environment.yml
@@ -8,9 +8,9 @@ dependencies:
 - patchelf
 - pip:
     - -f https://download.pytorch.org/whl/torch_stable.html
-    - numpy
+    - numpy==1.23.0
     - gym==0.18.0
-    - mujoco-py==2.0.2.13
+    - mujoco-py
     - matplotlib==3.3.4
     - torch==1.9.1+cu111
     - typed-argument-parserdiff --git a/code/config/locomotion_config.py b/code/config/locomotion_config.py
index 90173b3..64a8820 100644
--- a/code/config/locomotion_config.py
+++ b/code/config/locomotion_config.py
@@ -6,11 +6,12 @@ class Config(ParamsProto):
     # misc
     seed = 100
     device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
-    bucket = '/home/aajay/weights/'
+    bucket = '/home/yanyangji/research/RL/final/decision-diffuser/code/output'
     dataset = 'hopper-medium-expert-v2'
 
     ## model
-    model = 'models.TemporalUnet'
+    # model = 'models.TemporalUnet'
+    model = 'models.MLPnet'
     diffusion = 'models.GaussianInvDynDiffusion'
     horizon = 100
     n_diffusion_steps = 200
diff --git a/code/diffuser/models/diffusion.py b/code/diffuser/models/diffusion.py
index a1f74ea..f3b6b94 100644
--- a/code/diffuser/models/diffusion.py
+++ b/code/diffuser/models/diffusion.py
@@ -3,6 +3,8 @@ import torch
 from torch import nn
 import torch.nn.functional as F
 import pdb
+import clip
+from config.locomotion_config import Config
 
 import diffuser.utils as utils
 from .helpers import (
@@ -295,12 +297,22 @@ class GaussianInvDynDiffusion(nn.Module):
         condition_guidance_w=0.1, ar_inv=False, train_only_inv=False):
         super().__init__()
         self.horizon = horizon
-        self.observation_dim = observation_dim
-        self.action_dim = action_dim
+        self.observation_dim = observation_dim # 11
+        self.action_dim = action_dim # 3
         self.transition_dim = observation_dim + action_dim
         self.model = model
         self.ar_inv = ar_inv
         self.train_only_inv = train_only_inv
+
+        # Text embedding
+        device = Config.device
+        self.clip_model, _ = clip.load("ViT-B/32", device=device)
+        for p in self.clip_model.parameters():
+            p.requires_grad = False
+        with torch.no_grad():
+            self.text_cond = self.clip_model.encode_text(clip.tokenize('the man is hopping').to(device))
+        self.proj_layer = nn.Linear(512, self.observation_dim)
+
         if self.ar_inv:
             self.inv_model = ARInvModel(hidden_dim=hidden_dim, observation_dim=observation_dim, action_dim=action_dim)
         else:
@@ -538,6 +550,9 @@ class GaussianInvDynDiffusion(nn.Module):
         return loss, info
 
     def forward(self, cond, *args, **kwargs):
+        # bs = len(cond)
+        # forward_feats = self.proj_layer(self.text_cond.expand(bs, -1))
+        # return self.conditional_sample(cond=forward_feats, *args, **kwargs)
         return self.conditional_sample(cond=cond, *args, **kwargs)
 
 
diff --git a/code/diffuser/models/temporal.py b/code/diffuser/models/temporal.py
index 63155c2..8e1164c 100644
--- a/code/diffuser/models/temporal.py
+++ b/code/diffuser/models/temporal.py
@@ -354,6 +354,8 @@ class MLPnet(nn.Module):
             returns : [batch x 1]
         '''
         # Assumes horizon = 1
+        # print(cond)
+        # breakpoint()
         t = self.time_mlp(time)
 
         if self.returns_condition:
diff --git a/code/environment.yml b/code/environment.yml
index 31a3487..47a996c 100644
--- a/code/environment.yml
+++ b/code/environment.yml
@@ -8,9 +8,9 @@ dependencies:
 - patchelf
 - pip:
     - -f https://download.pytorch.org/whl/torch_stable.html
-    - numpy
+    - numpy==1.23.0
     - gym==0.18.0
-    - mujoco-py==2.0.2.13
+    - mujoco-py
     - matplotlib==3.3.4
     - torch==1.9.1+cu111
     - typed-argument-parserdiff --git a/code/config/locomotion_config.py b/code/config/locomotion_config.py
index 90173b3..64a8820 100644
--- a/code/config/locomotion_config.py
+++ b/code/config/locomotion_config.py
@@ -6,11 +6,12 @@ class Config(ParamsProto):
     # misc
     seed = 100
     device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
-    bucket = '/home/aajay/weights/'
+    bucket = '/home/yanyangji/research/RL/final/decision-diffuser/code/output'
     dataset = 'hopper-medium-expert-v2'
 
     ## model
-    model = 'models.TemporalUnet'
+    # model = 'models.TemporalUnet'
+    model = 'models.MLPnet'
     diffusion = 'models.GaussianInvDynDiffusion'
     horizon = 100
     n_diffusion_steps = 200
diff --git a/code/diffuser/models/diffusion.py b/code/diffuser/models/diffusion.py
index a1f74ea..77be9e9 100644
--- a/code/diffuser/models/diffusion.py
+++ b/code/diffuser/models/diffusion.py
@@ -3,6 +3,8 @@ import torch
 from torch import nn
 import torch.nn.functional as F
 import pdb
+import clip
+from config.locomotion_config import Config
 
 import diffuser.utils as utils
 from .helpers import (
@@ -295,12 +297,22 @@ class GaussianInvDynDiffusion(nn.Module):
         condition_guidance_w=0.1, ar_inv=False, train_only_inv=False):
         super().__init__()
         self.horizon = horizon
-        self.observation_dim = observation_dim
-        self.action_dim = action_dim
+        self.observation_dim = observation_dim # 11
+        self.action_dim = action_dim # 3
         self.transition_dim = observation_dim + action_dim
         self.model = model
         self.ar_inv = ar_inv
         self.train_only_inv = train_only_inv
+
+        # Text embedding
+        device = Config.device
+        self.clip_model, _ = clip.load("ViT-B/32", device=device)
+        for p in self.clip_model.parameters():
+            p.requires_grad = False
+        with torch.no_grad():
+            self.text_cond = self.clip_model.encode_text(clip.tokenize('the man is hopping').to(device))
+        self.proj_layer = nn.Linear(512, self.observation_dim)
+
         if self.ar_inv:
             self.inv_model = ARInvModel(hidden_dim=hidden_dim, observation_dim=observation_dim, action_dim=action_dim)
         else:
@@ -538,7 +550,10 @@ class GaussianInvDynDiffusion(nn.Module):
         return loss, info
 
     def forward(self, cond, *args, **kwargs):
-        return self.conditional_sample(cond=cond, *args, **kwargs)
+        # bs = len(cond)
+        # forward_feats = self.proj_layer(self.text_cond.expand(bs, -1))
+        # return self.conditional_sample(cond=forward_feats, *args, **kwargs)
+        return self.conditional_sample(cond=cond[0], *args, **kwargs)
 
 
 class ARInvModel(nn.Module):
diff --git a/code/diffuser/models/temporal.py b/code/diffuser/models/temporal.py
index 63155c2..8e1164c 100644
--- a/code/diffuser/models/temporal.py
+++ b/code/diffuser/models/temporal.py
@@ -354,6 +354,8 @@ class MLPnet(nn.Module):
             returns : [batch x 1]
         '''
         # Assumes horizon = 1
+        # print(cond)
+        # breakpoint()
         t = self.time_mlp(time)
 
         if self.returns_condition:
diff --git a/code/environment.yml b/code/environment.yml
index 31a3487..47a996c 100644
--- a/code/environment.yml
+++ b/code/environment.yml
@@ -8,9 +8,9 @@ dependencies:
 - patchelf
 - pip:
     - -f https://download.pytorch.org/whl/torch_stable.html
-    - numpy
+    - numpy==1.23.0
     - gym==0.18.0
-    - mujoco-py==2.0.2.13
+    - mujoco-py
     - matplotlib==3.3.4
     - torch==1.9.1+cu111
     - typed-argument-parserdiff --git a/code/config/locomotion_config.py b/code/config/locomotion_config.py
index 90173b3..64a8820 100644
--- a/code/config/locomotion_config.py
+++ b/code/config/locomotion_config.py
@@ -6,11 +6,12 @@ class Config(ParamsProto):
     # misc
     seed = 100
     device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
-    bucket = '/home/aajay/weights/'
+    bucket = '/home/yanyangji/research/RL/final/decision-diffuser/code/output'
     dataset = 'hopper-medium-expert-v2'
 
     ## model
-    model = 'models.TemporalUnet'
+    # model = 'models.TemporalUnet'
+    model = 'models.MLPnet'
     diffusion = 'models.GaussianInvDynDiffusion'
     horizon = 100
     n_diffusion_steps = 200
diff --git a/code/diffuser/models/diffusion.py b/code/diffuser/models/diffusion.py
index a1f74ea..f3b6b94 100644
--- a/code/diffuser/models/diffusion.py
+++ b/code/diffuser/models/diffusion.py
@@ -3,6 +3,8 @@ import torch
 from torch import nn
 import torch.nn.functional as F
 import pdb
+import clip
+from config.locomotion_config import Config
 
 import diffuser.utils as utils
 from .helpers import (
@@ -295,12 +297,22 @@ class GaussianInvDynDiffusion(nn.Module):
         condition_guidance_w=0.1, ar_inv=False, train_only_inv=False):
         super().__init__()
         self.horizon = horizon
-        self.observation_dim = observation_dim
-        self.action_dim = action_dim
+        self.observation_dim = observation_dim # 11
+        self.action_dim = action_dim # 3
         self.transition_dim = observation_dim + action_dim
         self.model = model
         self.ar_inv = ar_inv
         self.train_only_inv = train_only_inv
+
+        # Text embedding
+        device = Config.device
+        self.clip_model, _ = clip.load("ViT-B/32", device=device)
+        for p in self.clip_model.parameters():
+            p.requires_grad = False
+        with torch.no_grad():
+            self.text_cond = self.clip_model.encode_text(clip.tokenize('the man is hopping').to(device))
+        self.proj_layer = nn.Linear(512, self.observation_dim)
+
         if self.ar_inv:
             self.inv_model = ARInvModel(hidden_dim=hidden_dim, observation_dim=observation_dim, action_dim=action_dim)
         else:
@@ -538,6 +550,9 @@ class GaussianInvDynDiffusion(nn.Module):
         return loss, info
 
     def forward(self, cond, *args, **kwargs):
+        # bs = len(cond)
+        # forward_feats = self.proj_layer(self.text_cond.expand(bs, -1))
+        # return self.conditional_sample(cond=forward_feats, *args, **kwargs)
         return self.conditional_sample(cond=cond, *args, **kwargs)
 
 
diff --git a/code/diffuser/models/temporal.py b/code/diffuser/models/temporal.py
index 63155c2..8e1164c 100644
--- a/code/diffuser/models/temporal.py
+++ b/code/diffuser/models/temporal.py
@@ -354,6 +354,8 @@ class MLPnet(nn.Module):
             returns : [batch x 1]
         '''
         # Assumes horizon = 1
+        # print(cond)
+        # breakpoint()
         t = self.time_mlp(time)
 
         if self.returns_condition:
diff --git a/code/environment.yml b/code/environment.yml
index 31a3487..47a996c 100644
--- a/code/environment.yml
+++ b/code/environment.yml
@@ -8,9 +8,9 @@ dependencies:
 - patchelf
 - pip:
     - -f https://download.pytorch.org/whl/torch_stable.html
-    - numpy
+    - numpy==1.23.0
     - gym==0.18.0
-    - mujoco-py==2.0.2.13
+    - mujoco-py
     - matplotlib==3.3.4
     - torch==1.9.1+cu111
     - typed-argument-parserdiff --git a/code/config/locomotion_config.py b/code/config/locomotion_config.py
index 90173b3..84d0757 100644
--- a/code/config/locomotion_config.py
+++ b/code/config/locomotion_config.py
@@ -6,11 +6,12 @@ class Config(ParamsProto):
     # misc
     seed = 100
     device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
-    bucket = '/home/aajay/weights/'
+    bucket = '/home/yanyangji/research/RL/final/decision-diffuser/code/output'
     dataset = 'hopper-medium-expert-v2'
 
     ## model
     model = 'models.TemporalUnet'
+    # model = 'models.MLPnet'
     diffusion = 'models.GaussianInvDynDiffusion'
     horizon = 100
     n_diffusion_steps = 200
diff --git a/code/diffuser/models/diffusion.py b/code/diffuser/models/diffusion.py
index a1f74ea..f3b6b94 100644
--- a/code/diffuser/models/diffusion.py
+++ b/code/diffuser/models/diffusion.py
@@ -3,6 +3,8 @@ import torch
 from torch import nn
 import torch.nn.functional as F
 import pdb
+import clip
+from config.locomotion_config import Config
 
 import diffuser.utils as utils
 from .helpers import (
@@ -295,12 +297,22 @@ class GaussianInvDynDiffusion(nn.Module):
         condition_guidance_w=0.1, ar_inv=False, train_only_inv=False):
         super().__init__()
         self.horizon = horizon
-        self.observation_dim = observation_dim
-        self.action_dim = action_dim
+        self.observation_dim = observation_dim # 11
+        self.action_dim = action_dim # 3
         self.transition_dim = observation_dim + action_dim
         self.model = model
         self.ar_inv = ar_inv
         self.train_only_inv = train_only_inv
+
+        # Text embedding
+        device = Config.device
+        self.clip_model, _ = clip.load("ViT-B/32", device=device)
+        for p in self.clip_model.parameters():
+            p.requires_grad = False
+        with torch.no_grad():
+            self.text_cond = self.clip_model.encode_text(clip.tokenize('the man is hopping').to(device))
+        self.proj_layer = nn.Linear(512, self.observation_dim)
+
         if self.ar_inv:
             self.inv_model = ARInvModel(hidden_dim=hidden_dim, observation_dim=observation_dim, action_dim=action_dim)
         else:
@@ -538,6 +550,9 @@ class GaussianInvDynDiffusion(nn.Module):
         return loss, info
 
     def forward(self, cond, *args, **kwargs):
+        # bs = len(cond)
+        # forward_feats = self.proj_layer(self.text_cond.expand(bs, -1))
+        # return self.conditional_sample(cond=forward_feats, *args, **kwargs)
         return self.conditional_sample(cond=cond, *args, **kwargs)
 
 
diff --git a/code/diffuser/utils/training.py b/code/diffuser/utils/training.py
index e8dcdb4..a4200ea 100644
--- a/code/diffuser/utils/training.py
+++ b/code/diffuser/utils/training.py
@@ -113,6 +113,7 @@ class Trainer(object):
             for i in range(self.gradient_accumulate_every):
                 batch = next(self.dataloader)
                 batch = batch_to_device(batch, device=self.device)
+                breakpoint()
                 loss, infos = self.model.loss(*batch)
                 loss = loss / self.gradient_accumulate_every
                 loss.backward()
diff --git a/code/environment.yml b/code/environment.yml
index 31a3487..47a996c 100644
--- a/code/environment.yml
+++ b/code/environment.yml
@@ -8,9 +8,9 @@ dependencies:
 - patchelf
 - pip:
     - -f https://download.pytorch.org/whl/torch_stable.html
-    - numpy
+    - numpy==1.23.0
     - gym==0.18.0
-    - mujoco-py==2.0.2.13
+    - mujoco-py
     - matplotlib==3.3.4
     - torch==1.9.1+cu111
     - typed-argument-parserdiff --git a/code/config/locomotion_config.py b/code/config/locomotion_config.py
index 90173b3..64a8820 100644
--- a/code/config/locomotion_config.py
+++ b/code/config/locomotion_config.py
@@ -6,11 +6,12 @@ class Config(ParamsProto):
     # misc
     seed = 100
     device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
-    bucket = '/home/aajay/weights/'
+    bucket = '/home/yanyangji/research/RL/final/decision-diffuser/code/output'
     dataset = 'hopper-medium-expert-v2'
 
     ## model
-    model = 'models.TemporalUnet'
+    # model = 'models.TemporalUnet'
+    model = 'models.MLPnet'
     diffusion = 'models.GaussianInvDynDiffusion'
     horizon = 100
     n_diffusion_steps = 200
diff --git a/code/diffuser/datasets/sequence.py b/code/diffuser/datasets/sequence.py
index 31ac84f..f256554 100644
--- a/code/diffuser/datasets/sequence.py
+++ b/code/diffuser/datasets/sequence.py
@@ -76,7 +76,8 @@ class SequenceDataset(torch.utils.data.Dataset):
         '''
             condition on current observation for planning
         '''
-        return {0: observations[0]}
+        # return {0: observations[0]}
+        return observations[0]
 
     def __len__(self):
         return len(self.indices)
diff --git a/code/diffuser/models/diffusion.py b/code/diffuser/models/diffusion.py
index a1f74ea..f3b6b94 100644
--- a/code/diffuser/models/diffusion.py
+++ b/code/diffuser/models/diffusion.py
@@ -3,6 +3,8 @@ import torch
 from torch import nn
 import torch.nn.functional as F
 import pdb
+import clip
+from config.locomotion_config import Config
 
 import diffuser.utils as utils
 from .helpers import (
@@ -295,12 +297,22 @@ class GaussianInvDynDiffusion(nn.Module):
         condition_guidance_w=0.1, ar_inv=False, train_only_inv=False):
         super().__init__()
         self.horizon = horizon
-        self.observation_dim = observation_dim
-        self.action_dim = action_dim
+        self.observation_dim = observation_dim # 11
+        self.action_dim = action_dim # 3
         self.transition_dim = observation_dim + action_dim
         self.model = model
         self.ar_inv = ar_inv
         self.train_only_inv = train_only_inv
+
+        # Text embedding
+        device = Config.device
+        self.clip_model, _ = clip.load("ViT-B/32", device=device)
+        for p in self.clip_model.parameters():
+            p.requires_grad = False
+        with torch.no_grad():
+            self.text_cond = self.clip_model.encode_text(clip.tokenize('the man is hopping').to(device))
+        self.proj_layer = nn.Linear(512, self.observation_dim)
+
         if self.ar_inv:
             self.inv_model = ARInvModel(hidden_dim=hidden_dim, observation_dim=observation_dim, action_dim=action_dim)
         else:
@@ -538,6 +550,9 @@ class GaussianInvDynDiffusion(nn.Module):
         return loss, info
 
     def forward(self, cond, *args, **kwargs):
+        # bs = len(cond)
+        # forward_feats = self.proj_layer(self.text_cond.expand(bs, -1))
+        # return self.conditional_sample(cond=forward_feats, *args, **kwargs)
         return self.conditional_sample(cond=cond, *args, **kwargs)
 
 
diff --git a/code/diffuser/models/helpers.py b/code/diffuser/models/helpers.py
index 53859d4..7384d2a 100644
--- a/code/diffuser/models/helpers.py
+++ b/code/diffuser/models/helpers.py
@@ -91,8 +91,8 @@ def cosine_beta_schedule(timesteps, s=0.008, dtype=torch.float32):
     return torch.tensor(betas_clipped, dtype=dtype)
 
 def apply_conditioning(x, conditions, action_dim):
-    for t, val in conditions.items():
-        x[:, t, action_dim:] = val.clone()
+    # for t, val in conditions.items():
+    #     x[:, t, action_dim:] = val.clone() # batch size, time step, trajectory (all observation)
     return x
 
 #-----------------------------------------------------------------------------#
diff --git a/code/environment.yml b/code/environment.yml
index 31a3487..47a996c 100644
--- a/code/environment.yml
+++ b/code/environment.yml
@@ -8,9 +8,9 @@ dependencies:
 - patchelf
 - pip:
     - -f https://download.pytorch.org/whl/torch_stable.html
-    - numpy
+    - numpy==1.23.0
     - gym==0.18.0
-    - mujoco-py==2.0.2.13
+    - mujoco-py
     - matplotlib==3.3.4
     - torch==1.9.1+cu111
     - typed-argument-parserdiff --git a/code/config/locomotion_config.py b/code/config/locomotion_config.py
index 90173b3..64a8820 100644
--- a/code/config/locomotion_config.py
+++ b/code/config/locomotion_config.py
@@ -6,11 +6,12 @@ class Config(ParamsProto):
     # misc
     seed = 100
     device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
-    bucket = '/home/aajay/weights/'
+    bucket = '/home/yanyangji/research/RL/final/decision-diffuser/code/output'
     dataset = 'hopper-medium-expert-v2'
 
     ## model
-    model = 'models.TemporalUnet'
+    # model = 'models.TemporalUnet'
+    model = 'models.MLPnet'
     diffusion = 'models.GaussianInvDynDiffusion'
     horizon = 100
     n_diffusion_steps = 200
diff --git a/code/diffuser/datasets/sequence.py b/code/diffuser/datasets/sequence.py
index 31ac84f..f256554 100644
--- a/code/diffuser/datasets/sequence.py
+++ b/code/diffuser/datasets/sequence.py
@@ -76,7 +76,8 @@ class SequenceDataset(torch.utils.data.Dataset):
         '''
             condition on current observation for planning
         '''
-        return {0: observations[0]}
+        # return {0: observations[0]}
+        return observations[0]
 
     def __len__(self):
         return len(self.indices)
diff --git a/code/diffuser/models/diffusion.py b/code/diffuser/models/diffusion.py
index a1f74ea..f3b6b94 100644
--- a/code/diffuser/models/diffusion.py
+++ b/code/diffuser/models/diffusion.py
@@ -3,6 +3,8 @@ import torch
 from torch import nn
 import torch.nn.functional as F
 import pdb
+import clip
+from config.locomotion_config import Config
 
 import diffuser.utils as utils
 from .helpers import (
@@ -295,12 +297,22 @@ class GaussianInvDynDiffusion(nn.Module):
         condition_guidance_w=0.1, ar_inv=False, train_only_inv=False):
         super().__init__()
         self.horizon = horizon
-        self.observation_dim = observation_dim
-        self.action_dim = action_dim
+        self.observation_dim = observation_dim # 11
+        self.action_dim = action_dim # 3
         self.transition_dim = observation_dim + action_dim
         self.model = model
         self.ar_inv = ar_inv
         self.train_only_inv = train_only_inv
+
+        # Text embedding
+        device = Config.device
+        self.clip_model, _ = clip.load("ViT-B/32", device=device)
+        for p in self.clip_model.parameters():
+            p.requires_grad = False
+        with torch.no_grad():
+            self.text_cond = self.clip_model.encode_text(clip.tokenize('the man is hopping').to(device))
+        self.proj_layer = nn.Linear(512, self.observation_dim)
+
         if self.ar_inv:
             self.inv_model = ARInvModel(hidden_dim=hidden_dim, observation_dim=observation_dim, action_dim=action_dim)
         else:
@@ -538,6 +550,9 @@ class GaussianInvDynDiffusion(nn.Module):
         return loss, info
 
     def forward(self, cond, *args, **kwargs):
+        # bs = len(cond)
+        # forward_feats = self.proj_layer(self.text_cond.expand(bs, -1))
+        # return self.conditional_sample(cond=forward_feats, *args, **kwargs)
         return self.conditional_sample(cond=cond, *args, **kwargs)
 
 
diff --git a/code/diffuser/models/helpers.py b/code/diffuser/models/helpers.py
index 53859d4..7384d2a 100644
--- a/code/diffuser/models/helpers.py
+++ b/code/diffuser/models/helpers.py
@@ -91,8 +91,8 @@ def cosine_beta_schedule(timesteps, s=0.008, dtype=torch.float32):
     return torch.tensor(betas_clipped, dtype=dtype)
 
 def apply_conditioning(x, conditions, action_dim):
-    for t, val in conditions.items():
-        x[:, t, action_dim:] = val.clone()
+    # for t, val in conditions.items():
+    #     x[:, t, action_dim:] = val.clone() # batch size, time step, trajectory (all observation)
     return x
 
 #-----------------------------------------------------------------------------#
diff --git a/code/diffuser/models/temporal.py b/code/diffuser/models/temporal.py
index 63155c2..b0cee0f 100644
--- a/code/diffuser/models/temporal.py
+++ b/code/diffuser/models/temporal.py
@@ -365,7 +365,7 @@ class MLPnet(nn.Module):
             if force_dropout:
                 returns_embed = 0*returns_embed
             t = torch.cat([t, returns_embed], dim=-1)
-
+        breakpoint()
         inp = torch.cat([t, cond, x], dim=-1)
         out  = self.mlp(inp)
 
diff --git a/code/environment.yml b/code/environment.yml
index 31a3487..47a996c 100644
--- a/code/environment.yml
+++ b/code/environment.yml
@@ -8,9 +8,9 @@ dependencies:
 - patchelf
 - pip:
     - -f https://download.pytorch.org/whl/torch_stable.html
-    - numpy
+    - numpy==1.23.0
     - gym==0.18.0
-    - mujoco-py==2.0.2.13
+    - mujoco-py
     - matplotlib==3.3.4
     - torch==1.9.1+cu111
     - typed-argument-parserdiff --git a/code/config/locomotion_config.py b/code/config/locomotion_config.py
index 90173b3..9895e57 100644
--- a/code/config/locomotion_config.py
+++ b/code/config/locomotion_config.py
@@ -6,13 +6,15 @@ class Config(ParamsProto):
     # misc
     seed = 100
     device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
-    bucket = '/home/aajay/weights/'
+    bucket = '/home/yanyangji/research/RL/final/decision-diffuser/code/output'
     dataset = 'hopper-medium-expert-v2'
 
     ## model
-    model = 'models.TemporalUnet'
+    # model = 'models.TemporalUnet'
+    model = 'models.MLPnet'
     diffusion = 'models.GaussianInvDynDiffusion'
-    horizon = 100
+    # horizon = 100
+    horizon = 1
     n_diffusion_steps = 200
     action_weight = 10
     loss_weights = None
diff --git a/code/diffuser/datasets/sequence.py b/code/diffuser/datasets/sequence.py
index 31ac84f..f256554 100644
--- a/code/diffuser/datasets/sequence.py
+++ b/code/diffuser/datasets/sequence.py
@@ -76,7 +76,8 @@ class SequenceDataset(torch.utils.data.Dataset):
         '''
             condition on current observation for planning
         '''
-        return {0: observations[0]}
+        # return {0: observations[0]}
+        return observations[0]
 
     def __len__(self):
         return len(self.indices)
diff --git a/code/diffuser/models/diffusion.py b/code/diffuser/models/diffusion.py
index a1f74ea..0c8f1a1 100644
--- a/code/diffuser/models/diffusion.py
+++ b/code/diffuser/models/diffusion.py
@@ -3,6 +3,8 @@ import torch
 from torch import nn
 import torch.nn.functional as F
 import pdb
+import clip
+from config.locomotion_config import Config
 
 import diffuser.utils as utils
 from .helpers import (
@@ -295,12 +297,22 @@ class GaussianInvDynDiffusion(nn.Module):
         condition_guidance_w=0.1, ar_inv=False, train_only_inv=False):
         super().__init__()
         self.horizon = horizon
-        self.observation_dim = observation_dim
-        self.action_dim = action_dim
+        self.observation_dim = observation_dim # 11
+        self.action_dim = action_dim # 3
         self.transition_dim = observation_dim + action_dim
         self.model = model
         self.ar_inv = ar_inv
         self.train_only_inv = train_only_inv
+
+        # Text embedding
+        device = Config.device
+        self.clip_model, _ = clip.load("ViT-B/32", device=device)
+        for p in self.clip_model.parameters():
+            p.requires_grad = False
+        with torch.no_grad():
+            self.text_cond = self.clip_model.encode_text(clip.tokenize('the man is hopping').to(device))
+        self.proj_layer = nn.Linear(512, self.observation_dim)
+
         if self.ar_inv:
             self.inv_model = ARInvModel(hidden_dim=hidden_dim, observation_dim=observation_dim, action_dim=action_dim)
         else:
diff --git a/code/diffuser/models/helpers.py b/code/diffuser/models/helpers.py
index 53859d4..7384d2a 100644
--- a/code/diffuser/models/helpers.py
+++ b/code/diffuser/models/helpers.py
@@ -91,8 +91,8 @@ def cosine_beta_schedule(timesteps, s=0.008, dtype=torch.float32):
     return torch.tensor(betas_clipped, dtype=dtype)
 
 def apply_conditioning(x, conditions, action_dim):
-    for t, val in conditions.items():
-        x[:, t, action_dim:] = val.clone()
+    # for t, val in conditions.items():
+    #     x[:, t, action_dim:] = val.clone() # batch size, time step, trajectory (all observation)
     return x
 
 #-----------------------------------------------------------------------------#
diff --git a/code/diffuser/models/temporal.py b/code/diffuser/models/temporal.py
index 63155c2..ad48ecf 100644
--- a/code/diffuser/models/temporal.py
+++ b/code/diffuser/models/temporal.py
@@ -323,8 +323,8 @@ class MLPnet(nn.Module):
         self.returns_condition = returns_condition
         self.condition_dropout = condition_dropout
         self.calc_energy = calc_energy
-        self.transition_dim = transition_dim
-        self.action_dim = transition_dim - cond_dim
+        self.transition_dim = transition_dim + cond_dim
+        self.action_dim = transition_dim - cond_dim*2
 
         if self.returns_condition:
             self.returns_mlp = nn.Sequential(
@@ -365,7 +365,7 @@ class MLPnet(nn.Module):
             if force_dropout:
                 returns_embed = 0*returns_embed
             t = torch.cat([t, returns_embed], dim=-1)
-
+        # breakpoint()
         inp = torch.cat([t, cond, x], dim=-1)
         out  = self.mlp(inp)
 
diff --git a/code/environment.yml b/code/environment.yml
index 31a3487..47a996c 100644
--- a/code/environment.yml
+++ b/code/environment.yml
@@ -8,9 +8,9 @@ dependencies:
 - patchelf
 - pip:
     - -f https://download.pytorch.org/whl/torch_stable.html
-    - numpy
+    - numpy==1.23.0
     - gym==0.18.0
-    - mujoco-py==2.0.2.13
+    - mujoco-py
     - matplotlib==3.3.4
     - torch==1.9.1+cu111
     - typed-argument-parserdiff --git a/code/config/locomotion_config.py b/code/config/locomotion_config.py
index 90173b3..9895e57 100644
--- a/code/config/locomotion_config.py
+++ b/code/config/locomotion_config.py
@@ -6,13 +6,15 @@ class Config(ParamsProto):
     # misc
     seed = 100
     device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
-    bucket = '/home/aajay/weights/'
+    bucket = '/home/yanyangji/research/RL/final/decision-diffuser/code/output'
     dataset = 'hopper-medium-expert-v2'
 
     ## model
-    model = 'models.TemporalUnet'
+    # model = 'models.TemporalUnet'
+    model = 'models.MLPnet'
     diffusion = 'models.GaussianInvDynDiffusion'
-    horizon = 100
+    # horizon = 100
+    horizon = 1
     n_diffusion_steps = 200
     action_weight = 10
     loss_weights = None
diff --git a/code/diffuser/datasets/sequence.py b/code/diffuser/datasets/sequence.py
index 31ac84f..f256554 100644
--- a/code/diffuser/datasets/sequence.py
+++ b/code/diffuser/datasets/sequence.py
@@ -76,7 +76,8 @@ class SequenceDataset(torch.utils.data.Dataset):
         '''
             condition on current observation for planning
         '''
-        return {0: observations[0]}
+        # return {0: observations[0]}
+        return observations[0]
 
     def __len__(self):
         return len(self.indices)
diff --git a/code/diffuser/models/diffusion.py b/code/diffuser/models/diffusion.py
index a1f74ea..0c8f1a1 100644
--- a/code/diffuser/models/diffusion.py
+++ b/code/diffuser/models/diffusion.py
@@ -3,6 +3,8 @@ import torch
 from torch import nn
 import torch.nn.functional as F
 import pdb
+import clip
+from config.locomotion_config import Config
 
 import diffuser.utils as utils
 from .helpers import (
@@ -295,12 +297,22 @@ class GaussianInvDynDiffusion(nn.Module):
         condition_guidance_w=0.1, ar_inv=False, train_only_inv=False):
         super().__init__()
         self.horizon = horizon
-        self.observation_dim = observation_dim
-        self.action_dim = action_dim
+        self.observation_dim = observation_dim # 11
+        self.action_dim = action_dim # 3
         self.transition_dim = observation_dim + action_dim
         self.model = model
         self.ar_inv = ar_inv
         self.train_only_inv = train_only_inv
+
+        # Text embedding
+        device = Config.device
+        self.clip_model, _ = clip.load("ViT-B/32", device=device)
+        for p in self.clip_model.parameters():
+            p.requires_grad = False
+        with torch.no_grad():
+            self.text_cond = self.clip_model.encode_text(clip.tokenize('the man is hopping').to(device))
+        self.proj_layer = nn.Linear(512, self.observation_dim)
+
         if self.ar_inv:
             self.inv_model = ARInvModel(hidden_dim=hidden_dim, observation_dim=observation_dim, action_dim=action_dim)
         else:
diff --git a/code/diffuser/models/helpers.py b/code/diffuser/models/helpers.py
index 53859d4..7384d2a 100644
--- a/code/diffuser/models/helpers.py
+++ b/code/diffuser/models/helpers.py
@@ -91,8 +91,8 @@ def cosine_beta_schedule(timesteps, s=0.008, dtype=torch.float32):
     return torch.tensor(betas_clipped, dtype=dtype)
 
 def apply_conditioning(x, conditions, action_dim):
-    for t, val in conditions.items():
-        x[:, t, action_dim:] = val.clone()
+    # for t, val in conditions.items():
+    #     x[:, t, action_dim:] = val.clone() # batch size, time step, trajectory (all observation)
     return x
 
 #-----------------------------------------------------------------------------#
diff --git a/code/diffuser/models/temporal.py b/code/diffuser/models/temporal.py
index 63155c2..d73e019 100644
--- a/code/diffuser/models/temporal.py
+++ b/code/diffuser/models/temporal.py
@@ -323,7 +323,7 @@ class MLPnet(nn.Module):
         self.returns_condition = returns_condition
         self.condition_dropout = condition_dropout
         self.calc_energy = calc_energy
-        self.transition_dim = transition_dim
+        self.transition_dim = transition_dim + cond_dim
         self.action_dim = transition_dim - cond_dim
 
         if self.returns_condition:
@@ -365,8 +365,9 @@ class MLPnet(nn.Module):
             if force_dropout:
                 returns_embed = 0*returns_embed
             t = torch.cat([t, returns_embed], dim=-1)
-
+        breakpoint()
         inp = torch.cat([t, cond, x], dim=-1)
+        breakpoint()
         out  = self.mlp(inp)
 
         if self.calc_energy:
diff --git a/code/environment.yml b/code/environment.yml
index 31a3487..47a996c 100644
--- a/code/environment.yml
+++ b/code/environment.yml
@@ -8,9 +8,9 @@ dependencies:
 - patchelf
 - pip:
     - -f https://download.pytorch.org/whl/torch_stable.html
-    - numpy
+    - numpy==1.23.0
     - gym==0.18.0
-    - mujoco-py==2.0.2.13
+    - mujoco-py
     - matplotlib==3.3.4
     - torch==1.9.1+cu111
     - typed-argument-parserdiff --git a/code/config/locomotion_config.py b/code/config/locomotion_config.py
index 90173b3..9895e57 100644
--- a/code/config/locomotion_config.py
+++ b/code/config/locomotion_config.py
@@ -6,13 +6,15 @@ class Config(ParamsProto):
     # misc
     seed = 100
     device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
-    bucket = '/home/aajay/weights/'
+    bucket = '/home/yanyangji/research/RL/final/decision-diffuser/code/output'
     dataset = 'hopper-medium-expert-v2'
 
     ## model
-    model = 'models.TemporalUnet'
+    # model = 'models.TemporalUnet'
+    model = 'models.MLPnet'
     diffusion = 'models.GaussianInvDynDiffusion'
-    horizon = 100
+    # horizon = 100
+    horizon = 1
     n_diffusion_steps = 200
     action_weight = 10
     loss_weights = None
diff --git a/code/diffuser/datasets/sequence.py b/code/diffuser/datasets/sequence.py
index 31ac84f..f256554 100644
--- a/code/diffuser/datasets/sequence.py
+++ b/code/diffuser/datasets/sequence.py
@@ -76,7 +76,8 @@ class SequenceDataset(torch.utils.data.Dataset):
         '''
             condition on current observation for planning
         '''
-        return {0: observations[0]}
+        # return {0: observations[0]}
+        return observations[0]
 
     def __len__(self):
         return len(self.indices)
diff --git a/code/diffuser/models/diffusion.py b/code/diffuser/models/diffusion.py
index a1f74ea..0c8f1a1 100644
--- a/code/diffuser/models/diffusion.py
+++ b/code/diffuser/models/diffusion.py
@@ -3,6 +3,8 @@ import torch
 from torch import nn
 import torch.nn.functional as F
 import pdb
+import clip
+from config.locomotion_config import Config
 
 import diffuser.utils as utils
 from .helpers import (
@@ -295,12 +297,22 @@ class GaussianInvDynDiffusion(nn.Module):
         condition_guidance_w=0.1, ar_inv=False, train_only_inv=False):
         super().__init__()
         self.horizon = horizon
-        self.observation_dim = observation_dim
-        self.action_dim = action_dim
+        self.observation_dim = observation_dim # 11
+        self.action_dim = action_dim # 3
         self.transition_dim = observation_dim + action_dim
         self.model = model
         self.ar_inv = ar_inv
         self.train_only_inv = train_only_inv
+
+        # Text embedding
+        device = Config.device
+        self.clip_model, _ = clip.load("ViT-B/32", device=device)
+        for p in self.clip_model.parameters():
+            p.requires_grad = False
+        with torch.no_grad():
+            self.text_cond = self.clip_model.encode_text(clip.tokenize('the man is hopping').to(device))
+        self.proj_layer = nn.Linear(512, self.observation_dim)
+
         if self.ar_inv:
             self.inv_model = ARInvModel(hidden_dim=hidden_dim, observation_dim=observation_dim, action_dim=action_dim)
         else:
diff --git a/code/diffuser/models/helpers.py b/code/diffuser/models/helpers.py
index 53859d4..7384d2a 100644
--- a/code/diffuser/models/helpers.py
+++ b/code/diffuser/models/helpers.py
@@ -91,8 +91,8 @@ def cosine_beta_schedule(timesteps, s=0.008, dtype=torch.float32):
     return torch.tensor(betas_clipped, dtype=dtype)
 
 def apply_conditioning(x, conditions, action_dim):
-    for t, val in conditions.items():
-        x[:, t, action_dim:] = val.clone()
+    # for t, val in conditions.items():
+    #     x[:, t, action_dim:] = val.clone() # batch size, time step, trajectory (all observation)
     return x
 
 #-----------------------------------------------------------------------------#
diff --git a/code/diffuser/models/temporal.py b/code/diffuser/models/temporal.py
index 63155c2..5652a85 100644
--- a/code/diffuser/models/temporal.py
+++ b/code/diffuser/models/temporal.py
@@ -323,7 +323,7 @@ class MLPnet(nn.Module):
         self.returns_condition = returns_condition
         self.condition_dropout = condition_dropout
         self.calc_energy = calc_energy
-        self.transition_dim = transition_dim
+        self.transition_dim = transition_dim + cond_dim
         self.action_dim = transition_dim - cond_dim
 
         if self.returns_condition:
@@ -353,6 +353,7 @@ class MLPnet(nn.Module):
             cond: [batch x state]
             returns : [batch x 1]
         '''
+        x = x.squeeze(1)
         # Assumes horizon = 1
         t = self.time_mlp(time)
 
@@ -365,8 +366,9 @@ class MLPnet(nn.Module):
             if force_dropout:
                 returns_embed = 0*returns_embed
             t = torch.cat([t, returns_embed], dim=-1)
-
+        breakpoint()
         inp = torch.cat([t, cond, x], dim=-1)
+        breakpoint()
         out  = self.mlp(inp)
 
         if self.calc_energy:
diff --git a/code/environment.yml b/code/environment.yml
index 31a3487..47a996c 100644
--- a/code/environment.yml
+++ b/code/environment.yml
@@ -8,9 +8,9 @@ dependencies:
 - patchelf
 - pip:
     - -f https://download.pytorch.org/whl/torch_stable.html
-    - numpy
+    - numpy==1.23.0
     - gym==0.18.0
-    - mujoco-py==2.0.2.13
+    - mujoco-py
     - matplotlib==3.3.4
     - torch==1.9.1+cu111
     - typed-argument-parserdiff --git a/code/config/locomotion_config.py b/code/config/locomotion_config.py
index 90173b3..9895e57 100644
--- a/code/config/locomotion_config.py
+++ b/code/config/locomotion_config.py
@@ -6,13 +6,15 @@ class Config(ParamsProto):
     # misc
     seed = 100
     device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
-    bucket = '/home/aajay/weights/'
+    bucket = '/home/yanyangji/research/RL/final/decision-diffuser/code/output'
     dataset = 'hopper-medium-expert-v2'
 
     ## model
-    model = 'models.TemporalUnet'
+    # model = 'models.TemporalUnet'
+    model = 'models.MLPnet'
     diffusion = 'models.GaussianInvDynDiffusion'
-    horizon = 100
+    # horizon = 100
+    horizon = 1
     n_diffusion_steps = 200
     action_weight = 10
     loss_weights = None
diff --git a/code/diffuser/datasets/buffer.py b/code/diffuser/datasets/buffer.py
index 42bc734..29edcd8 100644
--- a/code/diffuser/datasets/buffer.py
+++ b/code/diffuser/datasets/buffer.py
@@ -9,7 +9,7 @@ class ReplayBuffer:
 
     def __init__(self, max_n_episodes, max_path_length, termination_penalty):
         self._dict = {
-            'path_lengths': np.zeros(max_n_episodes, dtype=np.int),
+            'path_lengths': np.zeros(max_n_episodes, dtype=np.int32),
         }
         self._count = 0
         self.max_n_episodes = max_n_episodes
diff --git a/code/diffuser/datasets/sequence.py b/code/diffuser/datasets/sequence.py
index 31ac84f..f256554 100644
--- a/code/diffuser/datasets/sequence.py
+++ b/code/diffuser/datasets/sequence.py
@@ -76,7 +76,8 @@ class SequenceDataset(torch.utils.data.Dataset):
         '''
             condition on current observation for planning
         '''
-        return {0: observations[0]}
+        # return {0: observations[0]}
+        return observations[0]
 
     def __len__(self):
         return len(self.indices)
diff --git a/code/diffuser/models/diffusion.py b/code/diffuser/models/diffusion.py
index a1f74ea..0c8f1a1 100644
--- a/code/diffuser/models/diffusion.py
+++ b/code/diffuser/models/diffusion.py
@@ -3,6 +3,8 @@ import torch
 from torch import nn
 import torch.nn.functional as F
 import pdb
+import clip
+from config.locomotion_config import Config
 
 import diffuser.utils as utils
 from .helpers import (
@@ -295,12 +297,22 @@ class GaussianInvDynDiffusion(nn.Module):
         condition_guidance_w=0.1, ar_inv=False, train_only_inv=False):
         super().__init__()
         self.horizon = horizon
-        self.observation_dim = observation_dim
-        self.action_dim = action_dim
+        self.observation_dim = observation_dim # 11
+        self.action_dim = action_dim # 3
         self.transition_dim = observation_dim + action_dim
         self.model = model
         self.ar_inv = ar_inv
         self.train_only_inv = train_only_inv
+
+        # Text embedding
+        device = Config.device
+        self.clip_model, _ = clip.load("ViT-B/32", device=device)
+        for p in self.clip_model.parameters():
+            p.requires_grad = False
+        with torch.no_grad():
+            self.text_cond = self.clip_model.encode_text(clip.tokenize('the man is hopping').to(device))
+        self.proj_layer = nn.Linear(512, self.observation_dim)
+
         if self.ar_inv:
             self.inv_model = ARInvModel(hidden_dim=hidden_dim, observation_dim=observation_dim, action_dim=action_dim)
         else:
diff --git a/code/diffuser/models/helpers.py b/code/diffuser/models/helpers.py
index 53859d4..7384d2a 100644
--- a/code/diffuser/models/helpers.py
+++ b/code/diffuser/models/helpers.py
@@ -91,8 +91,8 @@ def cosine_beta_schedule(timesteps, s=0.008, dtype=torch.float32):
     return torch.tensor(betas_clipped, dtype=dtype)
 
 def apply_conditioning(x, conditions, action_dim):
-    for t, val in conditions.items():
-        x[:, t, action_dim:] = val.clone()
+    # for t, val in conditions.items():
+    #     x[:, t, action_dim:] = val.clone() # batch size, time step, trajectory (all observation)
     return x
 
 #-----------------------------------------------------------------------------#
diff --git a/code/diffuser/models/temporal.py b/code/diffuser/models/temporal.py
index 63155c2..b635650 100644
--- a/code/diffuser/models/temporal.py
+++ b/code/diffuser/models/temporal.py
@@ -325,6 +325,7 @@ class MLPnet(nn.Module):
         self.calc_energy = calc_energy
         self.transition_dim = transition_dim
         self.action_dim = transition_dim - cond_dim
+        # self.action_dim = 3
 
         if self.returns_condition:
             self.returns_mlp = nn.Sequential(
@@ -365,8 +366,8 @@ class MLPnet(nn.Module):
             if force_dropout:
                 returns_embed = 0*returns_embed
             t = torch.cat([t, returns_embed], dim=-1)
-
         inp = torch.cat([t, cond, x], dim=-1)
+        # inp = torch.cat([t.unsqueeze(1), cond.unsqueez(1), x], dim=-1)
         out  = self.mlp(inp)
 
         if self.calc_energy:
diff --git a/code/diffuser/utils/training.py b/code/diffuser/utils/training.py
index e8dcdb4..a4200ea 100644
--- a/code/diffuser/utils/training.py
+++ b/code/diffuser/utils/training.py
@@ -113,6 +113,7 @@ class Trainer(object):
             for i in range(self.gradient_accumulate_every):
                 batch = next(self.dataloader)
                 batch = batch_to_device(batch, device=self.device)
+                breakpoint()
                 loss, infos = self.model.loss(*batch)
                 loss = loss / self.gradient_accumulate_every
                 loss.backward()
diff --git a/code/environment.yml b/code/environment.yml
index 31a3487..47a996c 100644
--- a/code/environment.yml
+++ b/code/environment.yml
@@ -8,9 +8,9 @@ dependencies:
 - patchelf
 - pip:
     - -f https://download.pytorch.org/whl/torch_stable.html
-    - numpy
+    - numpy==1.23.0
     - gym==0.18.0
-    - mujoco-py==2.0.2.13
+    - mujoco-py
     - matplotlib==3.3.4
     - torch==1.9.1+cu111
     - typed-argument-parserdiff --git a/code/config/locomotion_config.py b/code/config/locomotion_config.py
index 90173b3..65b8bbf 100644
--- a/code/config/locomotion_config.py
+++ b/code/config/locomotion_config.py
@@ -6,13 +6,15 @@ class Config(ParamsProto):
     # misc
     seed = 100
     device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
-    bucket = '/home/aajay/weights/'
+    bucket = '/home/yanyangji/research/RL/final/decision-diffuser/code/output'
     dataset = 'hopper-medium-expert-v2'
 
     ## model
     model = 'models.TemporalUnet'
+    # model = 'models.MLPnet'
     diffusion = 'models.GaussianInvDynDiffusion'
     horizon = 100
+    # horizon = 1
     n_diffusion_steps = 200
     action_weight = 10
     loss_weights = None
diff --git a/code/diffuser/datasets/buffer.py b/code/diffuser/datasets/buffer.py
index 42bc734..29edcd8 100644
--- a/code/diffuser/datasets/buffer.py
+++ b/code/diffuser/datasets/buffer.py
@@ -9,7 +9,7 @@ class ReplayBuffer:
 
     def __init__(self, max_n_episodes, max_path_length, termination_penalty):
         self._dict = {
-            'path_lengths': np.zeros(max_n_episodes, dtype=np.int),
+            'path_lengths': np.zeros(max_n_episodes, dtype=np.int32),
         }
         self._count = 0
         self.max_n_episodes = max_n_episodes
diff --git a/code/diffuser/datasets/sequence.py b/code/diffuser/datasets/sequence.py
index 31ac84f..f256554 100644
--- a/code/diffuser/datasets/sequence.py
+++ b/code/diffuser/datasets/sequence.py
@@ -76,7 +76,8 @@ class SequenceDataset(torch.utils.data.Dataset):
         '''
             condition on current observation for planning
         '''
-        return {0: observations[0]}
+        # return {0: observations[0]}
+        return observations[0]
 
     def __len__(self):
         return len(self.indices)
diff --git a/code/diffuser/models/diffusion.py b/code/diffuser/models/diffusion.py
index a1f74ea..0c8f1a1 100644
--- a/code/diffuser/models/diffusion.py
+++ b/code/diffuser/models/diffusion.py
@@ -3,6 +3,8 @@ import torch
 from torch import nn
 import torch.nn.functional as F
 import pdb
+import clip
+from config.locomotion_config import Config
 
 import diffuser.utils as utils
 from .helpers import (
@@ -295,12 +297,22 @@ class GaussianInvDynDiffusion(nn.Module):
         condition_guidance_w=0.1, ar_inv=False, train_only_inv=False):
         super().__init__()
         self.horizon = horizon
-        self.observation_dim = observation_dim
-        self.action_dim = action_dim
+        self.observation_dim = observation_dim # 11
+        self.action_dim = action_dim # 3
         self.transition_dim = observation_dim + action_dim
         self.model = model
         self.ar_inv = ar_inv
         self.train_only_inv = train_only_inv
+
+        # Text embedding
+        device = Config.device
+        self.clip_model, _ = clip.load("ViT-B/32", device=device)
+        for p in self.clip_model.parameters():
+            p.requires_grad = False
+        with torch.no_grad():
+            self.text_cond = self.clip_model.encode_text(clip.tokenize('the man is hopping').to(device))
+        self.proj_layer = nn.Linear(512, self.observation_dim)
+
         if self.ar_inv:
             self.inv_model = ARInvModel(hidden_dim=hidden_dim, observation_dim=observation_dim, action_dim=action_dim)
         else:
diff --git a/code/diffuser/models/helpers.py b/code/diffuser/models/helpers.py
index 53859d4..7384d2a 100644
--- a/code/diffuser/models/helpers.py
+++ b/code/diffuser/models/helpers.py
@@ -91,8 +91,8 @@ def cosine_beta_schedule(timesteps, s=0.008, dtype=torch.float32):
     return torch.tensor(betas_clipped, dtype=dtype)
 
 def apply_conditioning(x, conditions, action_dim):
-    for t, val in conditions.items():
-        x[:, t, action_dim:] = val.clone()
+    # for t, val in conditions.items():
+    #     x[:, t, action_dim:] = val.clone() # batch size, time step, trajectory (all observation)
     return x
 
 #-----------------------------------------------------------------------------#
diff --git a/code/diffuser/models/temporal.py b/code/diffuser/models/temporal.py
index 63155c2..b635650 100644
--- a/code/diffuser/models/temporal.py
+++ b/code/diffuser/models/temporal.py
@@ -325,6 +325,7 @@ class MLPnet(nn.Module):
         self.calc_energy = calc_energy
         self.transition_dim = transition_dim
         self.action_dim = transition_dim - cond_dim
+        # self.action_dim = 3
 
         if self.returns_condition:
             self.returns_mlp = nn.Sequential(
@@ -365,8 +366,8 @@ class MLPnet(nn.Module):
             if force_dropout:
                 returns_embed = 0*returns_embed
             t = torch.cat([t, returns_embed], dim=-1)
-
         inp = torch.cat([t, cond, x], dim=-1)
+        # inp = torch.cat([t.unsqueeze(1), cond.unsqueez(1), x], dim=-1)
         out  = self.mlp(inp)
 
         if self.calc_energy:
diff --git a/code/diffuser/utils/training.py b/code/diffuser/utils/training.py
index e8dcdb4..a4200ea 100644
--- a/code/diffuser/utils/training.py
+++ b/code/diffuser/utils/training.py
@@ -113,6 +113,7 @@ class Trainer(object):
             for i in range(self.gradient_accumulate_every):
                 batch = next(self.dataloader)
                 batch = batch_to_device(batch, device=self.device)
+                breakpoint()
                 loss, infos = self.model.loss(*batch)
                 loss = loss / self.gradient_accumulate_every
                 loss.backward()
diff --git a/code/environment.yml b/code/environment.yml
index 31a3487..47a996c 100644
--- a/code/environment.yml
+++ b/code/environment.yml
@@ -8,9 +8,9 @@ dependencies:
 - patchelf
 - pip:
     - -f https://download.pytorch.org/whl/torch_stable.html
-    - numpy
+    - numpy==1.23.0
     - gym==0.18.0
-    - mujoco-py==2.0.2.13
+    - mujoco-py
     - matplotlib==3.3.4
     - torch==1.9.1+cu111
     - typed-argument-parserdiff --git a/code/config/locomotion_config.py b/code/config/locomotion_config.py
index 90173b3..65b8bbf 100644
--- a/code/config/locomotion_config.py
+++ b/code/config/locomotion_config.py
@@ -6,13 +6,15 @@ class Config(ParamsProto):
     # misc
     seed = 100
     device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
-    bucket = '/home/aajay/weights/'
+    bucket = '/home/yanyangji/research/RL/final/decision-diffuser/code/output'
     dataset = 'hopper-medium-expert-v2'
 
     ## model
     model = 'models.TemporalUnet'
+    # model = 'models.MLPnet'
     diffusion = 'models.GaussianInvDynDiffusion'
     horizon = 100
+    # horizon = 1
     n_diffusion_steps = 200
     action_weight = 10
     loss_weights = None
diff --git a/code/diffuser/datasets/buffer.py b/code/diffuser/datasets/buffer.py
index 42bc734..29edcd8 100644
--- a/code/diffuser/datasets/buffer.py
+++ b/code/diffuser/datasets/buffer.py
@@ -9,7 +9,7 @@ class ReplayBuffer:
 
     def __init__(self, max_n_episodes, max_path_length, termination_penalty):
         self._dict = {
-            'path_lengths': np.zeros(max_n_episodes, dtype=np.int),
+            'path_lengths': np.zeros(max_n_episodes, dtype=np.int32),
         }
         self._count = 0
         self.max_n_episodes = max_n_episodes
diff --git a/code/diffuser/datasets/sequence.py b/code/diffuser/datasets/sequence.py
index 31ac84f..f256554 100644
--- a/code/diffuser/datasets/sequence.py
+++ b/code/diffuser/datasets/sequence.py
@@ -76,7 +76,8 @@ class SequenceDataset(torch.utils.data.Dataset):
         '''
             condition on current observation for planning
         '''
-        return {0: observations[0]}
+        # return {0: observations[0]}
+        return observations[0]
 
     def __len__(self):
         return len(self.indices)
diff --git a/code/diffuser/models/diffusion.py b/code/diffuser/models/diffusion.py
index a1f74ea..87a0a31 100644
--- a/code/diffuser/models/diffusion.py
+++ b/code/diffuser/models/diffusion.py
@@ -3,6 +3,8 @@ import torch
 from torch import nn
 import torch.nn.functional as F
 import pdb
+import clip
+from config.locomotion_config import Config
 
 import diffuser.utils as utils
 from .helpers import (
@@ -295,12 +297,22 @@ class GaussianInvDynDiffusion(nn.Module):
         condition_guidance_w=0.1, ar_inv=False, train_only_inv=False):
         super().__init__()
         self.horizon = horizon
-        self.observation_dim = observation_dim
-        self.action_dim = action_dim
+        self.observation_dim = observation_dim # 11
+        self.action_dim = action_dim # 3
         self.transition_dim = observation_dim + action_dim
         self.model = model
         self.ar_inv = ar_inv
         self.train_only_inv = train_only_inv
+
+        # Text embedding
+        device = Config.device
+        self.clip_model, _ = clip.load("ViT-B/32", device=device)
+        for p in self.clip_model.parameters():
+            p.requires_grad = False
+        with torch.no_grad():
+            self.text_cond = self.clip_model.encode_text(clip.tokenize('the man is hopping').to(device))
+        self.proj_layer = nn.Linear(512, self.observation_dim)
+
         if self.ar_inv:
             self.inv_model = ARInvModel(hidden_dim=hidden_dim, observation_dim=observation_dim, action_dim=action_dim)
         else:
@@ -487,6 +499,7 @@ class GaussianInvDynDiffusion(nn.Module):
         x_noisy = apply_conditioning(x_noisy, cond, 0)
 
         x_recon = self.model(x_noisy, cond, t, returns)
+        print(x_recon.shape)
 
         if not self.predict_epsilon:
             x_recon = apply_conditioning(x_recon, cond, 0)
diff --git a/code/diffuser/models/helpers.py b/code/diffuser/models/helpers.py
index 53859d4..7384d2a 100644
--- a/code/diffuser/models/helpers.py
+++ b/code/diffuser/models/helpers.py
@@ -91,8 +91,8 @@ def cosine_beta_schedule(timesteps, s=0.008, dtype=torch.float32):
     return torch.tensor(betas_clipped, dtype=dtype)
 
 def apply_conditioning(x, conditions, action_dim):
-    for t, val in conditions.items():
-        x[:, t, action_dim:] = val.clone()
+    # for t, val in conditions.items():
+    #     x[:, t, action_dim:] = val.clone() # batch size, time step, trajectory (all observation)
     return x
 
 #-----------------------------------------------------------------------------#
diff --git a/code/diffuser/models/temporal.py b/code/diffuser/models/temporal.py
index 63155c2..b635650 100644
--- a/code/diffuser/models/temporal.py
+++ b/code/diffuser/models/temporal.py
@@ -325,6 +325,7 @@ class MLPnet(nn.Module):
         self.calc_energy = calc_energy
         self.transition_dim = transition_dim
         self.action_dim = transition_dim - cond_dim
+        # self.action_dim = 3
 
         if self.returns_condition:
             self.returns_mlp = nn.Sequential(
@@ -365,8 +366,8 @@ class MLPnet(nn.Module):
             if force_dropout:
                 returns_embed = 0*returns_embed
             t = torch.cat([t, returns_embed], dim=-1)
-
         inp = torch.cat([t, cond, x], dim=-1)
+        # inp = torch.cat([t.unsqueeze(1), cond.unsqueez(1), x], dim=-1)
         out  = self.mlp(inp)
 
         if self.calc_energy:
diff --git a/code/diffuser/utils/training.py b/code/diffuser/utils/training.py
index e8dcdb4..a4200ea 100644
--- a/code/diffuser/utils/training.py
+++ b/code/diffuser/utils/training.py
@@ -113,6 +113,7 @@ class Trainer(object):
             for i in range(self.gradient_accumulate_every):
                 batch = next(self.dataloader)
                 batch = batch_to_device(batch, device=self.device)
+                breakpoint()
                 loss, infos = self.model.loss(*batch)
                 loss = loss / self.gradient_accumulate_every
                 loss.backward()
diff --git a/code/environment.yml b/code/environment.yml
index 31a3487..47a996c 100644
--- a/code/environment.yml
+++ b/code/environment.yml
@@ -8,9 +8,9 @@ dependencies:
 - patchelf
 - pip:
     - -f https://download.pytorch.org/whl/torch_stable.html
-    - numpy
+    - numpy==1.23.0
     - gym==0.18.0
-    - mujoco-py==2.0.2.13
+    - mujoco-py
     - matplotlib==3.3.4
     - torch==1.9.1+cu111
     - typed-argument-parserdiff --git a/code/config/locomotion_config.py b/code/config/locomotion_config.py
index 90173b3..65b8bbf 100644
--- a/code/config/locomotion_config.py
+++ b/code/config/locomotion_config.py
@@ -6,13 +6,15 @@ class Config(ParamsProto):
     # misc
     seed = 100
     device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
-    bucket = '/home/aajay/weights/'
+    bucket = '/home/yanyangji/research/RL/final/decision-diffuser/code/output'
     dataset = 'hopper-medium-expert-v2'
 
     ## model
     model = 'models.TemporalUnet'
+    # model = 'models.MLPnet'
     diffusion = 'models.GaussianInvDynDiffusion'
     horizon = 100
+    # horizon = 1
     n_diffusion_steps = 200
     action_weight = 10
     loss_weights = None
diff --git a/code/diffuser/datasets/buffer.py b/code/diffuser/datasets/buffer.py
index 42bc734..29edcd8 100644
--- a/code/diffuser/datasets/buffer.py
+++ b/code/diffuser/datasets/buffer.py
@@ -9,7 +9,7 @@ class ReplayBuffer:
 
     def __init__(self, max_n_episodes, max_path_length, termination_penalty):
         self._dict = {
-            'path_lengths': np.zeros(max_n_episodes, dtype=np.int),
+            'path_lengths': np.zeros(max_n_episodes, dtype=np.int32),
         }
         self._count = 0
         self.max_n_episodes = max_n_episodes
diff --git a/code/diffuser/datasets/sequence.py b/code/diffuser/datasets/sequence.py
index 31ac84f..f256554 100644
--- a/code/diffuser/datasets/sequence.py
+++ b/code/diffuser/datasets/sequence.py
@@ -76,7 +76,8 @@ class SequenceDataset(torch.utils.data.Dataset):
         '''
             condition on current observation for planning
         '''
-        return {0: observations[0]}
+        # return {0: observations[0]}
+        return observations[0]
 
     def __len__(self):
         return len(self.indices)
diff --git a/code/diffuser/models/diffusion.py b/code/diffuser/models/diffusion.py
index a1f74ea..b92849b 100644
--- a/code/diffuser/models/diffusion.py
+++ b/code/diffuser/models/diffusion.py
@@ -3,6 +3,8 @@ import torch
 from torch import nn
 import torch.nn.functional as F
 import pdb
+import clip
+from config.locomotion_config import Config
 
 import diffuser.utils as utils
 from .helpers import (
@@ -295,12 +297,22 @@ class GaussianInvDynDiffusion(nn.Module):
         condition_guidance_w=0.1, ar_inv=False, train_only_inv=False):
         super().__init__()
         self.horizon = horizon
-        self.observation_dim = observation_dim
-        self.action_dim = action_dim
+        self.observation_dim = observation_dim # 11
+        self.action_dim = action_dim # 3
         self.transition_dim = observation_dim + action_dim
         self.model = model
         self.ar_inv = ar_inv
         self.train_only_inv = train_only_inv
+
+        # Text embedding
+        device = Config.device
+        self.clip_model, _ = clip.load("ViT-B/32", device=device)
+        for p in self.clip_model.parameters():
+            p.requires_grad = False
+        with torch.no_grad():
+            self.text_cond = self.clip_model.encode_text(clip.tokenize('the man is hopping').to(device))
+        self.proj_layer = nn.Linear(512, self.observation_dim)
+
         if self.ar_inv:
             self.inv_model = ARInvModel(hidden_dim=hidden_dim, observation_dim=observation_dim, action_dim=action_dim)
         else:
@@ -487,6 +499,8 @@ class GaussianInvDynDiffusion(nn.Module):
         x_noisy = apply_conditioning(x_noisy, cond, 0)
 
         x_recon = self.model(x_noisy, cond, t, returns)
+        print(x_recon.shape)
+        breakpoint()
 
         if not self.predict_epsilon:
             x_recon = apply_conditioning(x_recon, cond, 0)
diff --git a/code/diffuser/models/helpers.py b/code/diffuser/models/helpers.py
index 53859d4..7384d2a 100644
--- a/code/diffuser/models/helpers.py
+++ b/code/diffuser/models/helpers.py
@@ -91,8 +91,8 @@ def cosine_beta_schedule(timesteps, s=0.008, dtype=torch.float32):
     return torch.tensor(betas_clipped, dtype=dtype)
 
 def apply_conditioning(x, conditions, action_dim):
-    for t, val in conditions.items():
-        x[:, t, action_dim:] = val.clone()
+    # for t, val in conditions.items():
+    #     x[:, t, action_dim:] = val.clone() # batch size, time step, trajectory (all observation)
     return x
 
 #-----------------------------------------------------------------------------#
diff --git a/code/diffuser/models/temporal.py b/code/diffuser/models/temporal.py
index 63155c2..b635650 100644
--- a/code/diffuser/models/temporal.py
+++ b/code/diffuser/models/temporal.py
@@ -325,6 +325,7 @@ class MLPnet(nn.Module):
         self.calc_energy = calc_energy
         self.transition_dim = transition_dim
         self.action_dim = transition_dim - cond_dim
+        # self.action_dim = 3
 
         if self.returns_condition:
             self.returns_mlp = nn.Sequential(
@@ -365,8 +366,8 @@ class MLPnet(nn.Module):
             if force_dropout:
                 returns_embed = 0*returns_embed
             t = torch.cat([t, returns_embed], dim=-1)
-
         inp = torch.cat([t, cond, x], dim=-1)
+        # inp = torch.cat([t.unsqueeze(1), cond.unsqueez(1), x], dim=-1)
         out  = self.mlp(inp)
 
         if self.calc_energy:
diff --git a/code/diffuser/utils/training.py b/code/diffuser/utils/training.py
index e8dcdb4..a4200ea 100644
--- a/code/diffuser/utils/training.py
+++ b/code/diffuser/utils/training.py
@@ -113,6 +113,7 @@ class Trainer(object):
             for i in range(self.gradient_accumulate_every):
                 batch = next(self.dataloader)
                 batch = batch_to_device(batch, device=self.device)
+                breakpoint()
                 loss, infos = self.model.loss(*batch)
                 loss = loss / self.gradient_accumulate_every
                 loss.backward()
diff --git a/code/environment.yml b/code/environment.yml
index 31a3487..47a996c 100644
--- a/code/environment.yml
+++ b/code/environment.yml
@@ -8,9 +8,9 @@ dependencies:
 - patchelf
 - pip:
     - -f https://download.pytorch.org/whl/torch_stable.html
-    - numpy
+    - numpy==1.23.0
     - gym==0.18.0
-    - mujoco-py==2.0.2.13
+    - mujoco-py
     - matplotlib==3.3.4
     - torch==1.9.1+cu111
     - typed-argument-parserdiff --git a/code/config/locomotion_config.py b/code/config/locomotion_config.py
index 90173b3..65b8bbf 100644
--- a/code/config/locomotion_config.py
+++ b/code/config/locomotion_config.py
@@ -6,13 +6,15 @@ class Config(ParamsProto):
     # misc
     seed = 100
     device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
-    bucket = '/home/aajay/weights/'
+    bucket = '/home/yanyangji/research/RL/final/decision-diffuser/code/output'
     dataset = 'hopper-medium-expert-v2'
 
     ## model
     model = 'models.TemporalUnet'
+    # model = 'models.MLPnet'
     diffusion = 'models.GaussianInvDynDiffusion'
     horizon = 100
+    # horizon = 1
     n_diffusion_steps = 200
     action_weight = 10
     loss_weights = None
diff --git a/code/diffuser/datasets/buffer.py b/code/diffuser/datasets/buffer.py
index 42bc734..29edcd8 100644
--- a/code/diffuser/datasets/buffer.py
+++ b/code/diffuser/datasets/buffer.py
@@ -9,7 +9,7 @@ class ReplayBuffer:
 
     def __init__(self, max_n_episodes, max_path_length, termination_penalty):
         self._dict = {
-            'path_lengths': np.zeros(max_n_episodes, dtype=np.int),
+            'path_lengths': np.zeros(max_n_episodes, dtype=np.int32),
         }
         self._count = 0
         self.max_n_episodes = max_n_episodes
diff --git a/code/diffuser/datasets/sequence.py b/code/diffuser/datasets/sequence.py
index 31ac84f..f256554 100644
--- a/code/diffuser/datasets/sequence.py
+++ b/code/diffuser/datasets/sequence.py
@@ -76,7 +76,8 @@ class SequenceDataset(torch.utils.data.Dataset):
         '''
             condition on current observation for planning
         '''
-        return {0: observations[0]}
+        # return {0: observations[0]}
+        return observations[0]
 
     def __len__(self):
         return len(self.indices)
diff --git a/code/diffuser/models/diffusion.py b/code/diffuser/models/diffusion.py
index a1f74ea..0c8f1a1 100644
--- a/code/diffuser/models/diffusion.py
+++ b/code/diffuser/models/diffusion.py
@@ -3,6 +3,8 @@ import torch
 from torch import nn
 import torch.nn.functional as F
 import pdb
+import clip
+from config.locomotion_config import Config
 
 import diffuser.utils as utils
 from .helpers import (
@@ -295,12 +297,22 @@ class GaussianInvDynDiffusion(nn.Module):
         condition_guidance_w=0.1, ar_inv=False, train_only_inv=False):
         super().__init__()
         self.horizon = horizon
-        self.observation_dim = observation_dim
-        self.action_dim = action_dim
+        self.observation_dim = observation_dim # 11
+        self.action_dim = action_dim # 3
         self.transition_dim = observation_dim + action_dim
         self.model = model
         self.ar_inv = ar_inv
         self.train_only_inv = train_only_inv
+
+        # Text embedding
+        device = Config.device
+        self.clip_model, _ = clip.load("ViT-B/32", device=device)
+        for p in self.clip_model.parameters():
+            p.requires_grad = False
+        with torch.no_grad():
+            self.text_cond = self.clip_model.encode_text(clip.tokenize('the man is hopping').to(device))
+        self.proj_layer = nn.Linear(512, self.observation_dim)
+
         if self.ar_inv:
             self.inv_model = ARInvModel(hidden_dim=hidden_dim, observation_dim=observation_dim, action_dim=action_dim)
         else:
diff --git a/code/diffuser/models/helpers.py b/code/diffuser/models/helpers.py
index 53859d4..7384d2a 100644
--- a/code/diffuser/models/helpers.py
+++ b/code/diffuser/models/helpers.py
@@ -91,8 +91,8 @@ def cosine_beta_schedule(timesteps, s=0.008, dtype=torch.float32):
     return torch.tensor(betas_clipped, dtype=dtype)
 
 def apply_conditioning(x, conditions, action_dim):
-    for t, val in conditions.items():
-        x[:, t, action_dim:] = val.clone()
+    # for t, val in conditions.items():
+    #     x[:, t, action_dim:] = val.clone() # batch size, time step, trajectory (all observation)
     return x
 
 #-----------------------------------------------------------------------------#
diff --git a/code/diffuser/models/temporal.py b/code/diffuser/models/temporal.py
index 63155c2..b635650 100644
--- a/code/diffuser/models/temporal.py
+++ b/code/diffuser/models/temporal.py
@@ -325,6 +325,7 @@ class MLPnet(nn.Module):
         self.calc_energy = calc_energy
         self.transition_dim = transition_dim
         self.action_dim = transition_dim - cond_dim
+        # self.action_dim = 3
 
         if self.returns_condition:
             self.returns_mlp = nn.Sequential(
@@ -365,8 +366,8 @@ class MLPnet(nn.Module):
             if force_dropout:
                 returns_embed = 0*returns_embed
             t = torch.cat([t, returns_embed], dim=-1)
-
         inp = torch.cat([t, cond, x], dim=-1)
+        # inp = torch.cat([t.unsqueeze(1), cond.unsqueez(1), x], dim=-1)
         out  = self.mlp(inp)
 
         if self.calc_energy:
diff --git a/code/environment.yml b/code/environment.yml
index 31a3487..47a996c 100644
--- a/code/environment.yml
+++ b/code/environment.yml
@@ -8,9 +8,9 @@ dependencies:
 - patchelf
 - pip:
     - -f https://download.pytorch.org/whl/torch_stable.html
-    - numpy
+    - numpy==1.23.0
     - gym==0.18.0
-    - mujoco-py==2.0.2.13
+    - mujoco-py
     - matplotlib==3.3.4
     - torch==1.9.1+cu111
     - typed-argument-parserdiff --git a/code/config/locomotion_config.py b/code/config/locomotion_config.py
index 90173b3..b56a675 100644
--- a/code/config/locomotion_config.py
+++ b/code/config/locomotion_config.py
@@ -6,13 +6,15 @@ class Config(ParamsProto):
     # misc
     seed = 100
     device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
-    bucket = '/home/aajay/weights/'
+    bucket = '/home/yanyangji/research/RL/final/decision-diffuser/code/output'
     dataset = 'hopper-medium-expert-v2'
 
     ## model
-    model = 'models.TemporalUnet'
+    # model = 'models.TemporalUnet'
+    model = 'models.MLPnet'
     diffusion = 'models.GaussianInvDynDiffusion'
     horizon = 100
+    # horizon = 1
     n_diffusion_steps = 200
     action_weight = 10
     loss_weights = None
diff --git a/code/diffuser/datasets/buffer.py b/code/diffuser/datasets/buffer.py
index 42bc734..29edcd8 100644
--- a/code/diffuser/datasets/buffer.py
+++ b/code/diffuser/datasets/buffer.py
@@ -9,7 +9,7 @@ class ReplayBuffer:
 
     def __init__(self, max_n_episodes, max_path_length, termination_penalty):
         self._dict = {
-            'path_lengths': np.zeros(max_n_episodes, dtype=np.int),
+            'path_lengths': np.zeros(max_n_episodes, dtype=np.int32),
         }
         self._count = 0
         self.max_n_episodes = max_n_episodes
diff --git a/code/diffuser/datasets/sequence.py b/code/diffuser/datasets/sequence.py
index 31ac84f..f256554 100644
--- a/code/diffuser/datasets/sequence.py
+++ b/code/diffuser/datasets/sequence.py
@@ -76,7 +76,8 @@ class SequenceDataset(torch.utils.data.Dataset):
         '''
             condition on current observation for planning
         '''
-        return {0: observations[0]}
+        # return {0: observations[0]}
+        return observations[0]
 
     def __len__(self):
         return len(self.indices)
diff --git a/code/diffuser/models/diffusion.py b/code/diffuser/models/diffusion.py
index a1f74ea..0c8f1a1 100644
--- a/code/diffuser/models/diffusion.py
+++ b/code/diffuser/models/diffusion.py
@@ -3,6 +3,8 @@ import torch
 from torch import nn
 import torch.nn.functional as F
 import pdb
+import clip
+from config.locomotion_config import Config
 
 import diffuser.utils as utils
 from .helpers import (
@@ -295,12 +297,22 @@ class GaussianInvDynDiffusion(nn.Module):
         condition_guidance_w=0.1, ar_inv=False, train_only_inv=False):
         super().__init__()
         self.horizon = horizon
-        self.observation_dim = observation_dim
-        self.action_dim = action_dim
+        self.observation_dim = observation_dim # 11
+        self.action_dim = action_dim # 3
         self.transition_dim = observation_dim + action_dim
         self.model = model
         self.ar_inv = ar_inv
         self.train_only_inv = train_only_inv
+
+        # Text embedding
+        device = Config.device
+        self.clip_model, _ = clip.load("ViT-B/32", device=device)
+        for p in self.clip_model.parameters():
+            p.requires_grad = False
+        with torch.no_grad():
+            self.text_cond = self.clip_model.encode_text(clip.tokenize('the man is hopping').to(device))
+        self.proj_layer = nn.Linear(512, self.observation_dim)
+
         if self.ar_inv:
             self.inv_model = ARInvModel(hidden_dim=hidden_dim, observation_dim=observation_dim, action_dim=action_dim)
         else:
diff --git a/code/diffuser/models/helpers.py b/code/diffuser/models/helpers.py
index 53859d4..7384d2a 100644
--- a/code/diffuser/models/helpers.py
+++ b/code/diffuser/models/helpers.py
@@ -91,8 +91,8 @@ def cosine_beta_schedule(timesteps, s=0.008, dtype=torch.float32):
     return torch.tensor(betas_clipped, dtype=dtype)
 
 def apply_conditioning(x, conditions, action_dim):
-    for t, val in conditions.items():
-        x[:, t, action_dim:] = val.clone()
+    # for t, val in conditions.items():
+    #     x[:, t, action_dim:] = val.clone() # batch size, time step, trajectory (all observation)
     return x
 
 #-----------------------------------------------------------------------------#
diff --git a/code/diffuser/models/temporal.py b/code/diffuser/models/temporal.py
index 63155c2..510ad68 100644
--- a/code/diffuser/models/temporal.py
+++ b/code/diffuser/models/temporal.py
@@ -325,6 +325,9 @@ class MLPnet(nn.Module):
         self.calc_energy = calc_energy
         self.transition_dim = transition_dim
         self.action_dim = transition_dim - cond_dim
+        self.cond_dim = cond_dim
+        breakpoint()
+        # self.action_dim = 3
 
         if self.returns_condition:
             self.returns_mlp = nn.Sequential(
@@ -365,8 +368,8 @@ class MLPnet(nn.Module):
             if force_dropout:
                 returns_embed = 0*returns_embed
             t = torch.cat([t, returns_embed], dim=-1)
-
         inp = torch.cat([t, cond, x], dim=-1)
+        # inp = torch.cat([t.unsqueeze(1), cond.unsqueez(1), x], dim=-1)
         out  = self.mlp(inp)
 
         if self.calc_energy:
diff --git a/code/environment.yml b/code/environment.yml
index 31a3487..47a996c 100644
--- a/code/environment.yml
+++ b/code/environment.yml
@@ -8,9 +8,9 @@ dependencies:
 - patchelf
 - pip:
     - -f https://download.pytorch.org/whl/torch_stable.html
-    - numpy
+    - numpy==1.23.0
     - gym==0.18.0
-    - mujoco-py==2.0.2.13
+    - mujoco-py
     - matplotlib==3.3.4
     - torch==1.9.1+cu111
     - typed-argument-parserdiff --git a/code/config/locomotion_config.py b/code/config/locomotion_config.py
index 90173b3..b79b7e6 100644
--- a/code/config/locomotion_config.py
+++ b/code/config/locomotion_config.py
@@ -6,7 +6,7 @@ class Config(ParamsProto):
     # misc
     seed = 100
     device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
-    bucket = '/home/aajay/weights/'
+    bucket = '/home/yanyangji/research/RL/final/decision-diffuser/code/output'
     dataset = 'hopper-medium-expert-v2'
 
     ## model
diff --git a/code/diffuser/datasets/buffer.py b/code/diffuser/datasets/buffer.py
index 42bc734..29edcd8 100644
--- a/code/diffuser/datasets/buffer.py
+++ b/code/diffuser/datasets/buffer.py
@@ -9,7 +9,7 @@ class ReplayBuffer:
 
     def __init__(self, max_n_episodes, max_path_length, termination_penalty):
         self._dict = {
-            'path_lengths': np.zeros(max_n_episodes, dtype=np.int),
+            'path_lengths': np.zeros(max_n_episodes, dtype=np.int32),
         }
         self._count = 0
         self.max_n_episodes = max_n_episodes
diff --git a/code/diffuser/datasets/sequence.py b/code/diffuser/datasets/sequence.py
index 31ac84f..f256554 100644
--- a/code/diffuser/datasets/sequence.py
+++ b/code/diffuser/datasets/sequence.py
@@ -76,7 +76,8 @@ class SequenceDataset(torch.utils.data.Dataset):
         '''
             condition on current observation for planning
         '''
-        return {0: observations[0]}
+        # return {0: observations[0]}
+        return observations[0]
 
     def __len__(self):
         return len(self.indices)
diff --git a/code/diffuser/models/diffusion.py b/code/diffuser/models/diffusion.py
index a1f74ea..0c8f1a1 100644
--- a/code/diffuser/models/diffusion.py
+++ b/code/diffuser/models/diffusion.py
@@ -3,6 +3,8 @@ import torch
 from torch import nn
 import torch.nn.functional as F
 import pdb
+import clip
+from config.locomotion_config import Config
 
 import diffuser.utils as utils
 from .helpers import (
@@ -295,12 +297,22 @@ class GaussianInvDynDiffusion(nn.Module):
         condition_guidance_w=0.1, ar_inv=False, train_only_inv=False):
         super().__init__()
         self.horizon = horizon
-        self.observation_dim = observation_dim
-        self.action_dim = action_dim
+        self.observation_dim = observation_dim # 11
+        self.action_dim = action_dim # 3
         self.transition_dim = observation_dim + action_dim
         self.model = model
         self.ar_inv = ar_inv
         self.train_only_inv = train_only_inv
+
+        # Text embedding
+        device = Config.device
+        self.clip_model, _ = clip.load("ViT-B/32", device=device)
+        for p in self.clip_model.parameters():
+            p.requires_grad = False
+        with torch.no_grad():
+            self.text_cond = self.clip_model.encode_text(clip.tokenize('the man is hopping').to(device))
+        self.proj_layer = nn.Linear(512, self.observation_dim)
+
         if self.ar_inv:
             self.inv_model = ARInvModel(hidden_dim=hidden_dim, observation_dim=observation_dim, action_dim=action_dim)
         else:
diff --git a/code/diffuser/models/helpers.py b/code/diffuser/models/helpers.py
index 53859d4..7384d2a 100644
--- a/code/diffuser/models/helpers.py
+++ b/code/diffuser/models/helpers.py
@@ -91,8 +91,8 @@ def cosine_beta_schedule(timesteps, s=0.008, dtype=torch.float32):
     return torch.tensor(betas_clipped, dtype=dtype)
 
 def apply_conditioning(x, conditions, action_dim):
-    for t, val in conditions.items():
-        x[:, t, action_dim:] = val.clone()
+    # for t, val in conditions.items():
+    #     x[:, t, action_dim:] = val.clone() # batch size, time step, trajectory (all observation)
     return x
 
 #-----------------------------------------------------------------------------#
diff --git a/code/diffuser/models/temporal.py b/code/diffuser/models/temporal.py
index 63155c2..7ad7bfc 100644
--- a/code/diffuser/models/temporal.py
+++ b/code/diffuser/models/temporal.py
@@ -88,6 +88,12 @@ class ResidualTemporalBlock(nn.Module):
             nn.Linear(embed_dim, out_channels),
             Rearrange('batch t -> batch t 1'),
         )
+        
+        self.cond_mlp = nn.Sequential(
+            act_fn,
+            nn.Linear(1, 1),
+            Rearrange(),
+        )
 
         self.residual_conv = nn.Conv1d(inp_channels, out_channels, 1) \
             if inp_channels != out_channels else nn.Identity()
@@ -99,6 +105,7 @@ class ResidualTemporalBlock(nn.Module):
             returns:
             out : [ batch_size x out_channels x horizon ]
         '''
+        breakpoint()
         out = self.blocks[0](x) + self.time_mlp(t)
         out = self.blocks[1](out)
 
@@ -325,6 +332,9 @@ class MLPnet(nn.Module):
         self.calc_energy = calc_energy
         self.transition_dim = transition_dim
         self.action_dim = transition_dim - cond_dim
+        self.cond_dim = cond_dim
+        breakpoint()
+        # self.action_dim = 3
 
         if self.returns_condition:
             self.returns_mlp = nn.Sequential(
@@ -365,8 +375,8 @@ class MLPnet(nn.Module):
             if force_dropout:
                 returns_embed = 0*returns_embed
             t = torch.cat([t, returns_embed], dim=-1)
-
         inp = torch.cat([t, cond, x], dim=-1)
+        # inp = torch.cat([t.unsqueeze(1), cond.unsqueez(1), x], dim=-1)
         out  = self.mlp(inp)
 
         if self.calc_energy:
diff --git a/code/environment.yml b/code/environment.yml
index 31a3487..47a996c 100644
--- a/code/environment.yml
+++ b/code/environment.yml
@@ -8,9 +8,9 @@ dependencies:
 - patchelf
 - pip:
     - -f https://download.pytorch.org/whl/torch_stable.html
-    - numpy
+    - numpy==1.23.0
     - gym==0.18.0
-    - mujoco-py==2.0.2.13
+    - mujoco-py
     - matplotlib==3.3.4
     - torch==1.9.1+cu111
     - typed-argument-parser
diff --git a/code/scripts/train.py b/code/scripts/train.py
index db05e5a..50dab51 100644
--- a/code/scripts/train.py
+++ b/code/scripts/train.py
@@ -59,7 +59,8 @@ def main(**deps):
             Config.model,
             savepath='model_config.pkl',
             horizon=Config.horizon,
-            transition_dim=observation_dim,
+            # transition_dim=observation_dim,
+            transition_dim=observation_dim + action_dim,
             cond_dim=observation_dim,
             dim_mults=Config.dim_mults,
             returns_condition=Config.returns_condition,diff --git a/code/config/locomotion_config.py b/code/config/locomotion_config.py
index 90173b3..b79b7e6 100644
--- a/code/config/locomotion_config.py
+++ b/code/config/locomotion_config.py
@@ -6,7 +6,7 @@ class Config(ParamsProto):
     # misc
     seed = 100
     device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
-    bucket = '/home/aajay/weights/'
+    bucket = '/home/yanyangji/research/RL/final/decision-diffuser/code/output'
     dataset = 'hopper-medium-expert-v2'
 
     ## model
diff --git a/code/diffuser/datasets/buffer.py b/code/diffuser/datasets/buffer.py
index 42bc734..29edcd8 100644
--- a/code/diffuser/datasets/buffer.py
+++ b/code/diffuser/datasets/buffer.py
@@ -9,7 +9,7 @@ class ReplayBuffer:
 
     def __init__(self, max_n_episodes, max_path_length, termination_penalty):
         self._dict = {
-            'path_lengths': np.zeros(max_n_episodes, dtype=np.int),
+            'path_lengths': np.zeros(max_n_episodes, dtype=np.int32),
         }
         self._count = 0
         self.max_n_episodes = max_n_episodes
diff --git a/code/diffuser/datasets/sequence.py b/code/diffuser/datasets/sequence.py
index 31ac84f..f256554 100644
--- a/code/diffuser/datasets/sequence.py
+++ b/code/diffuser/datasets/sequence.py
@@ -76,7 +76,8 @@ class SequenceDataset(torch.utils.data.Dataset):
         '''
             condition on current observation for planning
         '''
-        return {0: observations[0]}
+        # return {0: observations[0]}
+        return observations[0]
 
     def __len__(self):
         return len(self.indices)
diff --git a/code/diffuser/models/diffusion.py b/code/diffuser/models/diffusion.py
index a1f74ea..0c8f1a1 100644
--- a/code/diffuser/models/diffusion.py
+++ b/code/diffuser/models/diffusion.py
@@ -3,6 +3,8 @@ import torch
 from torch import nn
 import torch.nn.functional as F
 import pdb
+import clip
+from config.locomotion_config import Config
 
 import diffuser.utils as utils
 from .helpers import (
@@ -295,12 +297,22 @@ class GaussianInvDynDiffusion(nn.Module):
         condition_guidance_w=0.1, ar_inv=False, train_only_inv=False):
         super().__init__()
         self.horizon = horizon
-        self.observation_dim = observation_dim
-        self.action_dim = action_dim
+        self.observation_dim = observation_dim # 11
+        self.action_dim = action_dim # 3
         self.transition_dim = observation_dim + action_dim
         self.model = model
         self.ar_inv = ar_inv
         self.train_only_inv = train_only_inv
+
+        # Text embedding
+        device = Config.device
+        self.clip_model, _ = clip.load("ViT-B/32", device=device)
+        for p in self.clip_model.parameters():
+            p.requires_grad = False
+        with torch.no_grad():
+            self.text_cond = self.clip_model.encode_text(clip.tokenize('the man is hopping').to(device))
+        self.proj_layer = nn.Linear(512, self.observation_dim)
+
         if self.ar_inv:
             self.inv_model = ARInvModel(hidden_dim=hidden_dim, observation_dim=observation_dim, action_dim=action_dim)
         else:
diff --git a/code/diffuser/models/helpers.py b/code/diffuser/models/helpers.py
index 53859d4..7384d2a 100644
--- a/code/diffuser/models/helpers.py
+++ b/code/diffuser/models/helpers.py
@@ -91,8 +91,8 @@ def cosine_beta_schedule(timesteps, s=0.008, dtype=torch.float32):
     return torch.tensor(betas_clipped, dtype=dtype)
 
 def apply_conditioning(x, conditions, action_dim):
-    for t, val in conditions.items():
-        x[:, t, action_dim:] = val.clone()
+    # for t, val in conditions.items():
+    #     x[:, t, action_dim:] = val.clone() # batch size, time step, trajectory (all observation)
     return x
 
 #-----------------------------------------------------------------------------#
diff --git a/code/diffuser/models/temporal.py b/code/diffuser/models/temporal.py
index 63155c2..7d1bdce 100644
--- a/code/diffuser/models/temporal.py
+++ b/code/diffuser/models/temporal.py
@@ -88,6 +88,12 @@ class ResidualTemporalBlock(nn.Module):
             nn.Linear(embed_dim, out_channels),
             Rearrange('batch t -> batch t 1'),
         )
+        
+        # self.cond_mlp = nn.Sequential(
+        #     act_fn,
+        #     nn.Linear(1, 1),
+        #     Rearrange(),
+        # )
 
         self.residual_conv = nn.Conv1d(inp_channels, out_channels, 1) \
             if inp_channels != out_channels else nn.Identity()
@@ -325,6 +331,9 @@ class MLPnet(nn.Module):
         self.calc_energy = calc_energy
         self.transition_dim = transition_dim
         self.action_dim = transition_dim - cond_dim
+        self.cond_dim = cond_dim
+        breakpoint()
+        # self.action_dim = 3
 
         if self.returns_condition:
             self.returns_mlp = nn.Sequential(
@@ -365,8 +374,8 @@ class MLPnet(nn.Module):
             if force_dropout:
                 returns_embed = 0*returns_embed
             t = torch.cat([t, returns_embed], dim=-1)
-
         inp = torch.cat([t, cond, x], dim=-1)
+        # inp = torch.cat([t.unsqueeze(1), cond.unsqueez(1), x], dim=-1)
         out  = self.mlp(inp)
 
         if self.calc_energy:
diff --git a/code/environment.yml b/code/environment.yml
index 31a3487..47a996c 100644
--- a/code/environment.yml
+++ b/code/environment.yml
@@ -8,9 +8,9 @@ dependencies:
 - patchelf
 - pip:
     - -f https://download.pytorch.org/whl/torch_stable.html
-    - numpy
+    - numpy==1.23.0
     - gym==0.18.0
-    - mujoco-py==2.0.2.13
+    - mujoco-py
     - matplotlib==3.3.4
     - torch==1.9.1+cu111
     - typed-argument-parser
diff --git a/code/scripts/train.py b/code/scripts/train.py
index db05e5a..50dab51 100644
--- a/code/scripts/train.py
+++ b/code/scripts/train.py
@@ -59,7 +59,8 @@ def main(**deps):
             Config.model,
             savepath='model_config.pkl',
             horizon=Config.horizon,
-            transition_dim=observation_dim,
+            # transition_dim=observation_dim,
+            transition_dim=observation_dim + action_dim,
             cond_dim=observation_dim,
             dim_mults=Config.dim_mults,
             returns_condition=Config.returns_condition,diff --git a/code/config/locomotion_config.py b/code/config/locomotion_config.py
index 90173b3..b79b7e6 100644
--- a/code/config/locomotion_config.py
+++ b/code/config/locomotion_config.py
@@ -6,7 +6,7 @@ class Config(ParamsProto):
     # misc
     seed = 100
     device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
-    bucket = '/home/aajay/weights/'
+    bucket = '/home/yanyangji/research/RL/final/decision-diffuser/code/output'
     dataset = 'hopper-medium-expert-v2'
 
     ## model
diff --git a/code/diffuser/datasets/buffer.py b/code/diffuser/datasets/buffer.py
index 42bc734..29edcd8 100644
--- a/code/diffuser/datasets/buffer.py
+++ b/code/diffuser/datasets/buffer.py
@@ -9,7 +9,7 @@ class ReplayBuffer:
 
     def __init__(self, max_n_episodes, max_path_length, termination_penalty):
         self._dict = {
-            'path_lengths': np.zeros(max_n_episodes, dtype=np.int),
+            'path_lengths': np.zeros(max_n_episodes, dtype=np.int32),
         }
         self._count = 0
         self.max_n_episodes = max_n_episodes
diff --git a/code/diffuser/datasets/sequence.py b/code/diffuser/datasets/sequence.py
index 31ac84f..f256554 100644
--- a/code/diffuser/datasets/sequence.py
+++ b/code/diffuser/datasets/sequence.py
@@ -76,7 +76,8 @@ class SequenceDataset(torch.utils.data.Dataset):
         '''
             condition on current observation for planning
         '''
-        return {0: observations[0]}
+        # return {0: observations[0]}
+        return observations[0]
 
     def __len__(self):
         return len(self.indices)
diff --git a/code/diffuser/models/diffusion.py b/code/diffuser/models/diffusion.py
index a1f74ea..0c8f1a1 100644
--- a/code/diffuser/models/diffusion.py
+++ b/code/diffuser/models/diffusion.py
@@ -3,6 +3,8 @@ import torch
 from torch import nn
 import torch.nn.functional as F
 import pdb
+import clip
+from config.locomotion_config import Config
 
 import diffuser.utils as utils
 from .helpers import (
@@ -295,12 +297,22 @@ class GaussianInvDynDiffusion(nn.Module):
         condition_guidance_w=0.1, ar_inv=False, train_only_inv=False):
         super().__init__()
         self.horizon = horizon
-        self.observation_dim = observation_dim
-        self.action_dim = action_dim
+        self.observation_dim = observation_dim # 11
+        self.action_dim = action_dim # 3
         self.transition_dim = observation_dim + action_dim
         self.model = model
         self.ar_inv = ar_inv
         self.train_only_inv = train_only_inv
+
+        # Text embedding
+        device = Config.device
+        self.clip_model, _ = clip.load("ViT-B/32", device=device)
+        for p in self.clip_model.parameters():
+            p.requires_grad = False
+        with torch.no_grad():
+            self.text_cond = self.clip_model.encode_text(clip.tokenize('the man is hopping').to(device))
+        self.proj_layer = nn.Linear(512, self.observation_dim)
+
         if self.ar_inv:
             self.inv_model = ARInvModel(hidden_dim=hidden_dim, observation_dim=observation_dim, action_dim=action_dim)
         else:
diff --git a/code/diffuser/models/helpers.py b/code/diffuser/models/helpers.py
index 53859d4..7384d2a 100644
--- a/code/diffuser/models/helpers.py
+++ b/code/diffuser/models/helpers.py
@@ -91,8 +91,8 @@ def cosine_beta_schedule(timesteps, s=0.008, dtype=torch.float32):
     return torch.tensor(betas_clipped, dtype=dtype)
 
 def apply_conditioning(x, conditions, action_dim):
-    for t, val in conditions.items():
-        x[:, t, action_dim:] = val.clone()
+    # for t, val in conditions.items():
+    #     x[:, t, action_dim:] = val.clone() # batch size, time step, trajectory (all observation)
     return x
 
 #-----------------------------------------------------------------------------#
diff --git a/code/diffuser/models/temporal.py b/code/diffuser/models/temporal.py
index 63155c2..f1d897d 100644
--- a/code/diffuser/models/temporal.py
+++ b/code/diffuser/models/temporal.py
@@ -88,6 +88,12 @@ class ResidualTemporalBlock(nn.Module):
             nn.Linear(embed_dim, out_channels),
             Rearrange('batch t -> batch t 1'),
         )
+        
+        # self.cond_mlp = nn.Sequential(
+        #     act_fn,
+        #     nn.Linear(1, 1),
+        #     Rearrange(),
+        # )
 
         self.residual_conv = nn.Conv1d(inp_channels, out_channels, 1) \
             if inp_channels != out_channels else nn.Identity()
@@ -99,7 +105,9 @@ class ResidualTemporalBlock(nn.Module):
             returns:
             out : [ batch_size x out_channels x horizon ]
         '''
-        out = self.blocks[0](x) + self.time_mlp(t)
+        out = self.blocks[0](x)
+        tmp = self.time_mlp(t)
+        breakpoint()
         out = self.blocks[1](out)
 
         return out + self.residual_conv(x)
@@ -325,6 +333,9 @@ class MLPnet(nn.Module):
         self.calc_energy = calc_energy
         self.transition_dim = transition_dim
         self.action_dim = transition_dim - cond_dim
+        self.cond_dim = cond_dim
+        breakpoint()
+        # self.action_dim = 3
 
         if self.returns_condition:
             self.returns_mlp = nn.Sequential(
@@ -365,8 +376,8 @@ class MLPnet(nn.Module):
             if force_dropout:
                 returns_embed = 0*returns_embed
             t = torch.cat([t, returns_embed], dim=-1)
-
         inp = torch.cat([t, cond, x], dim=-1)
+        # inp = torch.cat([t.unsqueeze(1), cond.unsqueez(1), x], dim=-1)
         out  = self.mlp(inp)
 
         if self.calc_energy:
diff --git a/code/environment.yml b/code/environment.yml
index 31a3487..47a996c 100644
--- a/code/environment.yml
+++ b/code/environment.yml
@@ -8,9 +8,9 @@ dependencies:
 - patchelf
 - pip:
     - -f https://download.pytorch.org/whl/torch_stable.html
-    - numpy
+    - numpy==1.23.0
     - gym==0.18.0
-    - mujoco-py==2.0.2.13
+    - mujoco-py
     - matplotlib==3.3.4
     - torch==1.9.1+cu111
     - typed-argument-parser
diff --git a/code/scripts/train.py b/code/scripts/train.py
index db05e5a..50dab51 100644
--- a/code/scripts/train.py
+++ b/code/scripts/train.py
@@ -59,7 +59,8 @@ def main(**deps):
             Config.model,
             savepath='model_config.pkl',
             horizon=Config.horizon,
-            transition_dim=observation_dim,
+            # transition_dim=observation_dim,
+            transition_dim=observation_dim + action_dim,
             cond_dim=observation_dim,
             dim_mults=Config.dim_mults,
             returns_condition=Config.returns_condition,diff --git a/code/config/locomotion_config.py b/code/config/locomotion_config.py
index 90173b3..b79b7e6 100644
--- a/code/config/locomotion_config.py
+++ b/code/config/locomotion_config.py
@@ -6,7 +6,7 @@ class Config(ParamsProto):
     # misc
     seed = 100
     device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
-    bucket = '/home/aajay/weights/'
+    bucket = '/home/yanyangji/research/RL/final/decision-diffuser/code/output'
     dataset = 'hopper-medium-expert-v2'
 
     ## model
diff --git a/code/diffuser/datasets/buffer.py b/code/diffuser/datasets/buffer.py
index 42bc734..29edcd8 100644
--- a/code/diffuser/datasets/buffer.py
+++ b/code/diffuser/datasets/buffer.py
@@ -9,7 +9,7 @@ class ReplayBuffer:
 
     def __init__(self, max_n_episodes, max_path_length, termination_penalty):
         self._dict = {
-            'path_lengths': np.zeros(max_n_episodes, dtype=np.int),
+            'path_lengths': np.zeros(max_n_episodes, dtype=np.int32),
         }
         self._count = 0
         self.max_n_episodes = max_n_episodes
diff --git a/code/diffuser/datasets/sequence.py b/code/diffuser/datasets/sequence.py
index 31ac84f..f256554 100644
--- a/code/diffuser/datasets/sequence.py
+++ b/code/diffuser/datasets/sequence.py
@@ -76,7 +76,8 @@ class SequenceDataset(torch.utils.data.Dataset):
         '''
             condition on current observation for planning
         '''
-        return {0: observations[0]}
+        # return {0: observations[0]}
+        return observations[0]
 
     def __len__(self):
         return len(self.indices)
diff --git a/code/diffuser/models/diffusion.py b/code/diffuser/models/diffusion.py
index a1f74ea..0c8f1a1 100644
--- a/code/diffuser/models/diffusion.py
+++ b/code/diffuser/models/diffusion.py
@@ -3,6 +3,8 @@ import torch
 from torch import nn
 import torch.nn.functional as F
 import pdb
+import clip
+from config.locomotion_config import Config
 
 import diffuser.utils as utils
 from .helpers import (
@@ -295,12 +297,22 @@ class GaussianInvDynDiffusion(nn.Module):
         condition_guidance_w=0.1, ar_inv=False, train_only_inv=False):
         super().__init__()
         self.horizon = horizon
-        self.observation_dim = observation_dim
-        self.action_dim = action_dim
+        self.observation_dim = observation_dim # 11
+        self.action_dim = action_dim # 3
         self.transition_dim = observation_dim + action_dim
         self.model = model
         self.ar_inv = ar_inv
         self.train_only_inv = train_only_inv
+
+        # Text embedding
+        device = Config.device
+        self.clip_model, _ = clip.load("ViT-B/32", device=device)
+        for p in self.clip_model.parameters():
+            p.requires_grad = False
+        with torch.no_grad():
+            self.text_cond = self.clip_model.encode_text(clip.tokenize('the man is hopping').to(device))
+        self.proj_layer = nn.Linear(512, self.observation_dim)
+
         if self.ar_inv:
             self.inv_model = ARInvModel(hidden_dim=hidden_dim, observation_dim=observation_dim, action_dim=action_dim)
         else:
diff --git a/code/diffuser/models/helpers.py b/code/diffuser/models/helpers.py
index 53859d4..7384d2a 100644
--- a/code/diffuser/models/helpers.py
+++ b/code/diffuser/models/helpers.py
@@ -91,8 +91,8 @@ def cosine_beta_schedule(timesteps, s=0.008, dtype=torch.float32):
     return torch.tensor(betas_clipped, dtype=dtype)
 
 def apply_conditioning(x, conditions, action_dim):
-    for t, val in conditions.items():
-        x[:, t, action_dim:] = val.clone()
+    # for t, val in conditions.items():
+    #     x[:, t, action_dim:] = val.clone() # batch size, time step, trajectory (all observation)
     return x
 
 #-----------------------------------------------------------------------------#
diff --git a/code/diffuser/models/temporal.py b/code/diffuser/models/temporal.py
index 63155c2..7d1bdce 100644
--- a/code/diffuser/models/temporal.py
+++ b/code/diffuser/models/temporal.py
@@ -88,6 +88,12 @@ class ResidualTemporalBlock(nn.Module):
             nn.Linear(embed_dim, out_channels),
             Rearrange('batch t -> batch t 1'),
         )
+        
+        # self.cond_mlp = nn.Sequential(
+        #     act_fn,
+        #     nn.Linear(1, 1),
+        #     Rearrange(),
+        # )
 
         self.residual_conv = nn.Conv1d(inp_channels, out_channels, 1) \
             if inp_channels != out_channels else nn.Identity()
@@ -325,6 +331,9 @@ class MLPnet(nn.Module):
         self.calc_energy = calc_energy
         self.transition_dim = transition_dim
         self.action_dim = transition_dim - cond_dim
+        self.cond_dim = cond_dim
+        breakpoint()
+        # self.action_dim = 3
 
         if self.returns_condition:
             self.returns_mlp = nn.Sequential(
@@ -365,8 +374,8 @@ class MLPnet(nn.Module):
             if force_dropout:
                 returns_embed = 0*returns_embed
             t = torch.cat([t, returns_embed], dim=-1)
-
         inp = torch.cat([t, cond, x], dim=-1)
+        # inp = torch.cat([t.unsqueeze(1), cond.unsqueez(1), x], dim=-1)
         out  = self.mlp(inp)
 
         if self.calc_energy:
diff --git a/code/environment.yml b/code/environment.yml
index 31a3487..47a996c 100644
--- a/code/environment.yml
+++ b/code/environment.yml
@@ -8,9 +8,9 @@ dependencies:
 - patchelf
 - pip:
     - -f https://download.pytorch.org/whl/torch_stable.html
-    - numpy
+    - numpy==1.23.0
     - gym==0.18.0
-    - mujoco-py==2.0.2.13
+    - mujoco-py
     - matplotlib==3.3.4
     - torch==1.9.1+cu111
     - typed-argument-parser
diff --git a/code/scripts/train.py b/code/scripts/train.py
index db05e5a..50dab51 100644
--- a/code/scripts/train.py
+++ b/code/scripts/train.py
@@ -59,7 +59,8 @@ def main(**deps):
             Config.model,
             savepath='model_config.pkl',
             horizon=Config.horizon,
-            transition_dim=observation_dim,
+            # transition_dim=observation_dim,
+            transition_dim=observation_dim + action_dim,
             cond_dim=observation_dim,
             dim_mults=Config.dim_mults,
             returns_condition=Config.returns_condition,diff --git a/code/config/locomotion_config.py b/code/config/locomotion_config.py
index 90173b3..b79b7e6 100644
--- a/code/config/locomotion_config.py
+++ b/code/config/locomotion_config.py
@@ -6,7 +6,7 @@ class Config(ParamsProto):
     # misc
     seed = 100
     device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
-    bucket = '/home/aajay/weights/'
+    bucket = '/home/yanyangji/research/RL/final/decision-diffuser/code/output'
     dataset = 'hopper-medium-expert-v2'
 
     ## model
diff --git a/code/diffuser/datasets/buffer.py b/code/diffuser/datasets/buffer.py
index 42bc734..29edcd8 100644
--- a/code/diffuser/datasets/buffer.py
+++ b/code/diffuser/datasets/buffer.py
@@ -9,7 +9,7 @@ class ReplayBuffer:
 
     def __init__(self, max_n_episodes, max_path_length, termination_penalty):
         self._dict = {
-            'path_lengths': np.zeros(max_n_episodes, dtype=np.int),
+            'path_lengths': np.zeros(max_n_episodes, dtype=np.int32),
         }
         self._count = 0
         self.max_n_episodes = max_n_episodes
diff --git a/code/diffuser/datasets/sequence.py b/code/diffuser/datasets/sequence.py
index 31ac84f..f256554 100644
--- a/code/diffuser/datasets/sequence.py
+++ b/code/diffuser/datasets/sequence.py
@@ -76,7 +76,8 @@ class SequenceDataset(torch.utils.data.Dataset):
         '''
             condition on current observation for planning
         '''
-        return {0: observations[0]}
+        # return {0: observations[0]}
+        return observations[0]
 
     def __len__(self):
         return len(self.indices)
diff --git a/code/diffuser/models/diffusion.py b/code/diffuser/models/diffusion.py
index a1f74ea..0c8f1a1 100644
--- a/code/diffuser/models/diffusion.py
+++ b/code/diffuser/models/diffusion.py
@@ -3,6 +3,8 @@ import torch
 from torch import nn
 import torch.nn.functional as F
 import pdb
+import clip
+from config.locomotion_config import Config
 
 import diffuser.utils as utils
 from .helpers import (
@@ -295,12 +297,22 @@ class GaussianInvDynDiffusion(nn.Module):
         condition_guidance_w=0.1, ar_inv=False, train_only_inv=False):
         super().__init__()
         self.horizon = horizon
-        self.observation_dim = observation_dim
-        self.action_dim = action_dim
+        self.observation_dim = observation_dim # 11
+        self.action_dim = action_dim # 3
         self.transition_dim = observation_dim + action_dim
         self.model = model
         self.ar_inv = ar_inv
         self.train_only_inv = train_only_inv
+
+        # Text embedding
+        device = Config.device
+        self.clip_model, _ = clip.load("ViT-B/32", device=device)
+        for p in self.clip_model.parameters():
+            p.requires_grad = False
+        with torch.no_grad():
+            self.text_cond = self.clip_model.encode_text(clip.tokenize('the man is hopping').to(device))
+        self.proj_layer = nn.Linear(512, self.observation_dim)
+
         if self.ar_inv:
             self.inv_model = ARInvModel(hidden_dim=hidden_dim, observation_dim=observation_dim, action_dim=action_dim)
         else:
diff --git a/code/diffuser/models/helpers.py b/code/diffuser/models/helpers.py
index 53859d4..7384d2a 100644
--- a/code/diffuser/models/helpers.py
+++ b/code/diffuser/models/helpers.py
@@ -91,8 +91,8 @@ def cosine_beta_schedule(timesteps, s=0.008, dtype=torch.float32):
     return torch.tensor(betas_clipped, dtype=dtype)
 
 def apply_conditioning(x, conditions, action_dim):
-    for t, val in conditions.items():
-        x[:, t, action_dim:] = val.clone()
+    # for t, val in conditions.items():
+    #     x[:, t, action_dim:] = val.clone() # batch size, time step, trajectory (all observation)
     return x
 
 #-----------------------------------------------------------------------------#
diff --git a/code/diffuser/models/temporal.py b/code/diffuser/models/temporal.py
index 63155c2..7d1bdce 100644
--- a/code/diffuser/models/temporal.py
+++ b/code/diffuser/models/temporal.py
@@ -88,6 +88,12 @@ class ResidualTemporalBlock(nn.Module):
             nn.Linear(embed_dim, out_channels),
             Rearrange('batch t -> batch t 1'),
         )
+        
+        # self.cond_mlp = nn.Sequential(
+        #     act_fn,
+        #     nn.Linear(1, 1),
+        #     Rearrange(),
+        # )
 
         self.residual_conv = nn.Conv1d(inp_channels, out_channels, 1) \
             if inp_channels != out_channels else nn.Identity()
@@ -325,6 +331,9 @@ class MLPnet(nn.Module):
         self.calc_energy = calc_energy
         self.transition_dim = transition_dim
         self.action_dim = transition_dim - cond_dim
+        self.cond_dim = cond_dim
+        breakpoint()
+        # self.action_dim = 3
 
         if self.returns_condition:
             self.returns_mlp = nn.Sequential(
@@ -365,8 +374,8 @@ class MLPnet(nn.Module):
             if force_dropout:
                 returns_embed = 0*returns_embed
             t = torch.cat([t, returns_embed], dim=-1)
-
         inp = torch.cat([t, cond, x], dim=-1)
+        # inp = torch.cat([t.unsqueeze(1), cond.unsqueez(1), x], dim=-1)
         out  = self.mlp(inp)
 
         if self.calc_energy:
diff --git a/code/environment.yml b/code/environment.yml
index 31a3487..47a996c 100644
--- a/code/environment.yml
+++ b/code/environment.yml
@@ -8,9 +8,9 @@ dependencies:
 - patchelf
 - pip:
     - -f https://download.pytorch.org/whl/torch_stable.html
-    - numpy
+    - numpy==1.23.0
     - gym==0.18.0
-    - mujoco-py==2.0.2.13
+    - mujoco-py
     - matplotlib==3.3.4
     - torch==1.9.1+cu111
     - typed-argument-parser
diff --git a/code/scripts/train.py b/code/scripts/train.py
index db05e5a..c1a819e 100644
--- a/code/scripts/train.py
+++ b/code/scripts/train.py
@@ -59,7 +59,8 @@ def main(**deps):
             Config.model,
             savepath='model_config.pkl',
             horizon=Config.horizon,
-            transition_dim=observation_dim,
+            # transition_dim=observation_dim,
+            transition_dim=observation_dim + action_dim,
             cond_dim=observation_dim,
             dim_mults=Config.dim_mults,
             returns_condition=Config.returns_condition,
@@ -95,7 +96,7 @@ def main(**deps):
             Config.model,
             savepath='model_config.pkl',
             horizon=Config.horizon,
-            transition_dim=observation_dim + action_dim,
+            transition_dim=observation_dim,
             cond_dim=observation_dim,
             dim_mults=Config.dim_mults,
             returns_condition=Config.returns_condition,diff --git a/code/config/locomotion_config.py b/code/config/locomotion_config.py
index 90173b3..b79b7e6 100644
--- a/code/config/locomotion_config.py
+++ b/code/config/locomotion_config.py
@@ -6,7 +6,7 @@ class Config(ParamsProto):
     # misc
     seed = 100
     device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
-    bucket = '/home/aajay/weights/'
+    bucket = '/home/yanyangji/research/RL/final/decision-diffuser/code/output'
     dataset = 'hopper-medium-expert-v2'
 
     ## model
diff --git a/code/diffuser/datasets/buffer.py b/code/diffuser/datasets/buffer.py
index 42bc734..29edcd8 100644
--- a/code/diffuser/datasets/buffer.py
+++ b/code/diffuser/datasets/buffer.py
@@ -9,7 +9,7 @@ class ReplayBuffer:
 
     def __init__(self, max_n_episodes, max_path_length, termination_penalty):
         self._dict = {
-            'path_lengths': np.zeros(max_n_episodes, dtype=np.int),
+            'path_lengths': np.zeros(max_n_episodes, dtype=np.int32),
         }
         self._count = 0
         self.max_n_episodes = max_n_episodes
diff --git a/code/diffuser/datasets/sequence.py b/code/diffuser/datasets/sequence.py
index 31ac84f..f256554 100644
--- a/code/diffuser/datasets/sequence.py
+++ b/code/diffuser/datasets/sequence.py
@@ -76,7 +76,8 @@ class SequenceDataset(torch.utils.data.Dataset):
         '''
             condition on current observation for planning
         '''
-        return {0: observations[0]}
+        # return {0: observations[0]}
+        return observations[0]
 
     def __len__(self):
         return len(self.indices)
diff --git a/code/diffuser/models/diffusion.py b/code/diffuser/models/diffusion.py
index a1f74ea..0c8f1a1 100644
--- a/code/diffuser/models/diffusion.py
+++ b/code/diffuser/models/diffusion.py
@@ -3,6 +3,8 @@ import torch
 from torch import nn
 import torch.nn.functional as F
 import pdb
+import clip
+from config.locomotion_config import Config
 
 import diffuser.utils as utils
 from .helpers import (
@@ -295,12 +297,22 @@ class GaussianInvDynDiffusion(nn.Module):
         condition_guidance_w=0.1, ar_inv=False, train_only_inv=False):
         super().__init__()
         self.horizon = horizon
-        self.observation_dim = observation_dim
-        self.action_dim = action_dim
+        self.observation_dim = observation_dim # 11
+        self.action_dim = action_dim # 3
         self.transition_dim = observation_dim + action_dim
         self.model = model
         self.ar_inv = ar_inv
         self.train_only_inv = train_only_inv
+
+        # Text embedding
+        device = Config.device
+        self.clip_model, _ = clip.load("ViT-B/32", device=device)
+        for p in self.clip_model.parameters():
+            p.requires_grad = False
+        with torch.no_grad():
+            self.text_cond = self.clip_model.encode_text(clip.tokenize('the man is hopping').to(device))
+        self.proj_layer = nn.Linear(512, self.observation_dim)
+
         if self.ar_inv:
             self.inv_model = ARInvModel(hidden_dim=hidden_dim, observation_dim=observation_dim, action_dim=action_dim)
         else:
diff --git a/code/diffuser/models/helpers.py b/code/diffuser/models/helpers.py
index 53859d4..aecc561 100644
--- a/code/diffuser/models/helpers.py
+++ b/code/diffuser/models/helpers.py
@@ -92,7 +92,7 @@ def cosine_beta_schedule(timesteps, s=0.008, dtype=torch.float32):
 
 def apply_conditioning(x, conditions, action_dim):
     for t, val in conditions.items():
-        x[:, t, action_dim:] = val.clone()
+        x[:, t, action_dim:] = val.clone() # batch size, time step, trajectory (all observation)
     return x
 
 #-----------------------------------------------------------------------------#
diff --git a/code/diffuser/models/temporal.py b/code/diffuser/models/temporal.py
index 63155c2..7d1bdce 100644
--- a/code/diffuser/models/temporal.py
+++ b/code/diffuser/models/temporal.py
@@ -88,6 +88,12 @@ class ResidualTemporalBlock(nn.Module):
             nn.Linear(embed_dim, out_channels),
             Rearrange('batch t -> batch t 1'),
         )
+        
+        # self.cond_mlp = nn.Sequential(
+        #     act_fn,
+        #     nn.Linear(1, 1),
+        #     Rearrange(),
+        # )
 
         self.residual_conv = nn.Conv1d(inp_channels, out_channels, 1) \
             if inp_channels != out_channels else nn.Identity()
@@ -325,6 +331,9 @@ class MLPnet(nn.Module):
         self.calc_energy = calc_energy
         self.transition_dim = transition_dim
         self.action_dim = transition_dim - cond_dim
+        self.cond_dim = cond_dim
+        breakpoint()
+        # self.action_dim = 3
 
         if self.returns_condition:
             self.returns_mlp = nn.Sequential(
@@ -365,8 +374,8 @@ class MLPnet(nn.Module):
             if force_dropout:
                 returns_embed = 0*returns_embed
             t = torch.cat([t, returns_embed], dim=-1)
-
         inp = torch.cat([t, cond, x], dim=-1)
+        # inp = torch.cat([t.unsqueeze(1), cond.unsqueez(1), x], dim=-1)
         out  = self.mlp(inp)
 
         if self.calc_energy:
diff --git a/code/environment.yml b/code/environment.yml
index 31a3487..47a996c 100644
--- a/code/environment.yml
+++ b/code/environment.yml
@@ -8,9 +8,9 @@ dependencies:
 - patchelf
 - pip:
     - -f https://download.pytorch.org/whl/torch_stable.html
-    - numpy
+    - numpy==1.23.0
     - gym==0.18.0
-    - mujoco-py==2.0.2.13
+    - mujoco-py
     - matplotlib==3.3.4
     - torch==1.9.1+cu111
     - typed-argument-parser
diff --git a/code/scripts/train.py b/code/scripts/train.py
index db05e5a..c1a819e 100644
--- a/code/scripts/train.py
+++ b/code/scripts/train.py
@@ -59,7 +59,8 @@ def main(**deps):
             Config.model,
             savepath='model_config.pkl',
             horizon=Config.horizon,
-            transition_dim=observation_dim,
+            # transition_dim=observation_dim,
+            transition_dim=observation_dim + action_dim,
             cond_dim=observation_dim,
             dim_mults=Config.dim_mults,
             returns_condition=Config.returns_condition,
@@ -95,7 +96,7 @@ def main(**deps):
             Config.model,
             savepath='model_config.pkl',
             horizon=Config.horizon,
-            transition_dim=observation_dim + action_dim,
+            transition_dim=observation_dim,
             cond_dim=observation_dim,
             dim_mults=Config.dim_mults,
             returns_condition=Config.returns_condition,diff --git a/code/config/locomotion_config.py b/code/config/locomotion_config.py
index 90173b3..b79b7e6 100644
--- a/code/config/locomotion_config.py
+++ b/code/config/locomotion_config.py
@@ -6,7 +6,7 @@ class Config(ParamsProto):
     # misc
     seed = 100
     device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
-    bucket = '/home/aajay/weights/'
+    bucket = '/home/yanyangji/research/RL/final/decision-diffuser/code/output'
     dataset = 'hopper-medium-expert-v2'
 
     ## model
diff --git a/code/diffuser/datasets/buffer.py b/code/diffuser/datasets/buffer.py
index 42bc734..29edcd8 100644
--- a/code/diffuser/datasets/buffer.py
+++ b/code/diffuser/datasets/buffer.py
@@ -9,7 +9,7 @@ class ReplayBuffer:
 
     def __init__(self, max_n_episodes, max_path_length, termination_penalty):
         self._dict = {
-            'path_lengths': np.zeros(max_n_episodes, dtype=np.int),
+            'path_lengths': np.zeros(max_n_episodes, dtype=np.int32),
         }
         self._count = 0
         self.max_n_episodes = max_n_episodes
diff --git a/code/diffuser/datasets/sequence.py b/code/diffuser/datasets/sequence.py
index 31ac84f..ad977fc 100644
--- a/code/diffuser/datasets/sequence.py
+++ b/code/diffuser/datasets/sequence.py
@@ -77,6 +77,7 @@ class SequenceDataset(torch.utils.data.Dataset):
             condition on current observation for planning
         '''
         return {0: observations[0]}
+        # return observations[0]
 
     def __len__(self):
         return len(self.indices)
diff --git a/code/diffuser/models/diffusion.py b/code/diffuser/models/diffusion.py
index a1f74ea..0c8f1a1 100644
--- a/code/diffuser/models/diffusion.py
+++ b/code/diffuser/models/diffusion.py
@@ -3,6 +3,8 @@ import torch
 from torch import nn
 import torch.nn.functional as F
 import pdb
+import clip
+from config.locomotion_config import Config
 
 import diffuser.utils as utils
 from .helpers import (
@@ -295,12 +297,22 @@ class GaussianInvDynDiffusion(nn.Module):
         condition_guidance_w=0.1, ar_inv=False, train_only_inv=False):
         super().__init__()
         self.horizon = horizon
-        self.observation_dim = observation_dim
-        self.action_dim = action_dim
+        self.observation_dim = observation_dim # 11
+        self.action_dim = action_dim # 3
         self.transition_dim = observation_dim + action_dim
         self.model = model
         self.ar_inv = ar_inv
         self.train_only_inv = train_only_inv
+
+        # Text embedding
+        device = Config.device
+        self.clip_model, _ = clip.load("ViT-B/32", device=device)
+        for p in self.clip_model.parameters():
+            p.requires_grad = False
+        with torch.no_grad():
+            self.text_cond = self.clip_model.encode_text(clip.tokenize('the man is hopping').to(device))
+        self.proj_layer = nn.Linear(512, self.observation_dim)
+
         if self.ar_inv:
             self.inv_model = ARInvModel(hidden_dim=hidden_dim, observation_dim=observation_dim, action_dim=action_dim)
         else:
diff --git a/code/diffuser/models/helpers.py b/code/diffuser/models/helpers.py
index 53859d4..aecc561 100644
--- a/code/diffuser/models/helpers.py
+++ b/code/diffuser/models/helpers.py
@@ -92,7 +92,7 @@ def cosine_beta_schedule(timesteps, s=0.008, dtype=torch.float32):
 
 def apply_conditioning(x, conditions, action_dim):
     for t, val in conditions.items():
-        x[:, t, action_dim:] = val.clone()
+        x[:, t, action_dim:] = val.clone() # batch size, time step, trajectory (all observation)
     return x
 
 #-----------------------------------------------------------------------------#
diff --git a/code/diffuser/models/temporal.py b/code/diffuser/models/temporal.py
index 63155c2..7d1bdce 100644
--- a/code/diffuser/models/temporal.py
+++ b/code/diffuser/models/temporal.py
@@ -88,6 +88,12 @@ class ResidualTemporalBlock(nn.Module):
             nn.Linear(embed_dim, out_channels),
             Rearrange('batch t -> batch t 1'),
         )
+        
+        # self.cond_mlp = nn.Sequential(
+        #     act_fn,
+        #     nn.Linear(1, 1),
+        #     Rearrange(),
+        # )
 
         self.residual_conv = nn.Conv1d(inp_channels, out_channels, 1) \
             if inp_channels != out_channels else nn.Identity()
@@ -325,6 +331,9 @@ class MLPnet(nn.Module):
         self.calc_energy = calc_energy
         self.transition_dim = transition_dim
         self.action_dim = transition_dim - cond_dim
+        self.cond_dim = cond_dim
+        breakpoint()
+        # self.action_dim = 3
 
         if self.returns_condition:
             self.returns_mlp = nn.Sequential(
@@ -365,8 +374,8 @@ class MLPnet(nn.Module):
             if force_dropout:
                 returns_embed = 0*returns_embed
             t = torch.cat([t, returns_embed], dim=-1)
-
         inp = torch.cat([t, cond, x], dim=-1)
+        # inp = torch.cat([t.unsqueeze(1), cond.unsqueez(1), x], dim=-1)
         out  = self.mlp(inp)
 
         if self.calc_energy:
diff --git a/code/environment.yml b/code/environment.yml
index 31a3487..47a996c 100644
--- a/code/environment.yml
+++ b/code/environment.yml
@@ -8,9 +8,9 @@ dependencies:
 - patchelf
 - pip:
     - -f https://download.pytorch.org/whl/torch_stable.html
-    - numpy
+    - numpy==1.23.0
     - gym==0.18.0
-    - mujoco-py==2.0.2.13
+    - mujoco-py
     - matplotlib==3.3.4
     - torch==1.9.1+cu111
     - typed-argument-parser
diff --git a/code/scripts/train.py b/code/scripts/train.py
index db05e5a..c1a819e 100644
--- a/code/scripts/train.py
+++ b/code/scripts/train.py
@@ -59,7 +59,8 @@ def main(**deps):
             Config.model,
             savepath='model_config.pkl',
             horizon=Config.horizon,
-            transition_dim=observation_dim,
+            # transition_dim=observation_dim,
+            transition_dim=observation_dim + action_dim,
             cond_dim=observation_dim,
             dim_mults=Config.dim_mults,
             returns_condition=Config.returns_condition,
@@ -95,7 +96,7 @@ def main(**deps):
             Config.model,
             savepath='model_config.pkl',
             horizon=Config.horizon,
-            transition_dim=observation_dim + action_dim,
+            transition_dim=observation_dim,
             cond_dim=observation_dim,
             dim_mults=Config.dim_mults,
             returns_condition=Config.returns_condition,diff --git a/code/config/locomotion_config.py b/code/config/locomotion_config.py
index 90173b3..b79b7e6 100644
--- a/code/config/locomotion_config.py
+++ b/code/config/locomotion_config.py
@@ -6,7 +6,7 @@ class Config(ParamsProto):
     # misc
     seed = 100
     device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
-    bucket = '/home/aajay/weights/'
+    bucket = '/home/yanyangji/research/RL/final/decision-diffuser/code/output'
     dataset = 'hopper-medium-expert-v2'
 
     ## model
diff --git a/code/diffuser/datasets/buffer.py b/code/diffuser/datasets/buffer.py
index 42bc734..29edcd8 100644
--- a/code/diffuser/datasets/buffer.py
+++ b/code/diffuser/datasets/buffer.py
@@ -9,7 +9,7 @@ class ReplayBuffer:
 
     def __init__(self, max_n_episodes, max_path_length, termination_penalty):
         self._dict = {
-            'path_lengths': np.zeros(max_n_episodes, dtype=np.int),
+            'path_lengths': np.zeros(max_n_episodes, dtype=np.int32),
         }
         self._count = 0
         self.max_n_episodes = max_n_episodes
diff --git a/code/diffuser/datasets/sequence.py b/code/diffuser/datasets/sequence.py
index 31ac84f..ad977fc 100644
--- a/code/diffuser/datasets/sequence.py
+++ b/code/diffuser/datasets/sequence.py
@@ -77,6 +77,7 @@ class SequenceDataset(torch.utils.data.Dataset):
             condition on current observation for planning
         '''
         return {0: observations[0]}
+        # return observations[0]
 
     def __len__(self):
         return len(self.indices)
diff --git a/code/diffuser/models/diffusion.py b/code/diffuser/models/diffusion.py
index a1f74ea..0c8f1a1 100644
--- a/code/diffuser/models/diffusion.py
+++ b/code/diffuser/models/diffusion.py
@@ -3,6 +3,8 @@ import torch
 from torch import nn
 import torch.nn.functional as F
 import pdb
+import clip
+from config.locomotion_config import Config
 
 import diffuser.utils as utils
 from .helpers import (
@@ -295,12 +297,22 @@ class GaussianInvDynDiffusion(nn.Module):
         condition_guidance_w=0.1, ar_inv=False, train_only_inv=False):
         super().__init__()
         self.horizon = horizon
-        self.observation_dim = observation_dim
-        self.action_dim = action_dim
+        self.observation_dim = observation_dim # 11
+        self.action_dim = action_dim # 3
         self.transition_dim = observation_dim + action_dim
         self.model = model
         self.ar_inv = ar_inv
         self.train_only_inv = train_only_inv
+
+        # Text embedding
+        device = Config.device
+        self.clip_model, _ = clip.load("ViT-B/32", device=device)
+        for p in self.clip_model.parameters():
+            p.requires_grad = False
+        with torch.no_grad():
+            self.text_cond = self.clip_model.encode_text(clip.tokenize('the man is hopping').to(device))
+        self.proj_layer = nn.Linear(512, self.observation_dim)
+
         if self.ar_inv:
             self.inv_model = ARInvModel(hidden_dim=hidden_dim, observation_dim=observation_dim, action_dim=action_dim)
         else:
diff --git a/code/diffuser/models/helpers.py b/code/diffuser/models/helpers.py
index 53859d4..aecc561 100644
--- a/code/diffuser/models/helpers.py
+++ b/code/diffuser/models/helpers.py
@@ -92,7 +92,7 @@ def cosine_beta_schedule(timesteps, s=0.008, dtype=torch.float32):
 
 def apply_conditioning(x, conditions, action_dim):
     for t, val in conditions.items():
-        x[:, t, action_dim:] = val.clone()
+        x[:, t, action_dim:] = val.clone() # batch size, time step, trajectory (all observation)
     return x
 
 #-----------------------------------------------------------------------------#
diff --git a/code/diffuser/models/temporal.py b/code/diffuser/models/temporal.py
index 63155c2..a6bbeb4 100644
--- a/code/diffuser/models/temporal.py
+++ b/code/diffuser/models/temporal.py
@@ -325,6 +325,7 @@ class MLPnet(nn.Module):
         self.calc_energy = calc_energy
         self.transition_dim = transition_dim
         self.action_dim = transition_dim - cond_dim
+        self.cond_dim = cond_dim
 
         if self.returns_condition:
             self.returns_mlp = nn.Sequential(
@@ -365,8 +366,8 @@ class MLPnet(nn.Module):
             if force_dropout:
                 returns_embed = 0*returns_embed
             t = torch.cat([t, returns_embed], dim=-1)
-
         inp = torch.cat([t, cond, x], dim=-1)
+        # inp = torch.cat([t.unsqueeze(1), cond.unsqueez(1), x], dim=-1)
         out  = self.mlp(inp)
 
         if self.calc_energy:
diff --git a/code/environment.yml b/code/environment.yml
index 31a3487..47a996c 100644
--- a/code/environment.yml
+++ b/code/environment.yml
@@ -8,9 +8,9 @@ dependencies:
 - patchelf
 - pip:
     - -f https://download.pytorch.org/whl/torch_stable.html
-    - numpy
+    - numpy==1.23.0
     - gym==0.18.0
-    - mujoco-py==2.0.2.13
+    - mujoco-py
     - matplotlib==3.3.4
     - torch==1.9.1+cu111
     - typed-argument-parser
diff --git a/code/scripts/train.py b/code/scripts/train.py
index db05e5a..53b8e2f 100644
--- a/code/scripts/train.py
+++ b/code/scripts/train.py
@@ -60,6 +60,7 @@ def main(**deps):
             savepath='model_config.pkl',
             horizon=Config.horizon,
             transition_dim=observation_dim,
+            # transition_dim=observation_dim + action_dim,
             cond_dim=observation_dim,
             dim_mults=Config.dim_mults,
             returns_condition=Config.returns_condition,
@@ -95,7 +96,7 @@ def main(**deps):
             Config.model,
             savepath='model_config.pkl',
             horizon=Config.horizon,
-            transition_dim=observation_dim + action_dim,
+            transition_dim=observation_dim,
             cond_dim=observation_dim,
             dim_mults=Config.dim_mults,
             returns_condition=Config.returns_condition,diff --git a/code/config/locomotion_config.py b/code/config/locomotion_config.py
index 90173b3..b79b7e6 100644
--- a/code/config/locomotion_config.py
+++ b/code/config/locomotion_config.py
@@ -6,7 +6,7 @@ class Config(ParamsProto):
     # misc
     seed = 100
     device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
-    bucket = '/home/aajay/weights/'
+    bucket = '/home/yanyangji/research/RL/final/decision-diffuser/code/output'
     dataset = 'hopper-medium-expert-v2'
 
     ## model
diff --git a/code/diffuser/datasets/buffer.py b/code/diffuser/datasets/buffer.py
index 42bc734..29edcd8 100644
--- a/code/diffuser/datasets/buffer.py
+++ b/code/diffuser/datasets/buffer.py
@@ -9,7 +9,7 @@ class ReplayBuffer:
 
     def __init__(self, max_n_episodes, max_path_length, termination_penalty):
         self._dict = {
-            'path_lengths': np.zeros(max_n_episodes, dtype=np.int),
+            'path_lengths': np.zeros(max_n_episodes, dtype=np.int32),
         }
         self._count = 0
         self.max_n_episodes = max_n_episodes
diff --git a/code/diffuser/datasets/sequence.py b/code/diffuser/datasets/sequence.py
index 31ac84f..ad977fc 100644
--- a/code/diffuser/datasets/sequence.py
+++ b/code/diffuser/datasets/sequence.py
@@ -77,6 +77,7 @@ class SequenceDataset(torch.utils.data.Dataset):
             condition on current observation for planning
         '''
         return {0: observations[0]}
+        # return observations[0]
 
     def __len__(self):
         return len(self.indices)
diff --git a/code/diffuser/models/diffusion.py b/code/diffuser/models/diffusion.py
index a1f74ea..0c8f1a1 100644
--- a/code/diffuser/models/diffusion.py
+++ b/code/diffuser/models/diffusion.py
@@ -3,6 +3,8 @@ import torch
 from torch import nn
 import torch.nn.functional as F
 import pdb
+import clip
+from config.locomotion_config import Config
 
 import diffuser.utils as utils
 from .helpers import (
@@ -295,12 +297,22 @@ class GaussianInvDynDiffusion(nn.Module):
         condition_guidance_w=0.1, ar_inv=False, train_only_inv=False):
         super().__init__()
         self.horizon = horizon
-        self.observation_dim = observation_dim
-        self.action_dim = action_dim
+        self.observation_dim = observation_dim # 11
+        self.action_dim = action_dim # 3
         self.transition_dim = observation_dim + action_dim
         self.model = model
         self.ar_inv = ar_inv
         self.train_only_inv = train_only_inv
+
+        # Text embedding
+        device = Config.device
+        self.clip_model, _ = clip.load("ViT-B/32", device=device)
+        for p in self.clip_model.parameters():
+            p.requires_grad = False
+        with torch.no_grad():
+            self.text_cond = self.clip_model.encode_text(clip.tokenize('the man is hopping').to(device))
+        self.proj_layer = nn.Linear(512, self.observation_dim)
+
         if self.ar_inv:
             self.inv_model = ARInvModel(hidden_dim=hidden_dim, observation_dim=observation_dim, action_dim=action_dim)
         else:
diff --git a/code/diffuser/models/helpers.py b/code/diffuser/models/helpers.py
index 53859d4..aecc561 100644
--- a/code/diffuser/models/helpers.py
+++ b/code/diffuser/models/helpers.py
@@ -92,7 +92,7 @@ def cosine_beta_schedule(timesteps, s=0.008, dtype=torch.float32):
 
 def apply_conditioning(x, conditions, action_dim):
     for t, val in conditions.items():
-        x[:, t, action_dim:] = val.clone()
+        x[:, t, action_dim:] = val.clone() # batch size, time step, trajectory (all observation)
     return x
 
 #-----------------------------------------------------------------------------#
diff --git a/code/diffuser/models/temporal.py b/code/diffuser/models/temporal.py
index 63155c2..cb5f998 100644
--- a/code/diffuser/models/temporal.py
+++ b/code/diffuser/models/temporal.py
@@ -99,6 +99,7 @@ class ResidualTemporalBlock(nn.Module):
             returns:
             out : [ batch_size x out_channels x horizon ]
         '''
+        breakpoint()
         out = self.blocks[0](x) + self.time_mlp(t)
         out = self.blocks[1](out)
 
@@ -325,6 +326,7 @@ class MLPnet(nn.Module):
         self.calc_energy = calc_energy
         self.transition_dim = transition_dim
         self.action_dim = transition_dim - cond_dim
+        self.cond_dim = cond_dim
 
         if self.returns_condition:
             self.returns_mlp = nn.Sequential(
@@ -365,8 +367,8 @@ class MLPnet(nn.Module):
             if force_dropout:
                 returns_embed = 0*returns_embed
             t = torch.cat([t, returns_embed], dim=-1)
-
         inp = torch.cat([t, cond, x], dim=-1)
+        # inp = torch.cat([t.unsqueeze(1), cond.unsqueez(1), x], dim=-1)
         out  = self.mlp(inp)
 
         if self.calc_energy:
diff --git a/code/environment.yml b/code/environment.yml
index 31a3487..47a996c 100644
--- a/code/environment.yml
+++ b/code/environment.yml
@@ -8,9 +8,9 @@ dependencies:
 - patchelf
 - pip:
     - -f https://download.pytorch.org/whl/torch_stable.html
-    - numpy
+    - numpy==1.23.0
     - gym==0.18.0
-    - mujoco-py==2.0.2.13
+    - mujoco-py
     - matplotlib==3.3.4
     - torch==1.9.1+cu111
     - typed-argument-parser
diff --git a/code/scripts/train.py b/code/scripts/train.py
index db05e5a..53b8e2f 100644
--- a/code/scripts/train.py
+++ b/code/scripts/train.py
@@ -60,6 +60,7 @@ def main(**deps):
             savepath='model_config.pkl',
             horizon=Config.horizon,
             transition_dim=observation_dim,
+            # transition_dim=observation_dim + action_dim,
             cond_dim=observation_dim,
             dim_mults=Config.dim_mults,
             returns_condition=Config.returns_condition,
@@ -95,7 +96,7 @@ def main(**deps):
             Config.model,
             savepath='model_config.pkl',
             horizon=Config.horizon,
-            transition_dim=observation_dim + action_dim,
+            transition_dim=observation_dim,
             cond_dim=observation_dim,
             dim_mults=Config.dim_mults,
             returns_condition=Config.returns_condition,diff --git a/code/config/locomotion_config.py b/code/config/locomotion_config.py
index 90173b3..b79b7e6 100644
--- a/code/config/locomotion_config.py
+++ b/code/config/locomotion_config.py
@@ -6,7 +6,7 @@ class Config(ParamsProto):
     # misc
     seed = 100
     device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
-    bucket = '/home/aajay/weights/'
+    bucket = '/home/yanyangji/research/RL/final/decision-diffuser/code/output'
     dataset = 'hopper-medium-expert-v2'
 
     ## model
diff --git a/code/diffuser/datasets/buffer.py b/code/diffuser/datasets/buffer.py
index 42bc734..29edcd8 100644
--- a/code/diffuser/datasets/buffer.py
+++ b/code/diffuser/datasets/buffer.py
@@ -9,7 +9,7 @@ class ReplayBuffer:
 
     def __init__(self, max_n_episodes, max_path_length, termination_penalty):
         self._dict = {
-            'path_lengths': np.zeros(max_n_episodes, dtype=np.int),
+            'path_lengths': np.zeros(max_n_episodes, dtype=np.int32),
         }
         self._count = 0
         self.max_n_episodes = max_n_episodes
diff --git a/code/diffuser/datasets/sequence.py b/code/diffuser/datasets/sequence.py
index 31ac84f..ad977fc 100644
--- a/code/diffuser/datasets/sequence.py
+++ b/code/diffuser/datasets/sequence.py
@@ -77,6 +77,7 @@ class SequenceDataset(torch.utils.data.Dataset):
             condition on current observation for planning
         '''
         return {0: observations[0]}
+        # return observations[0]
 
     def __len__(self):
         return len(self.indices)
diff --git a/code/diffuser/models/diffusion.py b/code/diffuser/models/diffusion.py
index a1f74ea..0c8f1a1 100644
--- a/code/diffuser/models/diffusion.py
+++ b/code/diffuser/models/diffusion.py
@@ -3,6 +3,8 @@ import torch
 from torch import nn
 import torch.nn.functional as F
 import pdb
+import clip
+from config.locomotion_config import Config
 
 import diffuser.utils as utils
 from .helpers import (
@@ -295,12 +297,22 @@ class GaussianInvDynDiffusion(nn.Module):
         condition_guidance_w=0.1, ar_inv=False, train_only_inv=False):
         super().__init__()
         self.horizon = horizon
-        self.observation_dim = observation_dim
-        self.action_dim = action_dim
+        self.observation_dim = observation_dim # 11
+        self.action_dim = action_dim # 3
         self.transition_dim = observation_dim + action_dim
         self.model = model
         self.ar_inv = ar_inv
         self.train_only_inv = train_only_inv
+
+        # Text embedding
+        device = Config.device
+        self.clip_model, _ = clip.load("ViT-B/32", device=device)
+        for p in self.clip_model.parameters():
+            p.requires_grad = False
+        with torch.no_grad():
+            self.text_cond = self.clip_model.encode_text(clip.tokenize('the man is hopping').to(device))
+        self.proj_layer = nn.Linear(512, self.observation_dim)
+
         if self.ar_inv:
             self.inv_model = ARInvModel(hidden_dim=hidden_dim, observation_dim=observation_dim, action_dim=action_dim)
         else:
diff --git a/code/diffuser/models/helpers.py b/code/diffuser/models/helpers.py
index 53859d4..aecc561 100644
--- a/code/diffuser/models/helpers.py
+++ b/code/diffuser/models/helpers.py
@@ -92,7 +92,7 @@ def cosine_beta_schedule(timesteps, s=0.008, dtype=torch.float32):
 
 def apply_conditioning(x, conditions, action_dim):
     for t, val in conditions.items():
-        x[:, t, action_dim:] = val.clone()
+        x[:, t, action_dim:] = val.clone() # batch size, time step, trajectory (all observation)
     return x
 
 #-----------------------------------------------------------------------------#
diff --git a/code/diffuser/models/temporal.py b/code/diffuser/models/temporal.py
index 63155c2..a6bbeb4 100644
--- a/code/diffuser/models/temporal.py
+++ b/code/diffuser/models/temporal.py
@@ -325,6 +325,7 @@ class MLPnet(nn.Module):
         self.calc_energy = calc_energy
         self.transition_dim = transition_dim
         self.action_dim = transition_dim - cond_dim
+        self.cond_dim = cond_dim
 
         if self.returns_condition:
             self.returns_mlp = nn.Sequential(
@@ -365,8 +366,8 @@ class MLPnet(nn.Module):
             if force_dropout:
                 returns_embed = 0*returns_embed
             t = torch.cat([t, returns_embed], dim=-1)
-
         inp = torch.cat([t, cond, x], dim=-1)
+        # inp = torch.cat([t.unsqueeze(1), cond.unsqueez(1), x], dim=-1)
         out  = self.mlp(inp)
 
         if self.calc_energy:
diff --git a/code/diffuser/utils/training.py b/code/diffuser/utils/training.py
index e8dcdb4..a4200ea 100644
--- a/code/diffuser/utils/training.py
+++ b/code/diffuser/utils/training.py
@@ -113,6 +113,7 @@ class Trainer(object):
             for i in range(self.gradient_accumulate_every):
                 batch = next(self.dataloader)
                 batch = batch_to_device(batch, device=self.device)
+                breakpoint()
                 loss, infos = self.model.loss(*batch)
                 loss = loss / self.gradient_accumulate_every
                 loss.backward()
diff --git a/code/environment.yml b/code/environment.yml
index 31a3487..47a996c 100644
--- a/code/environment.yml
+++ b/code/environment.yml
@@ -8,9 +8,9 @@ dependencies:
 - patchelf
 - pip:
     - -f https://download.pytorch.org/whl/torch_stable.html
-    - numpy
+    - numpy==1.23.0
     - gym==0.18.0
-    - mujoco-py==2.0.2.13
+    - mujoco-py
     - matplotlib==3.3.4
     - torch==1.9.1+cu111
     - typed-argument-parser
diff --git a/code/scripts/train.py b/code/scripts/train.py
index db05e5a..53b8e2f 100644
--- a/code/scripts/train.py
+++ b/code/scripts/train.py
@@ -60,6 +60,7 @@ def main(**deps):
             savepath='model_config.pkl',
             horizon=Config.horizon,
             transition_dim=observation_dim,
+            # transition_dim=observation_dim + action_dim,
             cond_dim=observation_dim,
             dim_mults=Config.dim_mults,
             returns_condition=Config.returns_condition,
@@ -95,7 +96,7 @@ def main(**deps):
             Config.model,
             savepath='model_config.pkl',
             horizon=Config.horizon,
-            transition_dim=observation_dim + action_dim,
+            transition_dim=observation_dim,
             cond_dim=observation_dim,
             dim_mults=Config.dim_mults,
             returns_condition=Config.returns_condition,diff --git a/code/config/locomotion_config.py b/code/config/locomotion_config.py
index 90173b3..b79b7e6 100644
--- a/code/config/locomotion_config.py
+++ b/code/config/locomotion_config.py
@@ -6,7 +6,7 @@ class Config(ParamsProto):
     # misc
     seed = 100
     device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
-    bucket = '/home/aajay/weights/'
+    bucket = '/home/yanyangji/research/RL/final/decision-diffuser/code/output'
     dataset = 'hopper-medium-expert-v2'
 
     ## model
diff --git a/code/diffuser/datasets/buffer.py b/code/diffuser/datasets/buffer.py
index 42bc734..29edcd8 100644
--- a/code/diffuser/datasets/buffer.py
+++ b/code/diffuser/datasets/buffer.py
@@ -9,7 +9,7 @@ class ReplayBuffer:
 
     def __init__(self, max_n_episodes, max_path_length, termination_penalty):
         self._dict = {
-            'path_lengths': np.zeros(max_n_episodes, dtype=np.int),
+            'path_lengths': np.zeros(max_n_episodes, dtype=np.int32),
         }
         self._count = 0
         self.max_n_episodes = max_n_episodes
diff --git a/code/diffuser/datasets/sequence.py b/code/diffuser/datasets/sequence.py
index 31ac84f..f256554 100644
--- a/code/diffuser/datasets/sequence.py
+++ b/code/diffuser/datasets/sequence.py
@@ -76,7 +76,8 @@ class SequenceDataset(torch.utils.data.Dataset):
         '''
             condition on current observation for planning
         '''
-        return {0: observations[0]}
+        # return {0: observations[0]}
+        return observations[0]
 
     def __len__(self):
         return len(self.indices)
diff --git a/code/diffuser/models/diffusion.py b/code/diffuser/models/diffusion.py
index a1f74ea..0c8f1a1 100644
--- a/code/diffuser/models/diffusion.py
+++ b/code/diffuser/models/diffusion.py
@@ -3,6 +3,8 @@ import torch
 from torch import nn
 import torch.nn.functional as F
 import pdb
+import clip
+from config.locomotion_config import Config
 
 import diffuser.utils as utils
 from .helpers import (
@@ -295,12 +297,22 @@ class GaussianInvDynDiffusion(nn.Module):
         condition_guidance_w=0.1, ar_inv=False, train_only_inv=False):
         super().__init__()
         self.horizon = horizon
-        self.observation_dim = observation_dim
-        self.action_dim = action_dim
+        self.observation_dim = observation_dim # 11
+        self.action_dim = action_dim # 3
         self.transition_dim = observation_dim + action_dim
         self.model = model
         self.ar_inv = ar_inv
         self.train_only_inv = train_only_inv
+
+        # Text embedding
+        device = Config.device
+        self.clip_model, _ = clip.load("ViT-B/32", device=device)
+        for p in self.clip_model.parameters():
+            p.requires_grad = False
+        with torch.no_grad():
+            self.text_cond = self.clip_model.encode_text(clip.tokenize('the man is hopping').to(device))
+        self.proj_layer = nn.Linear(512, self.observation_dim)
+
         if self.ar_inv:
             self.inv_model = ARInvModel(hidden_dim=hidden_dim, observation_dim=observation_dim, action_dim=action_dim)
         else:
diff --git a/code/diffuser/models/helpers.py b/code/diffuser/models/helpers.py
index 53859d4..b976a8d 100644
--- a/code/diffuser/models/helpers.py
+++ b/code/diffuser/models/helpers.py
@@ -91,8 +91,9 @@ def cosine_beta_schedule(timesteps, s=0.008, dtype=torch.float32):
     return torch.tensor(betas_clipped, dtype=dtype)
 
 def apply_conditioning(x, conditions, action_dim):
-    for t, val in conditions.items():
-        x[:, t, action_dim:] = val.clone()
+    # for t, val in conditions.items():
+    #     x[:, t, action_dim:] = val.clone() # batch size, time step, trajectory (all observation)
+    x[:, 0, :] = conditions.clone()
     return x
 
 #-----------------------------------------------------------------------------#
diff --git a/code/diffuser/models/temporal.py b/code/diffuser/models/temporal.py
index 63155c2..c46f517 100644
--- a/code/diffuser/models/temporal.py
+++ b/code/diffuser/models/temporal.py
@@ -89,17 +89,23 @@ class ResidualTemporalBlock(nn.Module):
             Rearrange('batch t -> batch t 1'),
         )
 
+        self.cond_mlp = nn.Sequential(
+            act_fn,
+            nn.Linear(embed_dim, out_channels),
+            Rearrange('batch t -> batch t 1')
+        )
+
         self.residual_conv = nn.Conv1d(inp_channels, out_channels, 1) \
             if inp_channels != out_channels else nn.Identity()
 
-    def forward(self, x, t):
+    def forward(self, x, t, cond):
         '''
             x : [ batch_size x inp_channels x horizon ]
             t : [ batch_size x embed_dim ]
             returns:
             out : [ batch_size x out_channels x horizon ]
         '''
-        out = self.blocks[0](x) + self.time_mlp(t)
+        out = self.blocks[0](x) + self.time_mlp(t) + self.cond_mlp(cond)
         out = self.blocks[1](out)
 
         return out + self.residual_conv(x)
@@ -140,6 +146,13 @@ class TemporalUnet(nn.Module):
             act_fn,
             nn.Linear(dim * 4, dim),
         )
+        self.cond_mlp = nn.Sequential(
+            # SinusoidalPosEmb(dim),
+            nn.Linear(11, dim * 4),
+            act_fn,
+            nn.Linear(dim * 4, dim),
+        )
+
 
         self.returns_condition = returns_condition
         self.condition_dropout = condition_dropout
@@ -207,6 +220,7 @@ class TemporalUnet(nn.Module):
         x = einops.rearrange(x, 'b h t -> b t h')
 
         t = self.time_mlp(time)
+        c = self.cond_mlp(cond)
 
         if self.returns_condition:
             assert returns is not None
@@ -221,21 +235,21 @@ class TemporalUnet(nn.Module):
         h = []
 
         for resnet, resnet2, downsample in self.downs:
-            x = resnet(x, t)
-            x = resnet2(x, t)
+            x = resnet(x, t, c)
+            x = resnet2(x, t, c)
             h.append(x)
             x = downsample(x)
 
-        x = self.mid_block1(x, t)
-        x = self.mid_block2(x, t)
+        x = self.mid_block1(x, t, c)
+        x = self.mid_block2(x, t, c)
 
         # import pdb; pdb.set_trace()
 
         for resnet, resnet2, upsample in self.ups:
             x = torch.cat((x, h.pop()), dim=1)
-            x = resnet(x, t)
-            x = resnet2(x, t)
-            x = upsample(x)
+            x = resnet(x, t, c)
+            x = resnet2(x, t, c)
+            x = upsample(x, c)
 
         x = self.final_conv(x)
 
@@ -257,6 +271,7 @@ class TemporalUnet(nn.Module):
         x = einops.rearrange(x, 'b h t -> b t h')
 
         t = self.time_mlp(time)
+        c = self.cond_mlp(cond)
 
         if self.returns_condition:
             assert returns is not None
@@ -271,18 +286,18 @@ class TemporalUnet(nn.Module):
         h = []
 
         for resnet, resnet2, downsample in self.downs:
-            x = resnet(x, t)
-            x = resnet2(x, t)
+            x = resnet(x, t, c)
+            x = resnet2(x, t, c)
             h.append(x)
             x = downsample(x)
 
-        x = self.mid_block1(x, t)
-        x = self.mid_block2(x, t)
+        x = self.mid_block1(x, t, c)
+        x = self.mid_block2(x, t, c)
 
         for resnet, resnet2, upsample in self.ups:
             x = torch.cat((x, h.pop()), dim=1)
-            x = resnet(x, t)
-            x = resnet2(x, t)
+            x = resnet(x, t, c)
+            x = resnet2(x, t, c)
             x = upsample(x)
 
         x = self.final_conv(x)
@@ -325,6 +340,7 @@ class MLPnet(nn.Module):
         self.calc_energy = calc_energy
         self.transition_dim = transition_dim
         self.action_dim = transition_dim - cond_dim
+        self.cond_dim = cond_dim
 
         if self.returns_condition:
             self.returns_mlp = nn.Sequential(
@@ -365,8 +381,8 @@ class MLPnet(nn.Module):
             if force_dropout:
                 returns_embed = 0*returns_embed
             t = torch.cat([t, returns_embed], dim=-1)
-
         inp = torch.cat([t, cond, x], dim=-1)
+        # inp = torch.cat([t.unsqueeze(1), cond.unsqueez(1), x], dim=-1)
         out  = self.mlp(inp)
 
         if self.calc_energy:
diff --git a/code/diffuser/utils/training.py b/code/diffuser/utils/training.py
index e8dcdb4..a4200ea 100644
--- a/code/diffuser/utils/training.py
+++ b/code/diffuser/utils/training.py
@@ -113,6 +113,7 @@ class Trainer(object):
             for i in range(self.gradient_accumulate_every):
                 batch = next(self.dataloader)
                 batch = batch_to_device(batch, device=self.device)
+                breakpoint()
                 loss, infos = self.model.loss(*batch)
                 loss = loss / self.gradient_accumulate_every
                 loss.backward()
diff --git a/code/environment.yml b/code/environment.yml
index 31a3487..47a996c 100644
--- a/code/environment.yml
+++ b/code/environment.yml
@@ -8,9 +8,9 @@ dependencies:
 - patchelf
 - pip:
     - -f https://download.pytorch.org/whl/torch_stable.html
-    - numpy
+    - numpy==1.23.0
     - gym==0.18.0
-    - mujoco-py==2.0.2.13
+    - mujoco-py
     - matplotlib==3.3.4
     - torch==1.9.1+cu111
     - typed-argument-parser
diff --git a/code/scripts/train.py b/code/scripts/train.py
index db05e5a..53b8e2f 100644
--- a/code/scripts/train.py
+++ b/code/scripts/train.py
@@ -60,6 +60,7 @@ def main(**deps):
             savepath='model_config.pkl',
             horizon=Config.horizon,
             transition_dim=observation_dim,
+            # transition_dim=observation_dim + action_dim,
             cond_dim=observation_dim,
             dim_mults=Config.dim_mults,
             returns_condition=Config.returns_condition,
@@ -95,7 +96,7 @@ def main(**deps):
             Config.model,
             savepath='model_config.pkl',
             horizon=Config.horizon,
-            transition_dim=observation_dim + action_dim,
+            transition_dim=observation_dim,
             cond_dim=observation_dim,
             dim_mults=Config.dim_mults,
             returns_condition=Config.returns_condition,diff --git a/code/config/locomotion_config.py b/code/config/locomotion_config.py
index 90173b3..b79b7e6 100644
--- a/code/config/locomotion_config.py
+++ b/code/config/locomotion_config.py
@@ -6,7 +6,7 @@ class Config(ParamsProto):
     # misc
     seed = 100
     device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
-    bucket = '/home/aajay/weights/'
+    bucket = '/home/yanyangji/research/RL/final/decision-diffuser/code/output'
     dataset = 'hopper-medium-expert-v2'
 
     ## model
diff --git a/code/diffuser/datasets/buffer.py b/code/diffuser/datasets/buffer.py
index 42bc734..29edcd8 100644
--- a/code/diffuser/datasets/buffer.py
+++ b/code/diffuser/datasets/buffer.py
@@ -9,7 +9,7 @@ class ReplayBuffer:
 
     def __init__(self, max_n_episodes, max_path_length, termination_penalty):
         self._dict = {
-            'path_lengths': np.zeros(max_n_episodes, dtype=np.int),
+            'path_lengths': np.zeros(max_n_episodes, dtype=np.int32),
         }
         self._count = 0
         self.max_n_episodes = max_n_episodes
diff --git a/code/diffuser/datasets/sequence.py b/code/diffuser/datasets/sequence.py
index 31ac84f..f256554 100644
--- a/code/diffuser/datasets/sequence.py
+++ b/code/diffuser/datasets/sequence.py
@@ -76,7 +76,8 @@ class SequenceDataset(torch.utils.data.Dataset):
         '''
             condition on current observation for planning
         '''
-        return {0: observations[0]}
+        # return {0: observations[0]}
+        return observations[0]
 
     def __len__(self):
         return len(self.indices)
diff --git a/code/diffuser/models/diffusion.py b/code/diffuser/models/diffusion.py
index a1f74ea..0c8f1a1 100644
--- a/code/diffuser/models/diffusion.py
+++ b/code/diffuser/models/diffusion.py
@@ -3,6 +3,8 @@ import torch
 from torch import nn
 import torch.nn.functional as F
 import pdb
+import clip
+from config.locomotion_config import Config
 
 import diffuser.utils as utils
 from .helpers import (
@@ -295,12 +297,22 @@ class GaussianInvDynDiffusion(nn.Module):
         condition_guidance_w=0.1, ar_inv=False, train_only_inv=False):
         super().__init__()
         self.horizon = horizon
-        self.observation_dim = observation_dim
-        self.action_dim = action_dim
+        self.observation_dim = observation_dim # 11
+        self.action_dim = action_dim # 3
         self.transition_dim = observation_dim + action_dim
         self.model = model
         self.ar_inv = ar_inv
         self.train_only_inv = train_only_inv
+
+        # Text embedding
+        device = Config.device
+        self.clip_model, _ = clip.load("ViT-B/32", device=device)
+        for p in self.clip_model.parameters():
+            p.requires_grad = False
+        with torch.no_grad():
+            self.text_cond = self.clip_model.encode_text(clip.tokenize('the man is hopping').to(device))
+        self.proj_layer = nn.Linear(512, self.observation_dim)
+
         if self.ar_inv:
             self.inv_model = ARInvModel(hidden_dim=hidden_dim, observation_dim=observation_dim, action_dim=action_dim)
         else:
diff --git a/code/diffuser/models/helpers.py b/code/diffuser/models/helpers.py
index 53859d4..b976a8d 100644
--- a/code/diffuser/models/helpers.py
+++ b/code/diffuser/models/helpers.py
@@ -91,8 +91,9 @@ def cosine_beta_schedule(timesteps, s=0.008, dtype=torch.float32):
     return torch.tensor(betas_clipped, dtype=dtype)
 
 def apply_conditioning(x, conditions, action_dim):
-    for t, val in conditions.items():
-        x[:, t, action_dim:] = val.clone()
+    # for t, val in conditions.items():
+    #     x[:, t, action_dim:] = val.clone() # batch size, time step, trajectory (all observation)
+    x[:, 0, :] = conditions.clone()
     return x
 
 #-----------------------------------------------------------------------------#
diff --git a/code/diffuser/models/temporal.py b/code/diffuser/models/temporal.py
index 63155c2..f1a8b4f 100644
--- a/code/diffuser/models/temporal.py
+++ b/code/diffuser/models/temporal.py
@@ -89,10 +89,16 @@ class ResidualTemporalBlock(nn.Module):
             Rearrange('batch t -> batch t 1'),
         )
 
+        self.cond_mlp = nn.Sequential(
+            act_fn,
+            nn.Linear(embed_dim, out_channels),
+            Rearrange('batch t -> batch t 1')
+        )
+
         self.residual_conv = nn.Conv1d(inp_channels, out_channels, 1) \
             if inp_channels != out_channels else nn.Identity()
 
-    def forward(self, x, t):
+    def forward(self, x, t, cond):
         '''
             x : [ batch_size x inp_channels x horizon ]
             t : [ batch_size x embed_dim ]
@@ -100,6 +106,8 @@ class ResidualTemporalBlock(nn.Module):
             out : [ batch_size x out_channels x horizon ]
         '''
         out = self.blocks[0](x) + self.time_mlp(t)
+        c = self.cond_mlp(cond)
+        breakpoint()
         out = self.blocks[1](out)
 
         return out + self.residual_conv(x)
@@ -140,6 +148,13 @@ class TemporalUnet(nn.Module):
             act_fn,
             nn.Linear(dim * 4, dim),
         )
+        self.cond_mlp = nn.Sequential(
+            # SinusoidalPosEmb(dim),
+            nn.Linear(11, dim * 4),
+            act_fn,
+            nn.Linear(dim * 4, dim),
+        )
+
 
         self.returns_condition = returns_condition
         self.condition_dropout = condition_dropout
@@ -207,6 +222,7 @@ class TemporalUnet(nn.Module):
         x = einops.rearrange(x, 'b h t -> b t h')
 
         t = self.time_mlp(time)
+        c = self.cond_mlp(cond)
 
         if self.returns_condition:
             assert returns is not None
@@ -221,21 +237,21 @@ class TemporalUnet(nn.Module):
         h = []
 
         for resnet, resnet2, downsample in self.downs:
-            x = resnet(x, t)
-            x = resnet2(x, t)
+            x = resnet(x, t, c)
+            x = resnet2(x, t, c)
             h.append(x)
             x = downsample(x)
 
-        x = self.mid_block1(x, t)
-        x = self.mid_block2(x, t)
+        x = self.mid_block1(x, t, c)
+        x = self.mid_block2(x, t, c)
 
         # import pdb; pdb.set_trace()
 
         for resnet, resnet2, upsample in self.ups:
             x = torch.cat((x, h.pop()), dim=1)
-            x = resnet(x, t)
-            x = resnet2(x, t)
-            x = upsample(x)
+            x = resnet(x, t, c)
+            x = resnet2(x, t, c)
+            x = upsample(x, c)
 
         x = self.final_conv(x)
 
@@ -257,6 +273,7 @@ class TemporalUnet(nn.Module):
         x = einops.rearrange(x, 'b h t -> b t h')
 
         t = self.time_mlp(time)
+        c = self.cond_mlp(cond)
 
         if self.returns_condition:
             assert returns is not None
@@ -271,18 +288,18 @@ class TemporalUnet(nn.Module):
         h = []
 
         for resnet, resnet2, downsample in self.downs:
-            x = resnet(x, t)
-            x = resnet2(x, t)
+            x = resnet(x, t, c)
+            x = resnet2(x, t, c)
             h.append(x)
             x = downsample(x)
 
-        x = self.mid_block1(x, t)
-        x = self.mid_block2(x, t)
+        x = self.mid_block1(x, t, c)
+        x = self.mid_block2(x, t, c)
 
         for resnet, resnet2, upsample in self.ups:
             x = torch.cat((x, h.pop()), dim=1)
-            x = resnet(x, t)
-            x = resnet2(x, t)
+            x = resnet(x, t, c)
+            x = resnet2(x, t, c)
             x = upsample(x)
 
         x = self.final_conv(x)
@@ -325,6 +342,7 @@ class MLPnet(nn.Module):
         self.calc_energy = calc_energy
         self.transition_dim = transition_dim
         self.action_dim = transition_dim - cond_dim
+        self.cond_dim = cond_dim
 
         if self.returns_condition:
             self.returns_mlp = nn.Sequential(
@@ -365,8 +383,8 @@ class MLPnet(nn.Module):
             if force_dropout:
                 returns_embed = 0*returns_embed
             t = torch.cat([t, returns_embed], dim=-1)
-
         inp = torch.cat([t, cond, x], dim=-1)
+        # inp = torch.cat([t.unsqueeze(1), cond.unsqueez(1), x], dim=-1)
         out  = self.mlp(inp)
 
         if self.calc_energy:
diff --git a/code/diffuser/utils/training.py b/code/diffuser/utils/training.py
index e8dcdb4..a4200ea 100644
--- a/code/diffuser/utils/training.py
+++ b/code/diffuser/utils/training.py
@@ -113,6 +113,7 @@ class Trainer(object):
             for i in range(self.gradient_accumulate_every):
                 batch = next(self.dataloader)
                 batch = batch_to_device(batch, device=self.device)
+                breakpoint()
                 loss, infos = self.model.loss(*batch)
                 loss = loss / self.gradient_accumulate_every
                 loss.backward()
diff --git a/code/environment.yml b/code/environment.yml
index 31a3487..47a996c 100644
--- a/code/environment.yml
+++ b/code/environment.yml
@@ -8,9 +8,9 @@ dependencies:
 - patchelf
 - pip:
     - -f https://download.pytorch.org/whl/torch_stable.html
-    - numpy
+    - numpy==1.23.0
     - gym==0.18.0
-    - mujoco-py==2.0.2.13
+    - mujoco-py
     - matplotlib==3.3.4
     - torch==1.9.1+cu111
     - typed-argument-parser
diff --git a/code/scripts/train.py b/code/scripts/train.py
index db05e5a..53b8e2f 100644
--- a/code/scripts/train.py
+++ b/code/scripts/train.py
@@ -60,6 +60,7 @@ def main(**deps):
             savepath='model_config.pkl',
             horizon=Config.horizon,
             transition_dim=observation_dim,
+            # transition_dim=observation_dim + action_dim,
             cond_dim=observation_dim,
             dim_mults=Config.dim_mults,
             returns_condition=Config.returns_condition,
@@ -95,7 +96,7 @@ def main(**deps):
             Config.model,
             savepath='model_config.pkl',
             horizon=Config.horizon,
-            transition_dim=observation_dim + action_dim,
+            transition_dim=observation_dim,
             cond_dim=observation_dim,
             dim_mults=Config.dim_mults,
             returns_condition=Config.returns_condition,diff --git a/code/config/locomotion_config.py b/code/config/locomotion_config.py
index 90173b3..b79b7e6 100644
--- a/code/config/locomotion_config.py
+++ b/code/config/locomotion_config.py
@@ -6,7 +6,7 @@ class Config(ParamsProto):
     # misc
     seed = 100
     device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
-    bucket = '/home/aajay/weights/'
+    bucket = '/home/yanyangji/research/RL/final/decision-diffuser/code/output'
     dataset = 'hopper-medium-expert-v2'
 
     ## model
diff --git a/code/diffuser/datasets/buffer.py b/code/diffuser/datasets/buffer.py
index 42bc734..29edcd8 100644
--- a/code/diffuser/datasets/buffer.py
+++ b/code/diffuser/datasets/buffer.py
@@ -9,7 +9,7 @@ class ReplayBuffer:
 
     def __init__(self, max_n_episodes, max_path_length, termination_penalty):
         self._dict = {
-            'path_lengths': np.zeros(max_n_episodes, dtype=np.int),
+            'path_lengths': np.zeros(max_n_episodes, dtype=np.int32),
         }
         self._count = 0
         self.max_n_episodes = max_n_episodes
diff --git a/code/diffuser/datasets/sequence.py b/code/diffuser/datasets/sequence.py
index 31ac84f..f256554 100644
--- a/code/diffuser/datasets/sequence.py
+++ b/code/diffuser/datasets/sequence.py
@@ -76,7 +76,8 @@ class SequenceDataset(torch.utils.data.Dataset):
         '''
             condition on current observation for planning
         '''
-        return {0: observations[0]}
+        # return {0: observations[0]}
+        return observations[0]
 
     def __len__(self):
         return len(self.indices)
diff --git a/code/diffuser/models/diffusion.py b/code/diffuser/models/diffusion.py
index a1f74ea..0c8f1a1 100644
--- a/code/diffuser/models/diffusion.py
+++ b/code/diffuser/models/diffusion.py
@@ -3,6 +3,8 @@ import torch
 from torch import nn
 import torch.nn.functional as F
 import pdb
+import clip
+from config.locomotion_config import Config
 
 import diffuser.utils as utils
 from .helpers import (
@@ -295,12 +297,22 @@ class GaussianInvDynDiffusion(nn.Module):
         condition_guidance_w=0.1, ar_inv=False, train_only_inv=False):
         super().__init__()
         self.horizon = horizon
-        self.observation_dim = observation_dim
-        self.action_dim = action_dim
+        self.observation_dim = observation_dim # 11
+        self.action_dim = action_dim # 3
         self.transition_dim = observation_dim + action_dim
         self.model = model
         self.ar_inv = ar_inv
         self.train_only_inv = train_only_inv
+
+        # Text embedding
+        device = Config.device
+        self.clip_model, _ = clip.load("ViT-B/32", device=device)
+        for p in self.clip_model.parameters():
+            p.requires_grad = False
+        with torch.no_grad():
+            self.text_cond = self.clip_model.encode_text(clip.tokenize('the man is hopping').to(device))
+        self.proj_layer = nn.Linear(512, self.observation_dim)
+
         if self.ar_inv:
             self.inv_model = ARInvModel(hidden_dim=hidden_dim, observation_dim=observation_dim, action_dim=action_dim)
         else:
diff --git a/code/diffuser/models/helpers.py b/code/diffuser/models/helpers.py
index 53859d4..b976a8d 100644
--- a/code/diffuser/models/helpers.py
+++ b/code/diffuser/models/helpers.py
@@ -91,8 +91,9 @@ def cosine_beta_schedule(timesteps, s=0.008, dtype=torch.float32):
     return torch.tensor(betas_clipped, dtype=dtype)
 
 def apply_conditioning(x, conditions, action_dim):
-    for t, val in conditions.items():
-        x[:, t, action_dim:] = val.clone()
+    # for t, val in conditions.items():
+    #     x[:, t, action_dim:] = val.clone() # batch size, time step, trajectory (all observation)
+    x[:, 0, :] = conditions.clone()
     return x
 
 #-----------------------------------------------------------------------------#
diff --git a/code/diffuser/models/temporal.py b/code/diffuser/models/temporal.py
index 63155c2..f1a8b4f 100644
--- a/code/diffuser/models/temporal.py
+++ b/code/diffuser/models/temporal.py
@@ -89,10 +89,16 @@ class ResidualTemporalBlock(nn.Module):
             Rearrange('batch t -> batch t 1'),
         )
 
+        self.cond_mlp = nn.Sequential(
+            act_fn,
+            nn.Linear(embed_dim, out_channels),
+            Rearrange('batch t -> batch t 1')
+        )
+
         self.residual_conv = nn.Conv1d(inp_channels, out_channels, 1) \
             if inp_channels != out_channels else nn.Identity()
 
-    def forward(self, x, t):
+    def forward(self, x, t, cond):
         '''
             x : [ batch_size x inp_channels x horizon ]
             t : [ batch_size x embed_dim ]
@@ -100,6 +106,8 @@ class ResidualTemporalBlock(nn.Module):
             out : [ batch_size x out_channels x horizon ]
         '''
         out = self.blocks[0](x) + self.time_mlp(t)
+        c = self.cond_mlp(cond)
+        breakpoint()
         out = self.blocks[1](out)
 
         return out + self.residual_conv(x)
@@ -140,6 +148,13 @@ class TemporalUnet(nn.Module):
             act_fn,
             nn.Linear(dim * 4, dim),
         )
+        self.cond_mlp = nn.Sequential(
+            # SinusoidalPosEmb(dim),
+            nn.Linear(11, dim * 4),
+            act_fn,
+            nn.Linear(dim * 4, dim),
+        )
+
 
         self.returns_condition = returns_condition
         self.condition_dropout = condition_dropout
@@ -207,6 +222,7 @@ class TemporalUnet(nn.Module):
         x = einops.rearrange(x, 'b h t -> b t h')
 
         t = self.time_mlp(time)
+        c = self.cond_mlp(cond)
 
         if self.returns_condition:
             assert returns is not None
@@ -221,21 +237,21 @@ class TemporalUnet(nn.Module):
         h = []
 
         for resnet, resnet2, downsample in self.downs:
-            x = resnet(x, t)
-            x = resnet2(x, t)
+            x = resnet(x, t, c)
+            x = resnet2(x, t, c)
             h.append(x)
             x = downsample(x)
 
-        x = self.mid_block1(x, t)
-        x = self.mid_block2(x, t)
+        x = self.mid_block1(x, t, c)
+        x = self.mid_block2(x, t, c)
 
         # import pdb; pdb.set_trace()
 
         for resnet, resnet2, upsample in self.ups:
             x = torch.cat((x, h.pop()), dim=1)
-            x = resnet(x, t)
-            x = resnet2(x, t)
-            x = upsample(x)
+            x = resnet(x, t, c)
+            x = resnet2(x, t, c)
+            x = upsample(x, c)
 
         x = self.final_conv(x)
 
@@ -257,6 +273,7 @@ class TemporalUnet(nn.Module):
         x = einops.rearrange(x, 'b h t -> b t h')
 
         t = self.time_mlp(time)
+        c = self.cond_mlp(cond)
 
         if self.returns_condition:
             assert returns is not None
@@ -271,18 +288,18 @@ class TemporalUnet(nn.Module):
         h = []
 
         for resnet, resnet2, downsample in self.downs:
-            x = resnet(x, t)
-            x = resnet2(x, t)
+            x = resnet(x, t, c)
+            x = resnet2(x, t, c)
             h.append(x)
             x = downsample(x)
 
-        x = self.mid_block1(x, t)
-        x = self.mid_block2(x, t)
+        x = self.mid_block1(x, t, c)
+        x = self.mid_block2(x, t, c)
 
         for resnet, resnet2, upsample in self.ups:
             x = torch.cat((x, h.pop()), dim=1)
-            x = resnet(x, t)
-            x = resnet2(x, t)
+            x = resnet(x, t, c)
+            x = resnet2(x, t, c)
             x = upsample(x)
 
         x = self.final_conv(x)
@@ -325,6 +342,7 @@ class MLPnet(nn.Module):
         self.calc_energy = calc_energy
         self.transition_dim = transition_dim
         self.action_dim = transition_dim - cond_dim
+        self.cond_dim = cond_dim
 
         if self.returns_condition:
             self.returns_mlp = nn.Sequential(
@@ -365,8 +383,8 @@ class MLPnet(nn.Module):
             if force_dropout:
                 returns_embed = 0*returns_embed
             t = torch.cat([t, returns_embed], dim=-1)
-
         inp = torch.cat([t, cond, x], dim=-1)
+        # inp = torch.cat([t.unsqueeze(1), cond.unsqueez(1), x], dim=-1)
         out  = self.mlp(inp)
 
         if self.calc_energy:
diff --git a/code/diffuser/utils/training.py b/code/diffuser/utils/training.py
index e8dcdb4..a4200ea 100644
--- a/code/diffuser/utils/training.py
+++ b/code/diffuser/utils/training.py
@@ -113,6 +113,7 @@ class Trainer(object):
             for i in range(self.gradient_accumulate_every):
                 batch = next(self.dataloader)
                 batch = batch_to_device(batch, device=self.device)
+                breakpoint()
                 loss, infos = self.model.loss(*batch)
                 loss = loss / self.gradient_accumulate_every
                 loss.backward()
diff --git a/code/environment.yml b/code/environment.yml
index 31a3487..47a996c 100644
--- a/code/environment.yml
+++ b/code/environment.yml
@@ -8,9 +8,9 @@ dependencies:
 - patchelf
 - pip:
     - -f https://download.pytorch.org/whl/torch_stable.html
-    - numpy
+    - numpy==1.23.0
     - gym==0.18.0
-    - mujoco-py==2.0.2.13
+    - mujoco-py
     - matplotlib==3.3.4
     - torch==1.9.1+cu111
     - typed-argument-parser
diff --git a/code/scripts/train.py b/code/scripts/train.py
index db05e5a..53b8e2f 100644
--- a/code/scripts/train.py
+++ b/code/scripts/train.py
@@ -60,6 +60,7 @@ def main(**deps):
             savepath='model_config.pkl',
             horizon=Config.horizon,
             transition_dim=observation_dim,
+            # transition_dim=observation_dim + action_dim,
             cond_dim=observation_dim,
             dim_mults=Config.dim_mults,
             returns_condition=Config.returns_condition,
@@ -95,7 +96,7 @@ def main(**deps):
             Config.model,
             savepath='model_config.pkl',
             horizon=Config.horizon,
-            transition_dim=observation_dim + action_dim,
+            transition_dim=observation_dim,
             cond_dim=observation_dim,
             dim_mults=Config.dim_mults,
             returns_condition=Config.returns_condition,diff --git a/code/config/locomotion_config.py b/code/config/locomotion_config.py
index 90173b3..b79b7e6 100644
--- a/code/config/locomotion_config.py
+++ b/code/config/locomotion_config.py
@@ -6,7 +6,7 @@ class Config(ParamsProto):
     # misc
     seed = 100
     device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
-    bucket = '/home/aajay/weights/'
+    bucket = '/home/yanyangji/research/RL/final/decision-diffuser/code/output'
     dataset = 'hopper-medium-expert-v2'
 
     ## model
diff --git a/code/diffuser/datasets/buffer.py b/code/diffuser/datasets/buffer.py
index 42bc734..29edcd8 100644
--- a/code/diffuser/datasets/buffer.py
+++ b/code/diffuser/datasets/buffer.py
@@ -9,7 +9,7 @@ class ReplayBuffer:
 
     def __init__(self, max_n_episodes, max_path_length, termination_penalty):
         self._dict = {
-            'path_lengths': np.zeros(max_n_episodes, dtype=np.int),
+            'path_lengths': np.zeros(max_n_episodes, dtype=np.int32),
         }
         self._count = 0
         self.max_n_episodes = max_n_episodes
diff --git a/code/diffuser/datasets/sequence.py b/code/diffuser/datasets/sequence.py
index 31ac84f..f256554 100644
--- a/code/diffuser/datasets/sequence.py
+++ b/code/diffuser/datasets/sequence.py
@@ -76,7 +76,8 @@ class SequenceDataset(torch.utils.data.Dataset):
         '''
             condition on current observation for planning
         '''
-        return {0: observations[0]}
+        # return {0: observations[0]}
+        return observations[0]
 
     def __len__(self):
         return len(self.indices)
diff --git a/code/diffuser/models/diffusion.py b/code/diffuser/models/diffusion.py
index a1f74ea..0c8f1a1 100644
--- a/code/diffuser/models/diffusion.py
+++ b/code/diffuser/models/diffusion.py
@@ -3,6 +3,8 @@ import torch
 from torch import nn
 import torch.nn.functional as F
 import pdb
+import clip
+from config.locomotion_config import Config
 
 import diffuser.utils as utils
 from .helpers import (
@@ -295,12 +297,22 @@ class GaussianInvDynDiffusion(nn.Module):
         condition_guidance_w=0.1, ar_inv=False, train_only_inv=False):
         super().__init__()
         self.horizon = horizon
-        self.observation_dim = observation_dim
-        self.action_dim = action_dim
+        self.observation_dim = observation_dim # 11
+        self.action_dim = action_dim # 3
         self.transition_dim = observation_dim + action_dim
         self.model = model
         self.ar_inv = ar_inv
         self.train_only_inv = train_only_inv
+
+        # Text embedding
+        device = Config.device
+        self.clip_model, _ = clip.load("ViT-B/32", device=device)
+        for p in self.clip_model.parameters():
+            p.requires_grad = False
+        with torch.no_grad():
+            self.text_cond = self.clip_model.encode_text(clip.tokenize('the man is hopping').to(device))
+        self.proj_layer = nn.Linear(512, self.observation_dim)
+
         if self.ar_inv:
             self.inv_model = ARInvModel(hidden_dim=hidden_dim, observation_dim=observation_dim, action_dim=action_dim)
         else:
diff --git a/code/diffuser/models/helpers.py b/code/diffuser/models/helpers.py
index 53859d4..b976a8d 100644
--- a/code/diffuser/models/helpers.py
+++ b/code/diffuser/models/helpers.py
@@ -91,8 +91,9 @@ def cosine_beta_schedule(timesteps, s=0.008, dtype=torch.float32):
     return torch.tensor(betas_clipped, dtype=dtype)
 
 def apply_conditioning(x, conditions, action_dim):
-    for t, val in conditions.items():
-        x[:, t, action_dim:] = val.clone()
+    # for t, val in conditions.items():
+    #     x[:, t, action_dim:] = val.clone() # batch size, time step, trajectory (all observation)
+    x[:, 0, :] = conditions.clone()
     return x
 
 #-----------------------------------------------------------------------------#
diff --git a/code/diffuser/models/temporal.py b/code/diffuser/models/temporal.py
index 63155c2..e1fd5f2 100644
--- a/code/diffuser/models/temporal.py
+++ b/code/diffuser/models/temporal.py
@@ -89,10 +89,16 @@ class ResidualTemporalBlock(nn.Module):
             Rearrange('batch t -> batch t 1'),
         )
 
+        self.cond_mlp = nn.Sequential(
+            act_fn,
+            nn.Linear(embed_dim, out_channels),
+            Rearrange('batch t -> batch t 1')
+        )
+
         self.residual_conv = nn.Conv1d(inp_channels, out_channels, 1) \
             if inp_channels != out_channels else nn.Identity()
 
-    def forward(self, x, t):
+    def forward(self, x, t, cond):
         '''
             x : [ batch_size x inp_channels x horizon ]
             t : [ batch_size x embed_dim ]
@@ -100,6 +106,8 @@ class ResidualTemporalBlock(nn.Module):
             out : [ batch_size x out_channels x horizon ]
         '''
         out = self.blocks[0](x) + self.time_mlp(t)
+        c = self.cond_mlp(cond)
+        breakpoint()
         out = self.blocks[1](out)
 
         return out + self.residual_conv(x)
@@ -140,6 +148,13 @@ class TemporalUnet(nn.Module):
             act_fn,
             nn.Linear(dim * 4, dim),
         )
+        self.cond_mlp = nn.Sequential(
+            # SinusoidalPosEmb(dim),
+            nn.Linear(11, dim * 4),
+            act_fn,
+            nn.Linear(dim * 4, dim),
+        )
+
 
         self.returns_condition = returns_condition
         self.condition_dropout = condition_dropout
@@ -207,6 +222,7 @@ class TemporalUnet(nn.Module):
         x = einops.rearrange(x, 'b h t -> b t h')
 
         t = self.time_mlp(time)
+        c = self.cond_mlp(cond)
 
         if self.returns_condition:
             assert returns is not None
@@ -221,20 +237,20 @@ class TemporalUnet(nn.Module):
         h = []
 
         for resnet, resnet2, downsample in self.downs:
-            x = resnet(x, t)
-            x = resnet2(x, t)
+            x = resnet(x, t, c)
+            x = resnet2(x, t, c)
             h.append(x)
             x = downsample(x)
 
-        x = self.mid_block1(x, t)
-        x = self.mid_block2(x, t)
+        x = self.mid_block1(x, t, c)
+        x = self.mid_block2(x, t, c)
 
         # import pdb; pdb.set_trace()
 
         for resnet, resnet2, upsample in self.ups:
             x = torch.cat((x, h.pop()), dim=1)
-            x = resnet(x, t)
-            x = resnet2(x, t)
+            x = resnet(x, t, c)
+            x = resnet2(x, t, c)
             x = upsample(x)
 
         x = self.final_conv(x)
@@ -257,6 +273,7 @@ class TemporalUnet(nn.Module):
         x = einops.rearrange(x, 'b h t -> b t h')
 
         t = self.time_mlp(time)
+        c = self.cond_mlp(cond)
 
         if self.returns_condition:
             assert returns is not None
@@ -271,18 +288,18 @@ class TemporalUnet(nn.Module):
         h = []
 
         for resnet, resnet2, downsample in self.downs:
-            x = resnet(x, t)
-            x = resnet2(x, t)
+            x = resnet(x, t, c)
+            x = resnet2(x, t, c)
             h.append(x)
             x = downsample(x)
 
-        x = self.mid_block1(x, t)
-        x = self.mid_block2(x, t)
+        x = self.mid_block1(x, t, c)
+        x = self.mid_block2(x, t, c)
 
         for resnet, resnet2, upsample in self.ups:
             x = torch.cat((x, h.pop()), dim=1)
-            x = resnet(x, t)
-            x = resnet2(x, t)
+            x = resnet(x, t, c)
+            x = resnet2(x, t, c)
             x = upsample(x)
 
         x = self.final_conv(x)
@@ -325,6 +342,7 @@ class MLPnet(nn.Module):
         self.calc_energy = calc_energy
         self.transition_dim = transition_dim
         self.action_dim = transition_dim - cond_dim
+        self.cond_dim = cond_dim
 
         if self.returns_condition:
             self.returns_mlp = nn.Sequential(
@@ -365,8 +383,8 @@ class MLPnet(nn.Module):
             if force_dropout:
                 returns_embed = 0*returns_embed
             t = torch.cat([t, returns_embed], dim=-1)
-
         inp = torch.cat([t, cond, x], dim=-1)
+        # inp = torch.cat([t.unsqueeze(1), cond.unsqueez(1), x], dim=-1)
         out  = self.mlp(inp)
 
         if self.calc_energy:
diff --git a/code/diffuser/utils/training.py b/code/diffuser/utils/training.py
index e8dcdb4..a4200ea 100644
--- a/code/diffuser/utils/training.py
+++ b/code/diffuser/utils/training.py
@@ -113,6 +113,7 @@ class Trainer(object):
             for i in range(self.gradient_accumulate_every):
                 batch = next(self.dataloader)
                 batch = batch_to_device(batch, device=self.device)
+                breakpoint()
                 loss, infos = self.model.loss(*batch)
                 loss = loss / self.gradient_accumulate_every
                 loss.backward()
diff --git a/code/environment.yml b/code/environment.yml
index 31a3487..47a996c 100644
--- a/code/environment.yml
+++ b/code/environment.yml
@@ -8,9 +8,9 @@ dependencies:
 - patchelf
 - pip:
     - -f https://download.pytorch.org/whl/torch_stable.html
-    - numpy
+    - numpy==1.23.0
     - gym==0.18.0
-    - mujoco-py==2.0.2.13
+    - mujoco-py
     - matplotlib==3.3.4
     - torch==1.9.1+cu111
     - typed-argument-parser
diff --git a/code/scripts/train.py b/code/scripts/train.py
index db05e5a..53b8e2f 100644
--- a/code/scripts/train.py
+++ b/code/scripts/train.py
@@ -60,6 +60,7 @@ def main(**deps):
             savepath='model_config.pkl',
             horizon=Config.horizon,
             transition_dim=observation_dim,
+            # transition_dim=observation_dim + action_dim,
             cond_dim=observation_dim,
             dim_mults=Config.dim_mults,
             returns_condition=Config.returns_condition,
@@ -95,7 +96,7 @@ def main(**deps):
             Config.model,
             savepath='model_config.pkl',
             horizon=Config.horizon,
-            transition_dim=observation_dim + action_dim,
+            transition_dim=observation_dim,
             cond_dim=observation_dim,
             dim_mults=Config.dim_mults,
             returns_condition=Config.returns_condition,diff --git a/code/config/locomotion_config.py b/code/config/locomotion_config.py
index 90173b3..b79b7e6 100644
--- a/code/config/locomotion_config.py
+++ b/code/config/locomotion_config.py
@@ -6,7 +6,7 @@ class Config(ParamsProto):
     # misc
     seed = 100
     device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
-    bucket = '/home/aajay/weights/'
+    bucket = '/home/yanyangji/research/RL/final/decision-diffuser/code/output'
     dataset = 'hopper-medium-expert-v2'
 
     ## model
diff --git a/code/diffuser/datasets/buffer.py b/code/diffuser/datasets/buffer.py
index 42bc734..29edcd8 100644
--- a/code/diffuser/datasets/buffer.py
+++ b/code/diffuser/datasets/buffer.py
@@ -9,7 +9,7 @@ class ReplayBuffer:
 
     def __init__(self, max_n_episodes, max_path_length, termination_penalty):
         self._dict = {
-            'path_lengths': np.zeros(max_n_episodes, dtype=np.int),
+            'path_lengths': np.zeros(max_n_episodes, dtype=np.int32),
         }
         self._count = 0
         self.max_n_episodes = max_n_episodes
diff --git a/code/diffuser/datasets/sequence.py b/code/diffuser/datasets/sequence.py
index 31ac84f..f256554 100644
--- a/code/diffuser/datasets/sequence.py
+++ b/code/diffuser/datasets/sequence.py
@@ -76,7 +76,8 @@ class SequenceDataset(torch.utils.data.Dataset):
         '''
             condition on current observation for planning
         '''
-        return {0: observations[0]}
+        # return {0: observations[0]}
+        return observations[0]
 
     def __len__(self):
         return len(self.indices)
diff --git a/code/diffuser/models/diffusion.py b/code/diffuser/models/diffusion.py
index a1f74ea..0c8f1a1 100644
--- a/code/diffuser/models/diffusion.py
+++ b/code/diffuser/models/diffusion.py
@@ -3,6 +3,8 @@ import torch
 from torch import nn
 import torch.nn.functional as F
 import pdb
+import clip
+from config.locomotion_config import Config
 
 import diffuser.utils as utils
 from .helpers import (
@@ -295,12 +297,22 @@ class GaussianInvDynDiffusion(nn.Module):
         condition_guidance_w=0.1, ar_inv=False, train_only_inv=False):
         super().__init__()
         self.horizon = horizon
-        self.observation_dim = observation_dim
-        self.action_dim = action_dim
+        self.observation_dim = observation_dim # 11
+        self.action_dim = action_dim # 3
         self.transition_dim = observation_dim + action_dim
         self.model = model
         self.ar_inv = ar_inv
         self.train_only_inv = train_only_inv
+
+        # Text embedding
+        device = Config.device
+        self.clip_model, _ = clip.load("ViT-B/32", device=device)
+        for p in self.clip_model.parameters():
+            p.requires_grad = False
+        with torch.no_grad():
+            self.text_cond = self.clip_model.encode_text(clip.tokenize('the man is hopping').to(device))
+        self.proj_layer = nn.Linear(512, self.observation_dim)
+
         if self.ar_inv:
             self.inv_model = ARInvModel(hidden_dim=hidden_dim, observation_dim=observation_dim, action_dim=action_dim)
         else:
diff --git a/code/diffuser/models/helpers.py b/code/diffuser/models/helpers.py
index 53859d4..b976a8d 100644
--- a/code/diffuser/models/helpers.py
+++ b/code/diffuser/models/helpers.py
@@ -91,8 +91,9 @@ def cosine_beta_schedule(timesteps, s=0.008, dtype=torch.float32):
     return torch.tensor(betas_clipped, dtype=dtype)
 
 def apply_conditioning(x, conditions, action_dim):
-    for t, val in conditions.items():
-        x[:, t, action_dim:] = val.clone()
+    # for t, val in conditions.items():
+    #     x[:, t, action_dim:] = val.clone() # batch size, time step, trajectory (all observation)
+    x[:, 0, :] = conditions.clone()
     return x
 
 #-----------------------------------------------------------------------------#
diff --git a/code/diffuser/models/temporal.py b/code/diffuser/models/temporal.py
index 63155c2..ad60336 100644
--- a/code/diffuser/models/temporal.py
+++ b/code/diffuser/models/temporal.py
@@ -89,10 +89,16 @@ class ResidualTemporalBlock(nn.Module):
             Rearrange('batch t -> batch t 1'),
         )
 
+        self.cond_mlp = nn.Sequential(
+            act_fn,
+            nn.Linear(embed_dim, out_channels),
+            Rearrange('batch t -> batch t 1')
+        )
+
         self.residual_conv = nn.Conv1d(inp_channels, out_channels, 1) \
             if inp_channels != out_channels else nn.Identity()
 
-    def forward(self, x, t):
+    def forward(self, x, t, cond):
         '''
             x : [ batch_size x inp_channels x horizon ]
             t : [ batch_size x embed_dim ]
@@ -100,6 +106,8 @@ class ResidualTemporalBlock(nn.Module):
             out : [ batch_size x out_channels x horizon ]
         '''
         out = self.blocks[0](x) + self.time_mlp(t)
+        c = self.cond_mlp(cond)
+        breakpoint()
         out = self.blocks[1](out)
 
         return out + self.residual_conv(x)
@@ -140,6 +148,13 @@ class TemporalUnet(nn.Module):
             act_fn,
             nn.Linear(dim * 4, dim),
         )
+        self.cond_mlp = nn.Sequential(
+            # SinusoidalPosEmb(dim),
+            nn.Linear(11, dim * 4),
+            act_fn,
+            nn.Linear(dim * 4, dim),
+        )
+
 
         self.returns_condition = returns_condition
         self.condition_dropout = condition_dropout
@@ -207,6 +222,8 @@ class TemporalUnet(nn.Module):
         x = einops.rearrange(x, 'b h t -> b t h')
 
         t = self.time_mlp(time)
+        c = self.cond_mlp(cond)
+        breakpoint()
 
         if self.returns_condition:
             assert returns is not None
@@ -257,6 +274,7 @@ class TemporalUnet(nn.Module):
         x = einops.rearrange(x, 'b h t -> b t h')
 
         t = self.time_mlp(time)
+        c = self.cond_mlp(cond)
 
         if self.returns_condition:
             assert returns is not None
@@ -325,6 +343,7 @@ class MLPnet(nn.Module):
         self.calc_energy = calc_energy
         self.transition_dim = transition_dim
         self.action_dim = transition_dim - cond_dim
+        self.cond_dim = cond_dim
 
         if self.returns_condition:
             self.returns_mlp = nn.Sequential(
@@ -365,8 +384,8 @@ class MLPnet(nn.Module):
             if force_dropout:
                 returns_embed = 0*returns_embed
             t = torch.cat([t, returns_embed], dim=-1)
-
         inp = torch.cat([t, cond, x], dim=-1)
+        # inp = torch.cat([t.unsqueeze(1), cond.unsqueez(1), x], dim=-1)
         out  = self.mlp(inp)
 
         if self.calc_energy:
diff --git a/code/diffuser/utils/training.py b/code/diffuser/utils/training.py
index e8dcdb4..a4200ea 100644
--- a/code/diffuser/utils/training.py
+++ b/code/diffuser/utils/training.py
@@ -113,6 +113,7 @@ class Trainer(object):
             for i in range(self.gradient_accumulate_every):
                 batch = next(self.dataloader)
                 batch = batch_to_device(batch, device=self.device)
+                breakpoint()
                 loss, infos = self.model.loss(*batch)
                 loss = loss / self.gradient_accumulate_every
                 loss.backward()
diff --git a/code/environment.yml b/code/environment.yml
index 31a3487..47a996c 100644
--- a/code/environment.yml
+++ b/code/environment.yml
@@ -8,9 +8,9 @@ dependencies:
 - patchelf
 - pip:
     - -f https://download.pytorch.org/whl/torch_stable.html
-    - numpy
+    - numpy==1.23.0
     - gym==0.18.0
-    - mujoco-py==2.0.2.13
+    - mujoco-py
     - matplotlib==3.3.4
     - torch==1.9.1+cu111
     - typed-argument-parser
diff --git a/code/scripts/train.py b/code/scripts/train.py
index db05e5a..53b8e2f 100644
--- a/code/scripts/train.py
+++ b/code/scripts/train.py
@@ -60,6 +60,7 @@ def main(**deps):
             savepath='model_config.pkl',
             horizon=Config.horizon,
             transition_dim=observation_dim,
+            # transition_dim=observation_dim + action_dim,
             cond_dim=observation_dim,
             dim_mults=Config.dim_mults,
             returns_condition=Config.returns_condition,
@@ -95,7 +96,7 @@ def main(**deps):
             Config.model,
             savepath='model_config.pkl',
             horizon=Config.horizon,
-            transition_dim=observation_dim + action_dim,
+            transition_dim=observation_dim,
             cond_dim=observation_dim,
             dim_mults=Config.dim_mults,
             returns_condition=Config.returns_condition,diff --git a/code/config/locomotion_config.py b/code/config/locomotion_config.py
index 90173b3..b79b7e6 100644
--- a/code/config/locomotion_config.py
+++ b/code/config/locomotion_config.py
@@ -6,7 +6,7 @@ class Config(ParamsProto):
     # misc
     seed = 100
     device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
-    bucket = '/home/aajay/weights/'
+    bucket = '/home/yanyangji/research/RL/final/decision-diffuser/code/output'
     dataset = 'hopper-medium-expert-v2'
 
     ## model
diff --git a/code/diffuser/datasets/buffer.py b/code/diffuser/datasets/buffer.py
index 42bc734..29edcd8 100644
--- a/code/diffuser/datasets/buffer.py
+++ b/code/diffuser/datasets/buffer.py
@@ -9,7 +9,7 @@ class ReplayBuffer:
 
     def __init__(self, max_n_episodes, max_path_length, termination_penalty):
         self._dict = {
-            'path_lengths': np.zeros(max_n_episodes, dtype=np.int),
+            'path_lengths': np.zeros(max_n_episodes, dtype=np.int32),
         }
         self._count = 0
         self.max_n_episodes = max_n_episodes
diff --git a/code/diffuser/datasets/sequence.py b/code/diffuser/datasets/sequence.py
index 31ac84f..f256554 100644
--- a/code/diffuser/datasets/sequence.py
+++ b/code/diffuser/datasets/sequence.py
@@ -76,7 +76,8 @@ class SequenceDataset(torch.utils.data.Dataset):
         '''
             condition on current observation for planning
         '''
-        return {0: observations[0]}
+        # return {0: observations[0]}
+        return observations[0]
 
     def __len__(self):
         return len(self.indices)
diff --git a/code/diffuser/models/diffusion.py b/code/diffuser/models/diffusion.py
index a1f74ea..0c8f1a1 100644
--- a/code/diffuser/models/diffusion.py
+++ b/code/diffuser/models/diffusion.py
@@ -3,6 +3,8 @@ import torch
 from torch import nn
 import torch.nn.functional as F
 import pdb
+import clip
+from config.locomotion_config import Config
 
 import diffuser.utils as utils
 from .helpers import (
@@ -295,12 +297,22 @@ class GaussianInvDynDiffusion(nn.Module):
         condition_guidance_w=0.1, ar_inv=False, train_only_inv=False):
         super().__init__()
         self.horizon = horizon
-        self.observation_dim = observation_dim
-        self.action_dim = action_dim
+        self.observation_dim = observation_dim # 11
+        self.action_dim = action_dim # 3
         self.transition_dim = observation_dim + action_dim
         self.model = model
         self.ar_inv = ar_inv
         self.train_only_inv = train_only_inv
+
+        # Text embedding
+        device = Config.device
+        self.clip_model, _ = clip.load("ViT-B/32", device=device)
+        for p in self.clip_model.parameters():
+            p.requires_grad = False
+        with torch.no_grad():
+            self.text_cond = self.clip_model.encode_text(clip.tokenize('the man is hopping').to(device))
+        self.proj_layer = nn.Linear(512, self.observation_dim)
+
         if self.ar_inv:
             self.inv_model = ARInvModel(hidden_dim=hidden_dim, observation_dim=observation_dim, action_dim=action_dim)
         else:
diff --git a/code/diffuser/models/helpers.py b/code/diffuser/models/helpers.py
index 53859d4..b976a8d 100644
--- a/code/diffuser/models/helpers.py
+++ b/code/diffuser/models/helpers.py
@@ -91,8 +91,9 @@ def cosine_beta_schedule(timesteps, s=0.008, dtype=torch.float32):
     return torch.tensor(betas_clipped, dtype=dtype)
 
 def apply_conditioning(x, conditions, action_dim):
-    for t, val in conditions.items():
-        x[:, t, action_dim:] = val.clone()
+    # for t, val in conditions.items():
+    #     x[:, t, action_dim:] = val.clone() # batch size, time step, trajectory (all observation)
+    x[:, 0, :] = conditions.clone()
     return x
 
 #-----------------------------------------------------------------------------#
diff --git a/code/diffuser/models/temporal.py b/code/diffuser/models/temporal.py
index 63155c2..6d0c8b5 100644
--- a/code/diffuser/models/temporal.py
+++ b/code/diffuser/models/temporal.py
@@ -89,6 +89,12 @@ class ResidualTemporalBlock(nn.Module):
             Rearrange('batch t -> batch t 1'),
         )
 
+        self.cond_mlp = nn.Sequential(
+            act_fn,
+            nn.Linear(embed_dim, out_channels),
+            Rearrange('batch t -> batch t 1')
+        )
+
         self.residual_conv = nn.Conv1d(inp_channels, out_channels, 1) \
             if inp_channels != out_channels else nn.Identity()
 
@@ -100,6 +106,7 @@ class ResidualTemporalBlock(nn.Module):
             out : [ batch_size x out_channels x horizon ]
         '''
         out = self.blocks[0](x) + self.time_mlp(t)
+        breakpoint()
         out = self.blocks[1](out)
 
         return out + self.residual_conv(x)
@@ -140,6 +147,13 @@ class TemporalUnet(nn.Module):
             act_fn,
             nn.Linear(dim * 4, dim),
         )
+        self.cond_mlp = nn.Sequential(
+            # SinusoidalPosEmb(dim),
+            nn.Linear(11, dim * 4),
+            act_fn,
+            nn.Linear(dim * 4, dim),
+        )
+
 
         self.returns_condition = returns_condition
         self.condition_dropout = condition_dropout
@@ -207,6 +221,8 @@ class TemporalUnet(nn.Module):
         x = einops.rearrange(x, 'b h t -> b t h')
 
         t = self.time_mlp(time)
+        c = self.cond_mlp(cond)
+        # breakpoint()
 
         if self.returns_condition:
             assert returns is not None
@@ -257,6 +273,7 @@ class TemporalUnet(nn.Module):
         x = einops.rearrange(x, 'b h t -> b t h')
 
         t = self.time_mlp(time)
+        c = self.cond_mlp(cond)
 
         if self.returns_condition:
             assert returns is not None
@@ -325,6 +342,7 @@ class MLPnet(nn.Module):
         self.calc_energy = calc_energy
         self.transition_dim = transition_dim
         self.action_dim = transition_dim - cond_dim
+        self.cond_dim = cond_dim
 
         if self.returns_condition:
             self.returns_mlp = nn.Sequential(
@@ -365,8 +383,8 @@ class MLPnet(nn.Module):
             if force_dropout:
                 returns_embed = 0*returns_embed
             t = torch.cat([t, returns_embed], dim=-1)
-
         inp = torch.cat([t, cond, x], dim=-1)
+        # inp = torch.cat([t.unsqueeze(1), cond.unsqueez(1), x], dim=-1)
         out  = self.mlp(inp)
 
         if self.calc_energy:
diff --git a/code/diffuser/utils/training.py b/code/diffuser/utils/training.py
index e8dcdb4..a4200ea 100644
--- a/code/diffuser/utils/training.py
+++ b/code/diffuser/utils/training.py
@@ -113,6 +113,7 @@ class Trainer(object):
             for i in range(self.gradient_accumulate_every):
                 batch = next(self.dataloader)
                 batch = batch_to_device(batch, device=self.device)
+                breakpoint()
                 loss, infos = self.model.loss(*batch)
                 loss = loss / self.gradient_accumulate_every
                 loss.backward()
diff --git a/code/environment.yml b/code/environment.yml
index 31a3487..47a996c 100644
--- a/code/environment.yml
+++ b/code/environment.yml
@@ -8,9 +8,9 @@ dependencies:
 - patchelf
 - pip:
     - -f https://download.pytorch.org/whl/torch_stable.html
-    - numpy
+    - numpy==1.23.0
     - gym==0.18.0
-    - mujoco-py==2.0.2.13
+    - mujoco-py
     - matplotlib==3.3.4
     - torch==1.9.1+cu111
     - typed-argument-parser
diff --git a/code/scripts/train.py b/code/scripts/train.py
index db05e5a..53b8e2f 100644
--- a/code/scripts/train.py
+++ b/code/scripts/train.py
@@ -60,6 +60,7 @@ def main(**deps):
             savepath='model_config.pkl',
             horizon=Config.horizon,
             transition_dim=observation_dim,
+            # transition_dim=observation_dim + action_dim,
             cond_dim=observation_dim,
             dim_mults=Config.dim_mults,
             returns_condition=Config.returns_condition,
@@ -95,7 +96,7 @@ def main(**deps):
             Config.model,
             savepath='model_config.pkl',
             horizon=Config.horizon,
-            transition_dim=observation_dim + action_dim,
+            transition_dim=observation_dim,
             cond_dim=observation_dim,
             dim_mults=Config.dim_mults,
             returns_condition=Config.returns_condition,diff --git a/code/config/locomotion_config.py b/code/config/locomotion_config.py
index 90173b3..b79b7e6 100644
--- a/code/config/locomotion_config.py
+++ b/code/config/locomotion_config.py
@@ -6,7 +6,7 @@ class Config(ParamsProto):
     # misc
     seed = 100
     device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
-    bucket = '/home/aajay/weights/'
+    bucket = '/home/yanyangji/research/RL/final/decision-diffuser/code/output'
     dataset = 'hopper-medium-expert-v2'
 
     ## model
diff --git a/code/diffuser/datasets/buffer.py b/code/diffuser/datasets/buffer.py
index 42bc734..29edcd8 100644
--- a/code/diffuser/datasets/buffer.py
+++ b/code/diffuser/datasets/buffer.py
@@ -9,7 +9,7 @@ class ReplayBuffer:
 
     def __init__(self, max_n_episodes, max_path_length, termination_penalty):
         self._dict = {
-            'path_lengths': np.zeros(max_n_episodes, dtype=np.int),
+            'path_lengths': np.zeros(max_n_episodes, dtype=np.int32),
         }
         self._count = 0
         self.max_n_episodes = max_n_episodes
diff --git a/code/diffuser/datasets/sequence.py b/code/diffuser/datasets/sequence.py
index 31ac84f..f256554 100644
--- a/code/diffuser/datasets/sequence.py
+++ b/code/diffuser/datasets/sequence.py
@@ -76,7 +76,8 @@ class SequenceDataset(torch.utils.data.Dataset):
         '''
             condition on current observation for planning
         '''
-        return {0: observations[0]}
+        # return {0: observations[0]}
+        return observations[0]
 
     def __len__(self):
         return len(self.indices)
diff --git a/code/diffuser/models/diffusion.py b/code/diffuser/models/diffusion.py
index a1f74ea..0c8f1a1 100644
--- a/code/diffuser/models/diffusion.py
+++ b/code/diffuser/models/diffusion.py
@@ -3,6 +3,8 @@ import torch
 from torch import nn
 import torch.nn.functional as F
 import pdb
+import clip
+from config.locomotion_config import Config
 
 import diffuser.utils as utils
 from .helpers import (
@@ -295,12 +297,22 @@ class GaussianInvDynDiffusion(nn.Module):
         condition_guidance_w=0.1, ar_inv=False, train_only_inv=False):
         super().__init__()
         self.horizon = horizon
-        self.observation_dim = observation_dim
-        self.action_dim = action_dim
+        self.observation_dim = observation_dim # 11
+        self.action_dim = action_dim # 3
         self.transition_dim = observation_dim + action_dim
         self.model = model
         self.ar_inv = ar_inv
         self.train_only_inv = train_only_inv
+
+        # Text embedding
+        device = Config.device
+        self.clip_model, _ = clip.load("ViT-B/32", device=device)
+        for p in self.clip_model.parameters():
+            p.requires_grad = False
+        with torch.no_grad():
+            self.text_cond = self.clip_model.encode_text(clip.tokenize('the man is hopping').to(device))
+        self.proj_layer = nn.Linear(512, self.observation_dim)
+
         if self.ar_inv:
             self.inv_model = ARInvModel(hidden_dim=hidden_dim, observation_dim=observation_dim, action_dim=action_dim)
         else:
diff --git a/code/diffuser/models/helpers.py b/code/diffuser/models/helpers.py
index 53859d4..b976a8d 100644
--- a/code/diffuser/models/helpers.py
+++ b/code/diffuser/models/helpers.py
@@ -91,8 +91,9 @@ def cosine_beta_schedule(timesteps, s=0.008, dtype=torch.float32):
     return torch.tensor(betas_clipped, dtype=dtype)
 
 def apply_conditioning(x, conditions, action_dim):
-    for t, val in conditions.items():
-        x[:, t, action_dim:] = val.clone()
+    # for t, val in conditions.items():
+    #     x[:, t, action_dim:] = val.clone() # batch size, time step, trajectory (all observation)
+    x[:, 0, :] = conditions.clone()
     return x
 
 #-----------------------------------------------------------------------------#
diff --git a/code/diffuser/models/temporal.py b/code/diffuser/models/temporal.py
index 63155c2..3464ba6 100644
--- a/code/diffuser/models/temporal.py
+++ b/code/diffuser/models/temporal.py
@@ -89,17 +89,24 @@ class ResidualTemporalBlock(nn.Module):
             Rearrange('batch t -> batch t 1'),
         )
 
+        self.cond_mlp = nn.Sequential(
+            act_fn,
+            nn.Linear(128, out_channels),
+            Rearrange('batch t -> batch t 1')
+        )
+
         self.residual_conv = nn.Conv1d(inp_channels, out_channels, 1) \
             if inp_channels != out_channels else nn.Identity()
 
-    def forward(self, x, t):
+    def forward(self, x, t, cond):
         '''
             x : [ batch_size x inp_channels x horizon ]
             t : [ batch_size x embed_dim ]
             returns:
             out : [ batch_size x out_channels x horizon ]
         '''
-        out = self.blocks[0](x) + self.time_mlp(t)
+        out = self.blocks[0](x) + self.time_mlp(t) + self.cond_mlp(cond)
+        breakpoint()
         out = self.blocks[1](out)
 
         return out + self.residual_conv(x)
@@ -140,6 +147,13 @@ class TemporalUnet(nn.Module):
             act_fn,
             nn.Linear(dim * 4, dim),
         )
+        self.cond_mlp = nn.Sequential(
+            # SinusoidalPosEmb(dim),
+            nn.Linear(11, dim * 4),
+            act_fn,
+            nn.Linear(dim * 4, dim),
+        )
+
 
         self.returns_condition = returns_condition
         self.condition_dropout = condition_dropout
@@ -207,6 +221,7 @@ class TemporalUnet(nn.Module):
         x = einops.rearrange(x, 'b h t -> b t h')
 
         t = self.time_mlp(time)
+        c = self.cond_mlp(cond)
 
         if self.returns_condition:
             assert returns is not None
@@ -221,20 +236,20 @@ class TemporalUnet(nn.Module):
         h = []
 
         for resnet, resnet2, downsample in self.downs:
-            x = resnet(x, t)
-            x = resnet2(x, t)
+            x = resnet(x, t, c)
+            x = resnet2(x, t, c)
             h.append(x)
             x = downsample(x)
 
-        x = self.mid_block1(x, t)
-        x = self.mid_block2(x, t)
+        x = self.mid_block1(x, t, c)
+        x = self.mid_block2(x, t, c)
 
         # import pdb; pdb.set_trace()
 
         for resnet, resnet2, upsample in self.ups:
             x = torch.cat((x, h.pop()), dim=1)
-            x = resnet(x, t)
-            x = resnet2(x, t)
+            x = resnet(x, t, c)
+            x = resnet2(x, t, c)
             x = upsample(x)
 
         x = self.final_conv(x)
@@ -257,6 +272,7 @@ class TemporalUnet(nn.Module):
         x = einops.rearrange(x, 'b h t -> b t h')
 
         t = self.time_mlp(time)
+        c = self.cond_mlp(cond)
 
         if self.returns_condition:
             assert returns is not None
@@ -271,18 +287,18 @@ class TemporalUnet(nn.Module):
         h = []
 
         for resnet, resnet2, downsample in self.downs:
-            x = resnet(x, t)
-            x = resnet2(x, t)
+            x = resnet(x, t, c)
+            x = resnet2(x, t, c)
             h.append(x)
             x = downsample(x)
 
-        x = self.mid_block1(x, t)
-        x = self.mid_block2(x, t)
+        x = self.mid_block1(x, t, c)
+        x = self.mid_block2(x, t, c)
 
         for resnet, resnet2, upsample in self.ups:
             x = torch.cat((x, h.pop()), dim=1)
-            x = resnet(x, t)
-            x = resnet2(x, t)
+            x = resnet(x, t, c)
+            x = resnet2(x, t, c)
             x = upsample(x)
 
         x = self.final_conv(x)
@@ -325,6 +341,7 @@ class MLPnet(nn.Module):
         self.calc_energy = calc_energy
         self.transition_dim = transition_dim
         self.action_dim = transition_dim - cond_dim
+        self.cond_dim = cond_dim
 
         if self.returns_condition:
             self.returns_mlp = nn.Sequential(
@@ -365,8 +382,8 @@ class MLPnet(nn.Module):
             if force_dropout:
                 returns_embed = 0*returns_embed
             t = torch.cat([t, returns_embed], dim=-1)
-
         inp = torch.cat([t, cond, x], dim=-1)
+        # inp = torch.cat([t.unsqueeze(1), cond.unsqueez(1), x], dim=-1)
         out  = self.mlp(inp)
 
         if self.calc_energy:
diff --git a/code/diffuser/utils/training.py b/code/diffuser/utils/training.py
index e8dcdb4..a4200ea 100644
--- a/code/diffuser/utils/training.py
+++ b/code/diffuser/utils/training.py
@@ -113,6 +113,7 @@ class Trainer(object):
             for i in range(self.gradient_accumulate_every):
                 batch = next(self.dataloader)
                 batch = batch_to_device(batch, device=self.device)
+                breakpoint()
                 loss, infos = self.model.loss(*batch)
                 loss = loss / self.gradient_accumulate_every
                 loss.backward()
diff --git a/code/environment.yml b/code/environment.yml
index 31a3487..47a996c 100644
--- a/code/environment.yml
+++ b/code/environment.yml
@@ -8,9 +8,9 @@ dependencies:
 - patchelf
 - pip:
     - -f https://download.pytorch.org/whl/torch_stable.html
-    - numpy
+    - numpy==1.23.0
     - gym==0.18.0
-    - mujoco-py==2.0.2.13
+    - mujoco-py
     - matplotlib==3.3.4
     - torch==1.9.1+cu111
     - typed-argument-parser
diff --git a/code/scripts/train.py b/code/scripts/train.py
index db05e5a..53b8e2f 100644
--- a/code/scripts/train.py
+++ b/code/scripts/train.py
@@ -60,6 +60,7 @@ def main(**deps):
             savepath='model_config.pkl',
             horizon=Config.horizon,
             transition_dim=observation_dim,
+            # transition_dim=observation_dim + action_dim,
             cond_dim=observation_dim,
             dim_mults=Config.dim_mults,
             returns_condition=Config.returns_condition,
@@ -95,7 +96,7 @@ def main(**deps):
             Config.model,
             savepath='model_config.pkl',
             horizon=Config.horizon,
-            transition_dim=observation_dim + action_dim,
+            transition_dim=observation_dim,
             cond_dim=observation_dim,
             dim_mults=Config.dim_mults,
             returns_condition=Config.returns_condition,diff --git a/code/config/locomotion_config.py b/code/config/locomotion_config.py
index 90173b3..b79b7e6 100644
--- a/code/config/locomotion_config.py
+++ b/code/config/locomotion_config.py
@@ -6,7 +6,7 @@ class Config(ParamsProto):
     # misc
     seed = 100
     device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
-    bucket = '/home/aajay/weights/'
+    bucket = '/home/yanyangji/research/RL/final/decision-diffuser/code/output'
     dataset = 'hopper-medium-expert-v2'
 
     ## model
diff --git a/code/diffuser/datasets/buffer.py b/code/diffuser/datasets/buffer.py
index 42bc734..29edcd8 100644
--- a/code/diffuser/datasets/buffer.py
+++ b/code/diffuser/datasets/buffer.py
@@ -9,7 +9,7 @@ class ReplayBuffer:
 
     def __init__(self, max_n_episodes, max_path_length, termination_penalty):
         self._dict = {
-            'path_lengths': np.zeros(max_n_episodes, dtype=np.int),
+            'path_lengths': np.zeros(max_n_episodes, dtype=np.int32),
         }
         self._count = 0
         self.max_n_episodes = max_n_episodes
diff --git a/code/diffuser/datasets/sequence.py b/code/diffuser/datasets/sequence.py
index 31ac84f..f256554 100644
--- a/code/diffuser/datasets/sequence.py
+++ b/code/diffuser/datasets/sequence.py
@@ -76,7 +76,8 @@ class SequenceDataset(torch.utils.data.Dataset):
         '''
             condition on current observation for planning
         '''
-        return {0: observations[0]}
+        # return {0: observations[0]}
+        return observations[0]
 
     def __len__(self):
         return len(self.indices)
diff --git a/code/diffuser/models/diffusion.py b/code/diffuser/models/diffusion.py
index a1f74ea..0c8f1a1 100644
--- a/code/diffuser/models/diffusion.py
+++ b/code/diffuser/models/diffusion.py
@@ -3,6 +3,8 @@ import torch
 from torch import nn
 import torch.nn.functional as F
 import pdb
+import clip
+from config.locomotion_config import Config
 
 import diffuser.utils as utils
 from .helpers import (
@@ -295,12 +297,22 @@ class GaussianInvDynDiffusion(nn.Module):
         condition_guidance_w=0.1, ar_inv=False, train_only_inv=False):
         super().__init__()
         self.horizon = horizon
-        self.observation_dim = observation_dim
-        self.action_dim = action_dim
+        self.observation_dim = observation_dim # 11
+        self.action_dim = action_dim # 3
         self.transition_dim = observation_dim + action_dim
         self.model = model
         self.ar_inv = ar_inv
         self.train_only_inv = train_only_inv
+
+        # Text embedding
+        device = Config.device
+        self.clip_model, _ = clip.load("ViT-B/32", device=device)
+        for p in self.clip_model.parameters():
+            p.requires_grad = False
+        with torch.no_grad():
+            self.text_cond = self.clip_model.encode_text(clip.tokenize('the man is hopping').to(device))
+        self.proj_layer = nn.Linear(512, self.observation_dim)
+
         if self.ar_inv:
             self.inv_model = ARInvModel(hidden_dim=hidden_dim, observation_dim=observation_dim, action_dim=action_dim)
         else:
diff --git a/code/diffuser/models/helpers.py b/code/diffuser/models/helpers.py
index 53859d4..b976a8d 100644
--- a/code/diffuser/models/helpers.py
+++ b/code/diffuser/models/helpers.py
@@ -91,8 +91,9 @@ def cosine_beta_schedule(timesteps, s=0.008, dtype=torch.float32):
     return torch.tensor(betas_clipped, dtype=dtype)
 
 def apply_conditioning(x, conditions, action_dim):
-    for t, val in conditions.items():
-        x[:, t, action_dim:] = val.clone()
+    # for t, val in conditions.items():
+    #     x[:, t, action_dim:] = val.clone() # batch size, time step, trajectory (all observation)
+    x[:, 0, :] = conditions.clone()
     return x
 
 #-----------------------------------------------------------------------------#
diff --git a/code/diffuser/models/temporal.py b/code/diffuser/models/temporal.py
index 63155c2..2daddb2 100644
--- a/code/diffuser/models/temporal.py
+++ b/code/diffuser/models/temporal.py
@@ -89,17 +89,24 @@ class ResidualTemporalBlock(nn.Module):
             Rearrange('batch t -> batch t 1'),
         )
 
+        self.cond_mlp = nn.Sequential(
+            act_fn,
+            nn.Linear(128, out_channels),
+            Rearrange('batch t -> batch t 1')
+        )
+
         self.residual_conv = nn.Conv1d(inp_channels, out_channels, 1) \
             if inp_channels != out_channels else nn.Identity()
 
-    def forward(self, x, t):
+    def forward(self, x, t, cond):
         '''
             x : [ batch_size x inp_channels x horizon ]
             t : [ batch_size x embed_dim ]
             returns:
             out : [ batch_size x out_channels x horizon ]
         '''
-        out = self.blocks[0](x) + self.time_mlp(t)
+        out = self.blocks[0](x) + self.time_mlp(t) + self.cond_mlp(cond)
+        # breakpoint()
         out = self.blocks[1](out)
 
         return out + self.residual_conv(x)
@@ -140,6 +147,13 @@ class TemporalUnet(nn.Module):
             act_fn,
             nn.Linear(dim * 4, dim),
         )
+        self.cond_mlp = nn.Sequential(
+            # SinusoidalPosEmb(dim),
+            nn.Linear(11, dim * 4),
+            act_fn,
+            nn.Linear(dim * 4, dim),
+        )
+
 
         self.returns_condition = returns_condition
         self.condition_dropout = condition_dropout
@@ -207,6 +221,7 @@ class TemporalUnet(nn.Module):
         x = einops.rearrange(x, 'b h t -> b t h')
 
         t = self.time_mlp(time)
+        c = self.cond_mlp(cond)
 
         if self.returns_condition:
             assert returns is not None
@@ -221,20 +236,20 @@ class TemporalUnet(nn.Module):
         h = []
 
         for resnet, resnet2, downsample in self.downs:
-            x = resnet(x, t)
-            x = resnet2(x, t)
+            x = resnet(x, t, c)
+            x = resnet2(x, t, c)
             h.append(x)
             x = downsample(x)
 
-        x = self.mid_block1(x, t)
-        x = self.mid_block2(x, t)
+        x = self.mid_block1(x, t, c)
+        x = self.mid_block2(x, t, c)
 
         # import pdb; pdb.set_trace()
 
         for resnet, resnet2, upsample in self.ups:
             x = torch.cat((x, h.pop()), dim=1)
-            x = resnet(x, t)
-            x = resnet2(x, t)
+            x = resnet(x, t, c)
+            x = resnet2(x, t, c)
             x = upsample(x)
 
         x = self.final_conv(x)
@@ -257,6 +272,7 @@ class TemporalUnet(nn.Module):
         x = einops.rearrange(x, 'b h t -> b t h')
 
         t = self.time_mlp(time)
+        c = self.cond_mlp(cond)
 
         if self.returns_condition:
             assert returns is not None
@@ -271,18 +287,18 @@ class TemporalUnet(nn.Module):
         h = []
 
         for resnet, resnet2, downsample in self.downs:
-            x = resnet(x, t)
-            x = resnet2(x, t)
+            x = resnet(x, t, c)
+            x = resnet2(x, t, c)
             h.append(x)
             x = downsample(x)
 
-        x = self.mid_block1(x, t)
-        x = self.mid_block2(x, t)
+        x = self.mid_block1(x, t, c)
+        x = self.mid_block2(x, t, c)
 
         for resnet, resnet2, upsample in self.ups:
             x = torch.cat((x, h.pop()), dim=1)
-            x = resnet(x, t)
-            x = resnet2(x, t)
+            x = resnet(x, t, c)
+            x = resnet2(x, t, c)
             x = upsample(x)
 
         x = self.final_conv(x)
@@ -325,6 +341,7 @@ class MLPnet(nn.Module):
         self.calc_energy = calc_energy
         self.transition_dim = transition_dim
         self.action_dim = transition_dim - cond_dim
+        self.cond_dim = cond_dim
 
         if self.returns_condition:
             self.returns_mlp = nn.Sequential(
@@ -365,8 +382,8 @@ class MLPnet(nn.Module):
             if force_dropout:
                 returns_embed = 0*returns_embed
             t = torch.cat([t, returns_embed], dim=-1)
-
         inp = torch.cat([t, cond, x], dim=-1)
+        # inp = torch.cat([t.unsqueeze(1), cond.unsqueez(1), x], dim=-1)
         out  = self.mlp(inp)
 
         if self.calc_energy:
diff --git a/code/diffuser/utils/training.py b/code/diffuser/utils/training.py
index e8dcdb4..a4200ea 100644
--- a/code/diffuser/utils/training.py
+++ b/code/diffuser/utils/training.py
@@ -113,6 +113,7 @@ class Trainer(object):
             for i in range(self.gradient_accumulate_every):
                 batch = next(self.dataloader)
                 batch = batch_to_device(batch, device=self.device)
+                breakpoint()
                 loss, infos = self.model.loss(*batch)
                 loss = loss / self.gradient_accumulate_every
                 loss.backward()
diff --git a/code/environment.yml b/code/environment.yml
index 31a3487..47a996c 100644
--- a/code/environment.yml
+++ b/code/environment.yml
@@ -8,9 +8,9 @@ dependencies:
 - patchelf
 - pip:
     - -f https://download.pytorch.org/whl/torch_stable.html
-    - numpy
+    - numpy==1.23.0
     - gym==0.18.0
-    - mujoco-py==2.0.2.13
+    - mujoco-py
     - matplotlib==3.3.4
     - torch==1.9.1+cu111
     - typed-argument-parser
diff --git a/code/scripts/train.py b/code/scripts/train.py
index db05e5a..53b8e2f 100644
--- a/code/scripts/train.py
+++ b/code/scripts/train.py
@@ -60,6 +60,7 @@ def main(**deps):
             savepath='model_config.pkl',
             horizon=Config.horizon,
             transition_dim=observation_dim,
+            # transition_dim=observation_dim + action_dim,
             cond_dim=observation_dim,
             dim_mults=Config.dim_mults,
             returns_condition=Config.returns_condition,
@@ -95,7 +96,7 @@ def main(**deps):
             Config.model,
             savepath='model_config.pkl',
             horizon=Config.horizon,
-            transition_dim=observation_dim + action_dim,
+            transition_dim=observation_dim,
             cond_dim=observation_dim,
             dim_mults=Config.dim_mults,
             returns_condition=Config.returns_condition,diff --git a/code/config/locomotion_config.py b/code/config/locomotion_config.py
index 90173b3..b79b7e6 100644
--- a/code/config/locomotion_config.py
+++ b/code/config/locomotion_config.py
@@ -6,7 +6,7 @@ class Config(ParamsProto):
     # misc
     seed = 100
     device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
-    bucket = '/home/aajay/weights/'
+    bucket = '/home/yanyangji/research/RL/final/decision-diffuser/code/output'
     dataset = 'hopper-medium-expert-v2'
 
     ## model
diff --git a/code/diffuser/datasets/buffer.py b/code/diffuser/datasets/buffer.py
index 42bc734..29edcd8 100644
--- a/code/diffuser/datasets/buffer.py
+++ b/code/diffuser/datasets/buffer.py
@@ -9,7 +9,7 @@ class ReplayBuffer:
 
     def __init__(self, max_n_episodes, max_path_length, termination_penalty):
         self._dict = {
-            'path_lengths': np.zeros(max_n_episodes, dtype=np.int),
+            'path_lengths': np.zeros(max_n_episodes, dtype=np.int32),
         }
         self._count = 0
         self.max_n_episodes = max_n_episodes
diff --git a/code/diffuser/datasets/sequence.py b/code/diffuser/datasets/sequence.py
index 31ac84f..f256554 100644
--- a/code/diffuser/datasets/sequence.py
+++ b/code/diffuser/datasets/sequence.py
@@ -76,7 +76,8 @@ class SequenceDataset(torch.utils.data.Dataset):
         '''
             condition on current observation for planning
         '''
-        return {0: observations[0]}
+        # return {0: observations[0]}
+        return observations[0]
 
     def __len__(self):
         return len(self.indices)
diff --git a/code/diffuser/models/diffusion.py b/code/diffuser/models/diffusion.py
index a1f74ea..0c8f1a1 100644
--- a/code/diffuser/models/diffusion.py
+++ b/code/diffuser/models/diffusion.py
@@ -3,6 +3,8 @@ import torch
 from torch import nn
 import torch.nn.functional as F
 import pdb
+import clip
+from config.locomotion_config import Config
 
 import diffuser.utils as utils
 from .helpers import (
@@ -295,12 +297,22 @@ class GaussianInvDynDiffusion(nn.Module):
         condition_guidance_w=0.1, ar_inv=False, train_only_inv=False):
         super().__init__()
         self.horizon = horizon
-        self.observation_dim = observation_dim
-        self.action_dim = action_dim
+        self.observation_dim = observation_dim # 11
+        self.action_dim = action_dim # 3
         self.transition_dim = observation_dim + action_dim
         self.model = model
         self.ar_inv = ar_inv
         self.train_only_inv = train_only_inv
+
+        # Text embedding
+        device = Config.device
+        self.clip_model, _ = clip.load("ViT-B/32", device=device)
+        for p in self.clip_model.parameters():
+            p.requires_grad = False
+        with torch.no_grad():
+            self.text_cond = self.clip_model.encode_text(clip.tokenize('the man is hopping').to(device))
+        self.proj_layer = nn.Linear(512, self.observation_dim)
+
         if self.ar_inv:
             self.inv_model = ARInvModel(hidden_dim=hidden_dim, observation_dim=observation_dim, action_dim=action_dim)
         else:
diff --git a/code/diffuser/models/helpers.py b/code/diffuser/models/helpers.py
index 53859d4..b976a8d 100644
--- a/code/diffuser/models/helpers.py
+++ b/code/diffuser/models/helpers.py
@@ -91,8 +91,9 @@ def cosine_beta_schedule(timesteps, s=0.008, dtype=torch.float32):
     return torch.tensor(betas_clipped, dtype=dtype)
 
 def apply_conditioning(x, conditions, action_dim):
-    for t, val in conditions.items():
-        x[:, t, action_dim:] = val.clone()
+    # for t, val in conditions.items():
+    #     x[:, t, action_dim:] = val.clone() # batch size, time step, trajectory (all observation)
+    x[:, 0, :] = conditions.clone()
     return x
 
 #-----------------------------------------------------------------------------#
diff --git a/code/diffuser/models/temporal.py b/code/diffuser/models/temporal.py
index 63155c2..2daddb2 100644
--- a/code/diffuser/models/temporal.py
+++ b/code/diffuser/models/temporal.py
@@ -89,17 +89,24 @@ class ResidualTemporalBlock(nn.Module):
             Rearrange('batch t -> batch t 1'),
         )
 
+        self.cond_mlp = nn.Sequential(
+            act_fn,
+            nn.Linear(128, out_channels),
+            Rearrange('batch t -> batch t 1')
+        )
+
         self.residual_conv = nn.Conv1d(inp_channels, out_channels, 1) \
             if inp_channels != out_channels else nn.Identity()
 
-    def forward(self, x, t):
+    def forward(self, x, t, cond):
         '''
             x : [ batch_size x inp_channels x horizon ]
             t : [ batch_size x embed_dim ]
             returns:
             out : [ batch_size x out_channels x horizon ]
         '''
-        out = self.blocks[0](x) + self.time_mlp(t)
+        out = self.blocks[0](x) + self.time_mlp(t) + self.cond_mlp(cond)
+        # breakpoint()
         out = self.blocks[1](out)
 
         return out + self.residual_conv(x)
@@ -140,6 +147,13 @@ class TemporalUnet(nn.Module):
             act_fn,
             nn.Linear(dim * 4, dim),
         )
+        self.cond_mlp = nn.Sequential(
+            # SinusoidalPosEmb(dim),
+            nn.Linear(11, dim * 4),
+            act_fn,
+            nn.Linear(dim * 4, dim),
+        )
+
 
         self.returns_condition = returns_condition
         self.condition_dropout = condition_dropout
@@ -207,6 +221,7 @@ class TemporalUnet(nn.Module):
         x = einops.rearrange(x, 'b h t -> b t h')
 
         t = self.time_mlp(time)
+        c = self.cond_mlp(cond)
 
         if self.returns_condition:
             assert returns is not None
@@ -221,20 +236,20 @@ class TemporalUnet(nn.Module):
         h = []
 
         for resnet, resnet2, downsample in self.downs:
-            x = resnet(x, t)
-            x = resnet2(x, t)
+            x = resnet(x, t, c)
+            x = resnet2(x, t, c)
             h.append(x)
             x = downsample(x)
 
-        x = self.mid_block1(x, t)
-        x = self.mid_block2(x, t)
+        x = self.mid_block1(x, t, c)
+        x = self.mid_block2(x, t, c)
 
         # import pdb; pdb.set_trace()
 
         for resnet, resnet2, upsample in self.ups:
             x = torch.cat((x, h.pop()), dim=1)
-            x = resnet(x, t)
-            x = resnet2(x, t)
+            x = resnet(x, t, c)
+            x = resnet2(x, t, c)
             x = upsample(x)
 
         x = self.final_conv(x)
@@ -257,6 +272,7 @@ class TemporalUnet(nn.Module):
         x = einops.rearrange(x, 'b h t -> b t h')
 
         t = self.time_mlp(time)
+        c = self.cond_mlp(cond)
 
         if self.returns_condition:
             assert returns is not None
@@ -271,18 +287,18 @@ class TemporalUnet(nn.Module):
         h = []
 
         for resnet, resnet2, downsample in self.downs:
-            x = resnet(x, t)
-            x = resnet2(x, t)
+            x = resnet(x, t, c)
+            x = resnet2(x, t, c)
             h.append(x)
             x = downsample(x)
 
-        x = self.mid_block1(x, t)
-        x = self.mid_block2(x, t)
+        x = self.mid_block1(x, t, c)
+        x = self.mid_block2(x, t, c)
 
         for resnet, resnet2, upsample in self.ups:
             x = torch.cat((x, h.pop()), dim=1)
-            x = resnet(x, t)
-            x = resnet2(x, t)
+            x = resnet(x, t, c)
+            x = resnet2(x, t, c)
             x = upsample(x)
 
         x = self.final_conv(x)
@@ -325,6 +341,7 @@ class MLPnet(nn.Module):
         self.calc_energy = calc_energy
         self.transition_dim = transition_dim
         self.action_dim = transition_dim - cond_dim
+        self.cond_dim = cond_dim
 
         if self.returns_condition:
             self.returns_mlp = nn.Sequential(
@@ -365,8 +382,8 @@ class MLPnet(nn.Module):
             if force_dropout:
                 returns_embed = 0*returns_embed
             t = torch.cat([t, returns_embed], dim=-1)
-
         inp = torch.cat([t, cond, x], dim=-1)
+        # inp = torch.cat([t.unsqueeze(1), cond.unsqueez(1), x], dim=-1)
         out  = self.mlp(inp)
 
         if self.calc_energy:
diff --git a/code/diffuser/utils/training.py b/code/diffuser/utils/training.py
index e8dcdb4..0b6e0bf 100644
--- a/code/diffuser/utils/training.py
+++ b/code/diffuser/utils/training.py
@@ -113,6 +113,7 @@ class Trainer(object):
             for i in range(self.gradient_accumulate_every):
                 batch = next(self.dataloader)
                 batch = batch_to_device(batch, device=self.device)
+                # breakpoint()
                 loss, infos = self.model.loss(*batch)
                 loss = loss / self.gradient_accumulate_every
                 loss.backward()
diff --git a/code/environment.yml b/code/environment.yml
index 31a3487..47a996c 100644
--- a/code/environment.yml
+++ b/code/environment.yml
@@ -8,9 +8,9 @@ dependencies:
 - patchelf
 - pip:
     - -f https://download.pytorch.org/whl/torch_stable.html
-    - numpy
+    - numpy==1.23.0
     - gym==0.18.0
-    - mujoco-py==2.0.2.13
+    - mujoco-py
     - matplotlib==3.3.4
     - torch==1.9.1+cu111
     - typed-argument-parser
diff --git a/code/scripts/train.py b/code/scripts/train.py
index db05e5a..53b8e2f 100644
--- a/code/scripts/train.py
+++ b/code/scripts/train.py
@@ -60,6 +60,7 @@ def main(**deps):
             savepath='model_config.pkl',
             horizon=Config.horizon,
             transition_dim=observation_dim,
+            # transition_dim=observation_dim + action_dim,
             cond_dim=observation_dim,
             dim_mults=Config.dim_mults,
             returns_condition=Config.returns_condition,
@@ -95,7 +96,7 @@ def main(**deps):
             Config.model,
             savepath='model_config.pkl',
             horizon=Config.horizon,
-            transition_dim=observation_dim + action_dim,
+            transition_dim=observation_dim,
             cond_dim=observation_dim,
             dim_mults=Config.dim_mults,
             returns_condition=Config.returns_condition,diff --git a/code/config/locomotion_config.py b/code/config/locomotion_config.py
index 90173b3..b79b7e6 100644
--- a/code/config/locomotion_config.py
+++ b/code/config/locomotion_config.py
@@ -6,7 +6,7 @@ class Config(ParamsProto):
     # misc
     seed = 100
     device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
-    bucket = '/home/aajay/weights/'
+    bucket = '/home/yanyangji/research/RL/final/decision-diffuser/code/output'
     dataset = 'hopper-medium-expert-v2'
 
     ## model
diff --git a/code/diffuser/datasets/buffer.py b/code/diffuser/datasets/buffer.py
index 42bc734..29edcd8 100644
--- a/code/diffuser/datasets/buffer.py
+++ b/code/diffuser/datasets/buffer.py
@@ -9,7 +9,7 @@ class ReplayBuffer:
 
     def __init__(self, max_n_episodes, max_path_length, termination_penalty):
         self._dict = {
-            'path_lengths': np.zeros(max_n_episodes, dtype=np.int),
+            'path_lengths': np.zeros(max_n_episodes, dtype=np.int32),
         }
         self._count = 0
         self.max_n_episodes = max_n_episodes
diff --git a/code/diffuser/datasets/sequence.py b/code/diffuser/datasets/sequence.py
index 31ac84f..f256554 100644
--- a/code/diffuser/datasets/sequence.py
+++ b/code/diffuser/datasets/sequence.py
@@ -76,7 +76,8 @@ class SequenceDataset(torch.utils.data.Dataset):
         '''
             condition on current observation for planning
         '''
-        return {0: observations[0]}
+        # return {0: observations[0]}
+        return observations[0]
 
     def __len__(self):
         return len(self.indices)
diff --git a/code/diffuser/models/diffusion.py b/code/diffuser/models/diffusion.py
index a1f74ea..0c8f1a1 100644
--- a/code/diffuser/models/diffusion.py
+++ b/code/diffuser/models/diffusion.py
@@ -3,6 +3,8 @@ import torch
 from torch import nn
 import torch.nn.functional as F
 import pdb
+import clip
+from config.locomotion_config import Config
 
 import diffuser.utils as utils
 from .helpers import (
@@ -295,12 +297,22 @@ class GaussianInvDynDiffusion(nn.Module):
         condition_guidance_w=0.1, ar_inv=False, train_only_inv=False):
         super().__init__()
         self.horizon = horizon
-        self.observation_dim = observation_dim
-        self.action_dim = action_dim
+        self.observation_dim = observation_dim # 11
+        self.action_dim = action_dim # 3
         self.transition_dim = observation_dim + action_dim
         self.model = model
         self.ar_inv = ar_inv
         self.train_only_inv = train_only_inv
+
+        # Text embedding
+        device = Config.device
+        self.clip_model, _ = clip.load("ViT-B/32", device=device)
+        for p in self.clip_model.parameters():
+            p.requires_grad = False
+        with torch.no_grad():
+            self.text_cond = self.clip_model.encode_text(clip.tokenize('the man is hopping').to(device))
+        self.proj_layer = nn.Linear(512, self.observation_dim)
+
         if self.ar_inv:
             self.inv_model = ARInvModel(hidden_dim=hidden_dim, observation_dim=observation_dim, action_dim=action_dim)
         else:
diff --git a/code/diffuser/models/helpers.py b/code/diffuser/models/helpers.py
index 53859d4..b976a8d 100644
--- a/code/diffuser/models/helpers.py
+++ b/code/diffuser/models/helpers.py
@@ -91,8 +91,9 @@ def cosine_beta_schedule(timesteps, s=0.008, dtype=torch.float32):
     return torch.tensor(betas_clipped, dtype=dtype)
 
 def apply_conditioning(x, conditions, action_dim):
-    for t, val in conditions.items():
-        x[:, t, action_dim:] = val.clone()
+    # for t, val in conditions.items():
+    #     x[:, t, action_dim:] = val.clone() # batch size, time step, trajectory (all observation)
+    x[:, 0, :] = conditions.clone()
     return x
 
 #-----------------------------------------------------------------------------#
diff --git a/code/diffuser/models/temporal.py b/code/diffuser/models/temporal.py
index 63155c2..2daddb2 100644
--- a/code/diffuser/models/temporal.py
+++ b/code/diffuser/models/temporal.py
@@ -89,17 +89,24 @@ class ResidualTemporalBlock(nn.Module):
             Rearrange('batch t -> batch t 1'),
         )
 
+        self.cond_mlp = nn.Sequential(
+            act_fn,
+            nn.Linear(128, out_channels),
+            Rearrange('batch t -> batch t 1')
+        )
+
         self.residual_conv = nn.Conv1d(inp_channels, out_channels, 1) \
             if inp_channels != out_channels else nn.Identity()
 
-    def forward(self, x, t):
+    def forward(self, x, t, cond):
         '''
             x : [ batch_size x inp_channels x horizon ]
             t : [ batch_size x embed_dim ]
             returns:
             out : [ batch_size x out_channels x horizon ]
         '''
-        out = self.blocks[0](x) + self.time_mlp(t)
+        out = self.blocks[0](x) + self.time_mlp(t) + self.cond_mlp(cond)
+        # breakpoint()
         out = self.blocks[1](out)
 
         return out + self.residual_conv(x)
@@ -140,6 +147,13 @@ class TemporalUnet(nn.Module):
             act_fn,
             nn.Linear(dim * 4, dim),
         )
+        self.cond_mlp = nn.Sequential(
+            # SinusoidalPosEmb(dim),
+            nn.Linear(11, dim * 4),
+            act_fn,
+            nn.Linear(dim * 4, dim),
+        )
+
 
         self.returns_condition = returns_condition
         self.condition_dropout = condition_dropout
@@ -207,6 +221,7 @@ class TemporalUnet(nn.Module):
         x = einops.rearrange(x, 'b h t -> b t h')
 
         t = self.time_mlp(time)
+        c = self.cond_mlp(cond)
 
         if self.returns_condition:
             assert returns is not None
@@ -221,20 +236,20 @@ class TemporalUnet(nn.Module):
         h = []
 
         for resnet, resnet2, downsample in self.downs:
-            x = resnet(x, t)
-            x = resnet2(x, t)
+            x = resnet(x, t, c)
+            x = resnet2(x, t, c)
             h.append(x)
             x = downsample(x)
 
-        x = self.mid_block1(x, t)
-        x = self.mid_block2(x, t)
+        x = self.mid_block1(x, t, c)
+        x = self.mid_block2(x, t, c)
 
         # import pdb; pdb.set_trace()
 
         for resnet, resnet2, upsample in self.ups:
             x = torch.cat((x, h.pop()), dim=1)
-            x = resnet(x, t)
-            x = resnet2(x, t)
+            x = resnet(x, t, c)
+            x = resnet2(x, t, c)
             x = upsample(x)
 
         x = self.final_conv(x)
@@ -257,6 +272,7 @@ class TemporalUnet(nn.Module):
         x = einops.rearrange(x, 'b h t -> b t h')
 
         t = self.time_mlp(time)
+        c = self.cond_mlp(cond)
 
         if self.returns_condition:
             assert returns is not None
@@ -271,18 +287,18 @@ class TemporalUnet(nn.Module):
         h = []
 
         for resnet, resnet2, downsample in self.downs:
-            x = resnet(x, t)
-            x = resnet2(x, t)
+            x = resnet(x, t, c)
+            x = resnet2(x, t, c)
             h.append(x)
             x = downsample(x)
 
-        x = self.mid_block1(x, t)
-        x = self.mid_block2(x, t)
+        x = self.mid_block1(x, t, c)
+        x = self.mid_block2(x, t, c)
 
         for resnet, resnet2, upsample in self.ups:
             x = torch.cat((x, h.pop()), dim=1)
-            x = resnet(x, t)
-            x = resnet2(x, t)
+            x = resnet(x, t, c)
+            x = resnet2(x, t, c)
             x = upsample(x)
 
         x = self.final_conv(x)
@@ -325,6 +341,7 @@ class MLPnet(nn.Module):
         self.calc_energy = calc_energy
         self.transition_dim = transition_dim
         self.action_dim = transition_dim - cond_dim
+        self.cond_dim = cond_dim
 
         if self.returns_condition:
             self.returns_mlp = nn.Sequential(
@@ -365,8 +382,8 @@ class MLPnet(nn.Module):
             if force_dropout:
                 returns_embed = 0*returns_embed
             t = torch.cat([t, returns_embed], dim=-1)
-
         inp = torch.cat([t, cond, x], dim=-1)
+        # inp = torch.cat([t.unsqueeze(1), cond.unsqueez(1), x], dim=-1)
         out  = self.mlp(inp)
 
         if self.calc_energy:
diff --git a/code/diffuser/utils/arrays.py b/code/diffuser/utils/arrays.py
index ae9d45c..e9da536 100644
--- a/code/diffuser/utils/arrays.py
+++ b/code/diffuser/utils/arrays.py
@@ -64,10 +64,11 @@ def batchify(batch, device):
 	return type(batch)(*batched_vals)
 
 def apply_dict(fn, d, *args, **kwargs):
-	return {
-		k: fn(v, *args, **kwargs)
-		for k, v in d.items()
-	}
+	# return {
+	# 	k: fn(v, *args, **kwargs)
+	# 	for k, v in d.items()
+	# }
+	return d
 
 def normalize(x):
 	"""
diff --git a/code/diffuser/utils/training.py b/code/diffuser/utils/training.py
index e8dcdb4..0b6e0bf 100644
--- a/code/diffuser/utils/training.py
+++ b/code/diffuser/utils/training.py
@@ -113,6 +113,7 @@ class Trainer(object):
             for i in range(self.gradient_accumulate_every):
                 batch = next(self.dataloader)
                 batch = batch_to_device(batch, device=self.device)
+                # breakpoint()
                 loss, infos = self.model.loss(*batch)
                 loss = loss / self.gradient_accumulate_every
                 loss.backward()
diff --git a/code/environment.yml b/code/environment.yml
index 31a3487..47a996c 100644
--- a/code/environment.yml
+++ b/code/environment.yml
@@ -8,9 +8,9 @@ dependencies:
 - patchelf
 - pip:
     - -f https://download.pytorch.org/whl/torch_stable.html
-    - numpy
+    - numpy==1.23.0
     - gym==0.18.0
-    - mujoco-py==2.0.2.13
+    - mujoco-py
     - matplotlib==3.3.4
     - torch==1.9.1+cu111
     - typed-argument-parser
diff --git a/code/scripts/train.py b/code/scripts/train.py
index db05e5a..53b8e2f 100644
--- a/code/scripts/train.py
+++ b/code/scripts/train.py
@@ -60,6 +60,7 @@ def main(**deps):
             savepath='model_config.pkl',
             horizon=Config.horizon,
             transition_dim=observation_dim,
+            # transition_dim=observation_dim + action_dim,
             cond_dim=observation_dim,
             dim_mults=Config.dim_mults,
             returns_condition=Config.returns_condition,
@@ -95,7 +96,7 @@ def main(**deps):
             Config.model,
             savepath='model_config.pkl',
             horizon=Config.horizon,
-            transition_dim=observation_dim + action_dim,
+            transition_dim=observation_dim,
             cond_dim=observation_dim,
             dim_mults=Config.dim_mults,
             returns_condition=Config.returns_condition,diff --git a/code/config/locomotion_config.py b/code/config/locomotion_config.py
index 90173b3..b79b7e6 100644
--- a/code/config/locomotion_config.py
+++ b/code/config/locomotion_config.py
@@ -6,7 +6,7 @@ class Config(ParamsProto):
     # misc
     seed = 100
     device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
-    bucket = '/home/aajay/weights/'
+    bucket = '/home/yanyangji/research/RL/final/decision-diffuser/code/output'
     dataset = 'hopper-medium-expert-v2'
 
     ## model
diff --git a/code/diffuser/datasets/buffer.py b/code/diffuser/datasets/buffer.py
index 42bc734..29edcd8 100644
--- a/code/diffuser/datasets/buffer.py
+++ b/code/diffuser/datasets/buffer.py
@@ -9,7 +9,7 @@ class ReplayBuffer:
 
     def __init__(self, max_n_episodes, max_path_length, termination_penalty):
         self._dict = {
-            'path_lengths': np.zeros(max_n_episodes, dtype=np.int),
+            'path_lengths': np.zeros(max_n_episodes, dtype=np.int32),
         }
         self._count = 0
         self.max_n_episodes = max_n_episodes
diff --git a/code/diffuser/datasets/sequence.py b/code/diffuser/datasets/sequence.py
index 31ac84f..f256554 100644
--- a/code/diffuser/datasets/sequence.py
+++ b/code/diffuser/datasets/sequence.py
@@ -76,7 +76,8 @@ class SequenceDataset(torch.utils.data.Dataset):
         '''
             condition on current observation for planning
         '''
-        return {0: observations[0]}
+        # return {0: observations[0]}
+        return observations[0]
 
     def __len__(self):
         return len(self.indices)
diff --git a/code/diffuser/models/diffusion.py b/code/diffuser/models/diffusion.py
index a1f74ea..0c8f1a1 100644
--- a/code/diffuser/models/diffusion.py
+++ b/code/diffuser/models/diffusion.py
@@ -3,6 +3,8 @@ import torch
 from torch import nn
 import torch.nn.functional as F
 import pdb
+import clip
+from config.locomotion_config import Config
 
 import diffuser.utils as utils
 from .helpers import (
@@ -295,12 +297,22 @@ class GaussianInvDynDiffusion(nn.Module):
         condition_guidance_w=0.1, ar_inv=False, train_only_inv=False):
         super().__init__()
         self.horizon = horizon
-        self.observation_dim = observation_dim
-        self.action_dim = action_dim
+        self.observation_dim = observation_dim # 11
+        self.action_dim = action_dim # 3
         self.transition_dim = observation_dim + action_dim
         self.model = model
         self.ar_inv = ar_inv
         self.train_only_inv = train_only_inv
+
+        # Text embedding
+        device = Config.device
+        self.clip_model, _ = clip.load("ViT-B/32", device=device)
+        for p in self.clip_model.parameters():
+            p.requires_grad = False
+        with torch.no_grad():
+            self.text_cond = self.clip_model.encode_text(clip.tokenize('the man is hopping').to(device))
+        self.proj_layer = nn.Linear(512, self.observation_dim)
+
         if self.ar_inv:
             self.inv_model = ARInvModel(hidden_dim=hidden_dim, observation_dim=observation_dim, action_dim=action_dim)
         else:
diff --git a/code/diffuser/models/helpers.py b/code/diffuser/models/helpers.py
index 53859d4..b976a8d 100644
--- a/code/diffuser/models/helpers.py
+++ b/code/diffuser/models/helpers.py
@@ -91,8 +91,9 @@ def cosine_beta_schedule(timesteps, s=0.008, dtype=torch.float32):
     return torch.tensor(betas_clipped, dtype=dtype)
 
 def apply_conditioning(x, conditions, action_dim):
-    for t, val in conditions.items():
-        x[:, t, action_dim:] = val.clone()
+    # for t, val in conditions.items():
+    #     x[:, t, action_dim:] = val.clone() # batch size, time step, trajectory (all observation)
+    x[:, 0, :] = conditions.clone()
     return x
 
 #-----------------------------------------------------------------------------#
diff --git a/code/diffuser/models/temporal.py b/code/diffuser/models/temporal.py
index 63155c2..527b09a 100644
--- a/code/diffuser/models/temporal.py
+++ b/code/diffuser/models/temporal.py
@@ -89,10 +89,16 @@ class ResidualTemporalBlock(nn.Module):
             Rearrange('batch t -> batch t 1'),
         )
 
+        self.cond_mlp = nn.Sequential(
+            act_fn,
+            nn.Linear(128, out_channels),
+            Rearrange('batch t -> batch t 1')
+        )
+
         self.residual_conv = nn.Conv1d(inp_channels, out_channels, 1) \
             if inp_channels != out_channels else nn.Identity()
 
-    def forward(self, x, t):
+    def forward(self, x, t, cond):
         '''
             x : [ batch_size x inp_channels x horizon ]
             t : [ batch_size x embed_dim ]
@@ -100,6 +106,8 @@ class ResidualTemporalBlock(nn.Module):
             out : [ batch_size x out_channels x horizon ]
         '''
         out = self.blocks[0](x) + self.time_mlp(t)
+        # + self.cond_mlp(cond)
+        # breakpoint()
         out = self.blocks[1](out)
 
         return out + self.residual_conv(x)
@@ -140,6 +148,13 @@ class TemporalUnet(nn.Module):
             act_fn,
             nn.Linear(dim * 4, dim),
         )
+        self.cond_mlp = nn.Sequential(
+            # SinusoidalPosEmb(dim),
+            nn.Linear(11, dim * 4),
+            act_fn,
+            nn.Linear(dim * 4, dim),
+        )
+
 
         self.returns_condition = returns_condition
         self.condition_dropout = condition_dropout
@@ -207,6 +222,7 @@ class TemporalUnet(nn.Module):
         x = einops.rearrange(x, 'b h t -> b t h')
 
         t = self.time_mlp(time)
+        c = self.cond_mlp(cond)
 
         if self.returns_condition:
             assert returns is not None
@@ -221,20 +237,20 @@ class TemporalUnet(nn.Module):
         h = []
 
         for resnet, resnet2, downsample in self.downs:
-            x = resnet(x, t)
-            x = resnet2(x, t)
+            x = resnet(x, t, c)
+            x = resnet2(x, t, c)
             h.append(x)
             x = downsample(x)
 
-        x = self.mid_block1(x, t)
-        x = self.mid_block2(x, t)
+        x = self.mid_block1(x, t, c)
+        x = self.mid_block2(x, t, c)
 
         # import pdb; pdb.set_trace()
 
         for resnet, resnet2, upsample in self.ups:
             x = torch.cat((x, h.pop()), dim=1)
-            x = resnet(x, t)
-            x = resnet2(x, t)
+            x = resnet(x, t, c)
+            x = resnet2(x, t, c)
             x = upsample(x)
 
         x = self.final_conv(x)
@@ -257,6 +273,7 @@ class TemporalUnet(nn.Module):
         x = einops.rearrange(x, 'b h t -> b t h')
 
         t = self.time_mlp(time)
+        c = self.cond_mlp(cond)
 
         if self.returns_condition:
             assert returns is not None
@@ -271,18 +288,18 @@ class TemporalUnet(nn.Module):
         h = []
 
         for resnet, resnet2, downsample in self.downs:
-            x = resnet(x, t)
-            x = resnet2(x, t)
+            x = resnet(x, t, c)
+            x = resnet2(x, t, c)
             h.append(x)
             x = downsample(x)
 
-        x = self.mid_block1(x, t)
-        x = self.mid_block2(x, t)
+        x = self.mid_block1(x, t, c)
+        x = self.mid_block2(x, t, c)
 
         for resnet, resnet2, upsample in self.ups:
             x = torch.cat((x, h.pop()), dim=1)
-            x = resnet(x, t)
-            x = resnet2(x, t)
+            x = resnet(x, t, c)
+            x = resnet2(x, t, c)
             x = upsample(x)
 
         x = self.final_conv(x)
@@ -325,6 +342,7 @@ class MLPnet(nn.Module):
         self.calc_energy = calc_energy
         self.transition_dim = transition_dim
         self.action_dim = transition_dim - cond_dim
+        self.cond_dim = cond_dim
 
         if self.returns_condition:
             self.returns_mlp = nn.Sequential(
@@ -365,8 +383,8 @@ class MLPnet(nn.Module):
             if force_dropout:
                 returns_embed = 0*returns_embed
             t = torch.cat([t, returns_embed], dim=-1)
-
         inp = torch.cat([t, cond, x], dim=-1)
+        # inp = torch.cat([t.unsqueeze(1), cond.unsqueez(1), x], dim=-1)
         out  = self.mlp(inp)
 
         if self.calc_energy:
diff --git a/code/diffuser/utils/arrays.py b/code/diffuser/utils/arrays.py
index ae9d45c..e9da536 100644
--- a/code/diffuser/utils/arrays.py
+++ b/code/diffuser/utils/arrays.py
@@ -64,10 +64,11 @@ def batchify(batch, device):
 	return type(batch)(*batched_vals)
 
 def apply_dict(fn, d, *args, **kwargs):
-	return {
-		k: fn(v, *args, **kwargs)
-		for k, v in d.items()
-	}
+	# return {
+	# 	k: fn(v, *args, **kwargs)
+	# 	for k, v in d.items()
+	# }
+	return d
 
 def normalize(x):
 	"""
diff --git a/code/diffuser/utils/training.py b/code/diffuser/utils/training.py
index e8dcdb4..0b6e0bf 100644
--- a/code/diffuser/utils/training.py
+++ b/code/diffuser/utils/training.py
@@ -113,6 +113,7 @@ class Trainer(object):
             for i in range(self.gradient_accumulate_every):
                 batch = next(self.dataloader)
                 batch = batch_to_device(batch, device=self.device)
+                # breakpoint()
                 loss, infos = self.model.loss(*batch)
                 loss = loss / self.gradient_accumulate_every
                 loss.backward()
diff --git a/code/environment.yml b/code/environment.yml
index 31a3487..47a996c 100644
--- a/code/environment.yml
+++ b/code/environment.yml
@@ -8,9 +8,9 @@ dependencies:
 - patchelf
 - pip:
     - -f https://download.pytorch.org/whl/torch_stable.html
-    - numpy
+    - numpy==1.23.0
     - gym==0.18.0
-    - mujoco-py==2.0.2.13
+    - mujoco-py
     - matplotlib==3.3.4
     - torch==1.9.1+cu111
     - typed-argument-parser
diff --git a/code/scripts/train.py b/code/scripts/train.py
index db05e5a..53b8e2f 100644
--- a/code/scripts/train.py
+++ b/code/scripts/train.py
@@ -60,6 +60,7 @@ def main(**deps):
             savepath='model_config.pkl',
             horizon=Config.horizon,
             transition_dim=observation_dim,
+            # transition_dim=observation_dim + action_dim,
             cond_dim=observation_dim,
             dim_mults=Config.dim_mults,
             returns_condition=Config.returns_condition,
@@ -95,7 +96,7 @@ def main(**deps):
             Config.model,
             savepath='model_config.pkl',
             horizon=Config.horizon,
-            transition_dim=observation_dim + action_dim,
+            transition_dim=observation_dim,
             cond_dim=observation_dim,
             dim_mults=Config.dim_mults,
             returns_condition=Config.returns_condition,diff --git a/code/config/locomotion_config.py b/code/config/locomotion_config.py
index 90173b3..b79b7e6 100644
--- a/code/config/locomotion_config.py
+++ b/code/config/locomotion_config.py
@@ -6,7 +6,7 @@ class Config(ParamsProto):
     # misc
     seed = 100
     device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
-    bucket = '/home/aajay/weights/'
+    bucket = '/home/yanyangji/research/RL/final/decision-diffuser/code/output'
     dataset = 'hopper-medium-expert-v2'
 
     ## model
diff --git a/code/diffuser/datasets/buffer.py b/code/diffuser/datasets/buffer.py
index 42bc734..29edcd8 100644
--- a/code/diffuser/datasets/buffer.py
+++ b/code/diffuser/datasets/buffer.py
@@ -9,7 +9,7 @@ class ReplayBuffer:
 
     def __init__(self, max_n_episodes, max_path_length, termination_penalty):
         self._dict = {
-            'path_lengths': np.zeros(max_n_episodes, dtype=np.int),
+            'path_lengths': np.zeros(max_n_episodes, dtype=np.int32),
         }
         self._count = 0
         self.max_n_episodes = max_n_episodes
diff --git a/code/diffuser/datasets/sequence.py b/code/diffuser/datasets/sequence.py
index 31ac84f..f256554 100644
--- a/code/diffuser/datasets/sequence.py
+++ b/code/diffuser/datasets/sequence.py
@@ -76,7 +76,8 @@ class SequenceDataset(torch.utils.data.Dataset):
         '''
             condition on current observation for planning
         '''
-        return {0: observations[0]}
+        # return {0: observations[0]}
+        return observations[0]
 
     def __len__(self):
         return len(self.indices)
diff --git a/code/diffuser/models/diffusion.py b/code/diffuser/models/diffusion.py
index a1f74ea..0c8f1a1 100644
--- a/code/diffuser/models/diffusion.py
+++ b/code/diffuser/models/diffusion.py
@@ -3,6 +3,8 @@ import torch
 from torch import nn
 import torch.nn.functional as F
 import pdb
+import clip
+from config.locomotion_config import Config
 
 import diffuser.utils as utils
 from .helpers import (
@@ -295,12 +297,22 @@ class GaussianInvDynDiffusion(nn.Module):
         condition_guidance_w=0.1, ar_inv=False, train_only_inv=False):
         super().__init__()
         self.horizon = horizon
-        self.observation_dim = observation_dim
-        self.action_dim = action_dim
+        self.observation_dim = observation_dim # 11
+        self.action_dim = action_dim # 3
         self.transition_dim = observation_dim + action_dim
         self.model = model
         self.ar_inv = ar_inv
         self.train_only_inv = train_only_inv
+
+        # Text embedding
+        device = Config.device
+        self.clip_model, _ = clip.load("ViT-B/32", device=device)
+        for p in self.clip_model.parameters():
+            p.requires_grad = False
+        with torch.no_grad():
+            self.text_cond = self.clip_model.encode_text(clip.tokenize('the man is hopping').to(device))
+        self.proj_layer = nn.Linear(512, self.observation_dim)
+
         if self.ar_inv:
             self.inv_model = ARInvModel(hidden_dim=hidden_dim, observation_dim=observation_dim, action_dim=action_dim)
         else:
diff --git a/code/diffuser/models/helpers.py b/code/diffuser/models/helpers.py
index 53859d4..b976a8d 100644
--- a/code/diffuser/models/helpers.py
+++ b/code/diffuser/models/helpers.py
@@ -91,8 +91,9 @@ def cosine_beta_schedule(timesteps, s=0.008, dtype=torch.float32):
     return torch.tensor(betas_clipped, dtype=dtype)
 
 def apply_conditioning(x, conditions, action_dim):
-    for t, val in conditions.items():
-        x[:, t, action_dim:] = val.clone()
+    # for t, val in conditions.items():
+    #     x[:, t, action_dim:] = val.clone() # batch size, time step, trajectory (all observation)
+    x[:, 0, :] = conditions.clone()
     return x
 
 #-----------------------------------------------------------------------------#
diff --git a/code/diffuser/models/temporal.py b/code/diffuser/models/temporal.py
index 63155c2..8f97964 100644
--- a/code/diffuser/models/temporal.py
+++ b/code/diffuser/models/temporal.py
@@ -89,10 +89,16 @@ class ResidualTemporalBlock(nn.Module):
             Rearrange('batch t -> batch t 1'),
         )
 
+        self.cond_mlp = nn.Sequential(
+            act_fn,
+            nn.Linear(128, out_channels),
+            Rearrange('batch t -> batch t 1')
+        )
+
         self.residual_conv = nn.Conv1d(inp_channels, out_channels, 1) \
             if inp_channels != out_channels else nn.Identity()
 
-    def forward(self, x, t):
+    def forward(self, x, t, cond):
         '''
             x : [ batch_size x inp_channels x horizon ]
             t : [ batch_size x embed_dim ]
@@ -100,6 +106,8 @@ class ResidualTemporalBlock(nn.Module):
             out : [ batch_size x out_channels x horizon ]
         '''
         out = self.blocks[0](x) + self.time_mlp(t)
+        # + self.cond_mlp(cond)
+        # breakpoint()
         out = self.blocks[1](out)
 
         return out + self.residual_conv(x)
@@ -140,6 +148,12 @@ class TemporalUnet(nn.Module):
             act_fn,
             nn.Linear(dim * 4, dim),
         )
+        self.cond_mlp = nn.Sequential(
+            # SinusoidalPosEmb(dim),
+            nn.Linear(11, dim * 4),
+            act_fn,
+            nn.Linear(dim * 4, dim),
+        )
 
         self.returns_condition = returns_condition
         self.condition_dropout = condition_dropout
@@ -207,6 +221,7 @@ class TemporalUnet(nn.Module):
         x = einops.rearrange(x, 'b h t -> b t h')
 
         t = self.time_mlp(time)
+        c = self.cond_mlp(cond)
 
         if self.returns_condition:
             assert returns is not None
@@ -221,20 +236,20 @@ class TemporalUnet(nn.Module):
         h = []
 
         for resnet, resnet2, downsample in self.downs:
-            x = resnet(x, t)
-            x = resnet2(x, t)
+            x = resnet(x, t, c)
+            x = resnet2(x, t, c)
             h.append(x)
             x = downsample(x)
 
-        x = self.mid_block1(x, t)
-        x = self.mid_block2(x, t)
+        x = self.mid_block1(x, t, c)
+        x = self.mid_block2(x, t, c)
 
         # import pdb; pdb.set_trace()
 
         for resnet, resnet2, upsample in self.ups:
             x = torch.cat((x, h.pop()), dim=1)
-            x = resnet(x, t)
-            x = resnet2(x, t)
+            x = resnet(x, t, c)
+            x = resnet2(x, t, c)
             x = upsample(x)
 
         x = self.final_conv(x)
@@ -257,6 +272,7 @@ class TemporalUnet(nn.Module):
         x = einops.rearrange(x, 'b h t -> b t h')
 
         t = self.time_mlp(time)
+        c = self.cond_mlp(cond)
 
         if self.returns_condition:
             assert returns is not None
@@ -271,18 +287,18 @@ class TemporalUnet(nn.Module):
         h = []
 
         for resnet, resnet2, downsample in self.downs:
-            x = resnet(x, t)
-            x = resnet2(x, t)
+            x = resnet(x, t, c)
+            x = resnet2(x, t, c)
             h.append(x)
             x = downsample(x)
 
-        x = self.mid_block1(x, t)
-        x = self.mid_block2(x, t)
+        x = self.mid_block1(x, t, c)
+        x = self.mid_block2(x, t, c)
 
         for resnet, resnet2, upsample in self.ups:
             x = torch.cat((x, h.pop()), dim=1)
-            x = resnet(x, t)
-            x = resnet2(x, t)
+            x = resnet(x, t, c)
+            x = resnet2(x, t, c)
             x = upsample(x)
 
         x = self.final_conv(x)
@@ -325,6 +341,7 @@ class MLPnet(nn.Module):
         self.calc_energy = calc_energy
         self.transition_dim = transition_dim
         self.action_dim = transition_dim - cond_dim
+        self.cond_dim = cond_dim
 
         if self.returns_condition:
             self.returns_mlp = nn.Sequential(
@@ -365,8 +382,8 @@ class MLPnet(nn.Module):
             if force_dropout:
                 returns_embed = 0*returns_embed
             t = torch.cat([t, returns_embed], dim=-1)
-
         inp = torch.cat([t, cond, x], dim=-1)
+        # inp = torch.cat([t.unsqueeze(1), cond.unsqueez(1), x], dim=-1)
         out  = self.mlp(inp)
 
         if self.calc_energy:
diff --git a/code/diffuser/utils/arrays.py b/code/diffuser/utils/arrays.py
index ae9d45c..e9da536 100644
--- a/code/diffuser/utils/arrays.py
+++ b/code/diffuser/utils/arrays.py
@@ -64,10 +64,11 @@ def batchify(batch, device):
 	return type(batch)(*batched_vals)
 
 def apply_dict(fn, d, *args, **kwargs):
-	return {
-		k: fn(v, *args, **kwargs)
-		for k, v in d.items()
-	}
+	# return {
+	# 	k: fn(v, *args, **kwargs)
+	# 	for k, v in d.items()
+	# }
+	return d
 
 def normalize(x):
 	"""
diff --git a/code/diffuser/utils/training.py b/code/diffuser/utils/training.py
index e8dcdb4..0b6e0bf 100644
--- a/code/diffuser/utils/training.py
+++ b/code/diffuser/utils/training.py
@@ -113,6 +113,7 @@ class Trainer(object):
             for i in range(self.gradient_accumulate_every):
                 batch = next(self.dataloader)
                 batch = batch_to_device(batch, device=self.device)
+                # breakpoint()
                 loss, infos = self.model.loss(*batch)
                 loss = loss / self.gradient_accumulate_every
                 loss.backward()
diff --git a/code/environment.yml b/code/environment.yml
index 31a3487..47a996c 100644
--- a/code/environment.yml
+++ b/code/environment.yml
@@ -8,9 +8,9 @@ dependencies:
 - patchelf
 - pip:
     - -f https://download.pytorch.org/whl/torch_stable.html
-    - numpy
+    - numpy==1.23.0
     - gym==0.18.0
-    - mujoco-py==2.0.2.13
+    - mujoco-py
     - matplotlib==3.3.4
     - torch==1.9.1+cu111
     - typed-argument-parser
diff --git a/code/scripts/train.py b/code/scripts/train.py
index db05e5a..53b8e2f 100644
--- a/code/scripts/train.py
+++ b/code/scripts/train.py
@@ -60,6 +60,7 @@ def main(**deps):
             savepath='model_config.pkl',
             horizon=Config.horizon,
             transition_dim=observation_dim,
+            # transition_dim=observation_dim + action_dim,
             cond_dim=observation_dim,
             dim_mults=Config.dim_mults,
             returns_condition=Config.returns_condition,
@@ -95,7 +96,7 @@ def main(**deps):
             Config.model,
             savepath='model_config.pkl',
             horizon=Config.horizon,
-            transition_dim=observation_dim + action_dim,
+            transition_dim=observation_dim,
             cond_dim=observation_dim,
             dim_mults=Config.dim_mults,
             returns_condition=Config.returns_condition,diff --git a/code/config/locomotion_config.py b/code/config/locomotion_config.py
index 90173b3..b79b7e6 100644
--- a/code/config/locomotion_config.py
+++ b/code/config/locomotion_config.py
@@ -6,7 +6,7 @@ class Config(ParamsProto):
     # misc
     seed = 100
     device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
-    bucket = '/home/aajay/weights/'
+    bucket = '/home/yanyangji/research/RL/final/decision-diffuser/code/output'
     dataset = 'hopper-medium-expert-v2'
 
     ## model
diff --git a/code/diffuser/datasets/buffer.py b/code/diffuser/datasets/buffer.py
index 42bc734..29edcd8 100644
--- a/code/diffuser/datasets/buffer.py
+++ b/code/diffuser/datasets/buffer.py
@@ -9,7 +9,7 @@ class ReplayBuffer:
 
     def __init__(self, max_n_episodes, max_path_length, termination_penalty):
         self._dict = {
-            'path_lengths': np.zeros(max_n_episodes, dtype=np.int),
+            'path_lengths': np.zeros(max_n_episodes, dtype=np.int32),
         }
         self._count = 0
         self.max_n_episodes = max_n_episodes
diff --git a/code/diffuser/datasets/sequence.py b/code/diffuser/datasets/sequence.py
index 31ac84f..f256554 100644
--- a/code/diffuser/datasets/sequence.py
+++ b/code/diffuser/datasets/sequence.py
@@ -76,7 +76,8 @@ class SequenceDataset(torch.utils.data.Dataset):
         '''
             condition on current observation for planning
         '''
-        return {0: observations[0]}
+        # return {0: observations[0]}
+        return observations[0]
 
     def __len__(self):
         return len(self.indices)
diff --git a/code/diffuser/models/diffusion.py b/code/diffuser/models/diffusion.py
index a1f74ea..0c8f1a1 100644
--- a/code/diffuser/models/diffusion.py
+++ b/code/diffuser/models/diffusion.py
@@ -3,6 +3,8 @@ import torch
 from torch import nn
 import torch.nn.functional as F
 import pdb
+import clip
+from config.locomotion_config import Config
 
 import diffuser.utils as utils
 from .helpers import (
@@ -295,12 +297,22 @@ class GaussianInvDynDiffusion(nn.Module):
         condition_guidance_w=0.1, ar_inv=False, train_only_inv=False):
         super().__init__()
         self.horizon = horizon
-        self.observation_dim = observation_dim
-        self.action_dim = action_dim
+        self.observation_dim = observation_dim # 11
+        self.action_dim = action_dim # 3
         self.transition_dim = observation_dim + action_dim
         self.model = model
         self.ar_inv = ar_inv
         self.train_only_inv = train_only_inv
+
+        # Text embedding
+        device = Config.device
+        self.clip_model, _ = clip.load("ViT-B/32", device=device)
+        for p in self.clip_model.parameters():
+            p.requires_grad = False
+        with torch.no_grad():
+            self.text_cond = self.clip_model.encode_text(clip.tokenize('the man is hopping').to(device))
+        self.proj_layer = nn.Linear(512, self.observation_dim)
+
         if self.ar_inv:
             self.inv_model = ARInvModel(hidden_dim=hidden_dim, observation_dim=observation_dim, action_dim=action_dim)
         else:
diff --git a/code/diffuser/models/helpers.py b/code/diffuser/models/helpers.py
index 53859d4..b976a8d 100644
--- a/code/diffuser/models/helpers.py
+++ b/code/diffuser/models/helpers.py
@@ -91,8 +91,9 @@ def cosine_beta_schedule(timesteps, s=0.008, dtype=torch.float32):
     return torch.tensor(betas_clipped, dtype=dtype)
 
 def apply_conditioning(x, conditions, action_dim):
-    for t, val in conditions.items():
-        x[:, t, action_dim:] = val.clone()
+    # for t, val in conditions.items():
+    #     x[:, t, action_dim:] = val.clone() # batch size, time step, trajectory (all observation)
+    x[:, 0, :] = conditions.clone()
     return x
 
 #-----------------------------------------------------------------------------#
diff --git a/code/diffuser/models/temporal.py b/code/diffuser/models/temporal.py
index 63155c2..1ac05f4 100644
--- a/code/diffuser/models/temporal.py
+++ b/code/diffuser/models/temporal.py
@@ -89,10 +89,16 @@ class ResidualTemporalBlock(nn.Module):
             Rearrange('batch t -> batch t 1'),
         )
 
+        self.cond_mlp = nn.Sequential(
+            act_fn,
+            nn.Linear(128, out_channels),
+            Rearrange('batch t -> batch t 1')
+        )
+
         self.residual_conv = nn.Conv1d(inp_channels, out_channels, 1) \
             if inp_channels != out_channels else nn.Identity()
 
-    def forward(self, x, t):
+    def forward(self, x, t, cond=None):
         '''
             x : [ batch_size x inp_channels x horizon ]
             t : [ batch_size x embed_dim ]
@@ -100,6 +106,8 @@ class ResidualTemporalBlock(nn.Module):
             out : [ batch_size x out_channels x horizon ]
         '''
         out = self.blocks[0](x) + self.time_mlp(t)
+        # + self.cond_mlp(cond)
+        # breakpoint()
         out = self.blocks[1](out)
 
         return out + self.residual_conv(x)
@@ -140,6 +148,12 @@ class TemporalUnet(nn.Module):
             act_fn,
             nn.Linear(dim * 4, dim),
         )
+        self.cond_mlp = nn.Sequential(
+            # SinusoidalPosEmb(dim),
+            nn.Linear(11, dim * 4),
+            act_fn,
+            nn.Linear(dim * 4, dim),
+        )
 
         self.returns_condition = returns_condition
         self.condition_dropout = condition_dropout
@@ -207,6 +221,7 @@ class TemporalUnet(nn.Module):
         x = einops.rearrange(x, 'b h t -> b t h')
 
         t = self.time_mlp(time)
+        c = self.cond_mlp(cond)
 
         if self.returns_condition:
             assert returns is not None
@@ -257,6 +272,7 @@ class TemporalUnet(nn.Module):
         x = einops.rearrange(x, 'b h t -> b t h')
 
         t = self.time_mlp(time)
+        c = self.cond_mlp(cond)
 
         if self.returns_condition:
             assert returns is not None
@@ -325,6 +341,7 @@ class MLPnet(nn.Module):
         self.calc_energy = calc_energy
         self.transition_dim = transition_dim
         self.action_dim = transition_dim - cond_dim
+        self.cond_dim = cond_dim
 
         if self.returns_condition:
             self.returns_mlp = nn.Sequential(
@@ -365,8 +382,8 @@ class MLPnet(nn.Module):
             if force_dropout:
                 returns_embed = 0*returns_embed
             t = torch.cat([t, returns_embed], dim=-1)
-
         inp = torch.cat([t, cond, x], dim=-1)
+        # inp = torch.cat([t.unsqueeze(1), cond.unsqueez(1), x], dim=-1)
         out  = self.mlp(inp)
 
         if self.calc_energy:
diff --git a/code/diffuser/utils/arrays.py b/code/diffuser/utils/arrays.py
index ae9d45c..e9da536 100644
--- a/code/diffuser/utils/arrays.py
+++ b/code/diffuser/utils/arrays.py
@@ -64,10 +64,11 @@ def batchify(batch, device):
 	return type(batch)(*batched_vals)
 
 def apply_dict(fn, d, *args, **kwargs):
-	return {
-		k: fn(v, *args, **kwargs)
-		for k, v in d.items()
-	}
+	# return {
+	# 	k: fn(v, *args, **kwargs)
+	# 	for k, v in d.items()
+	# }
+	return d
 
 def normalize(x):
 	"""
diff --git a/code/diffuser/utils/training.py b/code/diffuser/utils/training.py
index e8dcdb4..0b6e0bf 100644
--- a/code/diffuser/utils/training.py
+++ b/code/diffuser/utils/training.py
@@ -113,6 +113,7 @@ class Trainer(object):
             for i in range(self.gradient_accumulate_every):
                 batch = next(self.dataloader)
                 batch = batch_to_device(batch, device=self.device)
+                # breakpoint()
                 loss, infos = self.model.loss(*batch)
                 loss = loss / self.gradient_accumulate_every
                 loss.backward()
diff --git a/code/environment.yml b/code/environment.yml
index 31a3487..47a996c 100644
--- a/code/environment.yml
+++ b/code/environment.yml
@@ -8,9 +8,9 @@ dependencies:
 - patchelf
 - pip:
     - -f https://download.pytorch.org/whl/torch_stable.html
-    - numpy
+    - numpy==1.23.0
     - gym==0.18.0
-    - mujoco-py==2.0.2.13
+    - mujoco-py
     - matplotlib==3.3.4
     - torch==1.9.1+cu111
     - typed-argument-parser
diff --git a/code/scripts/train.py b/code/scripts/train.py
index db05e5a..53b8e2f 100644
--- a/code/scripts/train.py
+++ b/code/scripts/train.py
@@ -60,6 +60,7 @@ def main(**deps):
             savepath='model_config.pkl',
             horizon=Config.horizon,
             transition_dim=observation_dim,
+            # transition_dim=observation_dim + action_dim,
             cond_dim=observation_dim,
             dim_mults=Config.dim_mults,
             returns_condition=Config.returns_condition,
@@ -95,7 +96,7 @@ def main(**deps):
             Config.model,
             savepath='model_config.pkl',
             horizon=Config.horizon,
-            transition_dim=observation_dim + action_dim,
+            transition_dim=observation_dim,
             cond_dim=observation_dim,
             dim_mults=Config.dim_mults,
             returns_condition=Config.returns_condition,diff --git a/code/config/locomotion_config.py b/code/config/locomotion_config.py
index 90173b3..b79b7e6 100644
--- a/code/config/locomotion_config.py
+++ b/code/config/locomotion_config.py
@@ -6,7 +6,7 @@ class Config(ParamsProto):
     # misc
     seed = 100
     device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
-    bucket = '/home/aajay/weights/'
+    bucket = '/home/yanyangji/research/RL/final/decision-diffuser/code/output'
     dataset = 'hopper-medium-expert-v2'
 
     ## model
diff --git a/code/diffuser/datasets/buffer.py b/code/diffuser/datasets/buffer.py
index 42bc734..29edcd8 100644
--- a/code/diffuser/datasets/buffer.py
+++ b/code/diffuser/datasets/buffer.py
@@ -9,7 +9,7 @@ class ReplayBuffer:
 
     def __init__(self, max_n_episodes, max_path_length, termination_penalty):
         self._dict = {
-            'path_lengths': np.zeros(max_n_episodes, dtype=np.int),
+            'path_lengths': np.zeros(max_n_episodes, dtype=np.int32),
         }
         self._count = 0
         self.max_n_episodes = max_n_episodes
diff --git a/code/diffuser/datasets/sequence.py b/code/diffuser/datasets/sequence.py
index 31ac84f..ad977fc 100644
--- a/code/diffuser/datasets/sequence.py
+++ b/code/diffuser/datasets/sequence.py
@@ -77,6 +77,7 @@ class SequenceDataset(torch.utils.data.Dataset):
             condition on current observation for planning
         '''
         return {0: observations[0]}
+        # return observations[0]
 
     def __len__(self):
         return len(self.indices)
diff --git a/code/diffuser/models/diffusion.py b/code/diffuser/models/diffusion.py
index a1f74ea..0c8f1a1 100644
--- a/code/diffuser/models/diffusion.py
+++ b/code/diffuser/models/diffusion.py
@@ -3,6 +3,8 @@ import torch
 from torch import nn
 import torch.nn.functional as F
 import pdb
+import clip
+from config.locomotion_config import Config
 
 import diffuser.utils as utils
 from .helpers import (
@@ -295,12 +297,22 @@ class GaussianInvDynDiffusion(nn.Module):
         condition_guidance_w=0.1, ar_inv=False, train_only_inv=False):
         super().__init__()
         self.horizon = horizon
-        self.observation_dim = observation_dim
-        self.action_dim = action_dim
+        self.observation_dim = observation_dim # 11
+        self.action_dim = action_dim # 3
         self.transition_dim = observation_dim + action_dim
         self.model = model
         self.ar_inv = ar_inv
         self.train_only_inv = train_only_inv
+
+        # Text embedding
+        device = Config.device
+        self.clip_model, _ = clip.load("ViT-B/32", device=device)
+        for p in self.clip_model.parameters():
+            p.requires_grad = False
+        with torch.no_grad():
+            self.text_cond = self.clip_model.encode_text(clip.tokenize('the man is hopping').to(device))
+        self.proj_layer = nn.Linear(512, self.observation_dim)
+
         if self.ar_inv:
             self.inv_model = ARInvModel(hidden_dim=hidden_dim, observation_dim=observation_dim, action_dim=action_dim)
         else:
diff --git a/code/diffuser/models/helpers.py b/code/diffuser/models/helpers.py
index 53859d4..be91af3 100644
--- a/code/diffuser/models/helpers.py
+++ b/code/diffuser/models/helpers.py
@@ -92,7 +92,8 @@ def cosine_beta_schedule(timesteps, s=0.008, dtype=torch.float32):
 
 def apply_conditioning(x, conditions, action_dim):
     for t, val in conditions.items():
-        x[:, t, action_dim:] = val.clone()
+        x[:, t, action_dim:] = val.clone() # batch size, time step, trajectory (all observation)
+    # x[:, 0, :] = conditions.clone()
     return x
 
 #-----------------------------------------------------------------------------#
diff --git a/code/diffuser/models/temporal.py b/code/diffuser/models/temporal.py
index 63155c2..1ac05f4 100644
--- a/code/diffuser/models/temporal.py
+++ b/code/diffuser/models/temporal.py
@@ -89,10 +89,16 @@ class ResidualTemporalBlock(nn.Module):
             Rearrange('batch t -> batch t 1'),
         )
 
+        self.cond_mlp = nn.Sequential(
+            act_fn,
+            nn.Linear(128, out_channels),
+            Rearrange('batch t -> batch t 1')
+        )
+
         self.residual_conv = nn.Conv1d(inp_channels, out_channels, 1) \
             if inp_channels != out_channels else nn.Identity()
 
-    def forward(self, x, t):
+    def forward(self, x, t, cond=None):
         '''
             x : [ batch_size x inp_channels x horizon ]
             t : [ batch_size x embed_dim ]
@@ -100,6 +106,8 @@ class ResidualTemporalBlock(nn.Module):
             out : [ batch_size x out_channels x horizon ]
         '''
         out = self.blocks[0](x) + self.time_mlp(t)
+        # + self.cond_mlp(cond)
+        # breakpoint()
         out = self.blocks[1](out)
 
         return out + self.residual_conv(x)
@@ -140,6 +148,12 @@ class TemporalUnet(nn.Module):
             act_fn,
             nn.Linear(dim * 4, dim),
         )
+        self.cond_mlp = nn.Sequential(
+            # SinusoidalPosEmb(dim),
+            nn.Linear(11, dim * 4),
+            act_fn,
+            nn.Linear(dim * 4, dim),
+        )
 
         self.returns_condition = returns_condition
         self.condition_dropout = condition_dropout
@@ -207,6 +221,7 @@ class TemporalUnet(nn.Module):
         x = einops.rearrange(x, 'b h t -> b t h')
 
         t = self.time_mlp(time)
+        c = self.cond_mlp(cond)
 
         if self.returns_condition:
             assert returns is not None
@@ -257,6 +272,7 @@ class TemporalUnet(nn.Module):
         x = einops.rearrange(x, 'b h t -> b t h')
 
         t = self.time_mlp(time)
+        c = self.cond_mlp(cond)
 
         if self.returns_condition:
             assert returns is not None
@@ -325,6 +341,7 @@ class MLPnet(nn.Module):
         self.calc_energy = calc_energy
         self.transition_dim = transition_dim
         self.action_dim = transition_dim - cond_dim
+        self.cond_dim = cond_dim
 
         if self.returns_condition:
             self.returns_mlp = nn.Sequential(
@@ -365,8 +382,8 @@ class MLPnet(nn.Module):
             if force_dropout:
                 returns_embed = 0*returns_embed
             t = torch.cat([t, returns_embed], dim=-1)
-
         inp = torch.cat([t, cond, x], dim=-1)
+        # inp = torch.cat([t.unsqueeze(1), cond.unsqueez(1), x], dim=-1)
         out  = self.mlp(inp)
 
         if self.calc_energy:
diff --git a/code/diffuser/utils/arrays.py b/code/diffuser/utils/arrays.py
index ae9d45c..e9da536 100644
--- a/code/diffuser/utils/arrays.py
+++ b/code/diffuser/utils/arrays.py
@@ -64,10 +64,11 @@ def batchify(batch, device):
 	return type(batch)(*batched_vals)
 
 def apply_dict(fn, d, *args, **kwargs):
-	return {
-		k: fn(v, *args, **kwargs)
-		for k, v in d.items()
-	}
+	# return {
+	# 	k: fn(v, *args, **kwargs)
+	# 	for k, v in d.items()
+	# }
+	return d
 
 def normalize(x):
 	"""
diff --git a/code/diffuser/utils/training.py b/code/diffuser/utils/training.py
index e8dcdb4..0b6e0bf 100644
--- a/code/diffuser/utils/training.py
+++ b/code/diffuser/utils/training.py
@@ -113,6 +113,7 @@ class Trainer(object):
             for i in range(self.gradient_accumulate_every):
                 batch = next(self.dataloader)
                 batch = batch_to_device(batch, device=self.device)
+                # breakpoint()
                 loss, infos = self.model.loss(*batch)
                 loss = loss / self.gradient_accumulate_every
                 loss.backward()
diff --git a/code/environment.yml b/code/environment.yml
index 31a3487..47a996c 100644
--- a/code/environment.yml
+++ b/code/environment.yml
@@ -8,9 +8,9 @@ dependencies:
 - patchelf
 - pip:
     - -f https://download.pytorch.org/whl/torch_stable.html
-    - numpy
+    - numpy==1.23.0
     - gym==0.18.0
-    - mujoco-py==2.0.2.13
+    - mujoco-py
     - matplotlib==3.3.4
     - torch==1.9.1+cu111
     - typed-argument-parser
diff --git a/code/scripts/train.py b/code/scripts/train.py
index db05e5a..53b8e2f 100644
--- a/code/scripts/train.py
+++ b/code/scripts/train.py
@@ -60,6 +60,7 @@ def main(**deps):
             savepath='model_config.pkl',
             horizon=Config.horizon,
             transition_dim=observation_dim,
+            # transition_dim=observation_dim + action_dim,
             cond_dim=observation_dim,
             dim_mults=Config.dim_mults,
             returns_condition=Config.returns_condition,
@@ -95,7 +96,7 @@ def main(**deps):
             Config.model,
             savepath='model_config.pkl',
             horizon=Config.horizon,
-            transition_dim=observation_dim + action_dim,
+            transition_dim=observation_dim,
             cond_dim=observation_dim,
             dim_mults=Config.dim_mults,
             returns_condition=Config.returns_condition,diff --git a/code/config/locomotion_config.py b/code/config/locomotion_config.py
index 90173b3..b79b7e6 100644
--- a/code/config/locomotion_config.py
+++ b/code/config/locomotion_config.py
@@ -6,7 +6,7 @@ class Config(ParamsProto):
     # misc
     seed = 100
     device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
-    bucket = '/home/aajay/weights/'
+    bucket = '/home/yanyangji/research/RL/final/decision-diffuser/code/output'
     dataset = 'hopper-medium-expert-v2'
 
     ## model
diff --git a/code/diffuser/datasets/buffer.py b/code/diffuser/datasets/buffer.py
index 42bc734..29edcd8 100644
--- a/code/diffuser/datasets/buffer.py
+++ b/code/diffuser/datasets/buffer.py
@@ -9,7 +9,7 @@ class ReplayBuffer:
 
     def __init__(self, max_n_episodes, max_path_length, termination_penalty):
         self._dict = {
-            'path_lengths': np.zeros(max_n_episodes, dtype=np.int),
+            'path_lengths': np.zeros(max_n_episodes, dtype=np.int32),
         }
         self._count = 0
         self.max_n_episodes = max_n_episodes
diff --git a/code/diffuser/datasets/sequence.py b/code/diffuser/datasets/sequence.py
index 31ac84f..ad977fc 100644
--- a/code/diffuser/datasets/sequence.py
+++ b/code/diffuser/datasets/sequence.py
@@ -77,6 +77,7 @@ class SequenceDataset(torch.utils.data.Dataset):
             condition on current observation for planning
         '''
         return {0: observations[0]}
+        # return observations[0]
 
     def __len__(self):
         return len(self.indices)
diff --git a/code/diffuser/models/diffusion.py b/code/diffuser/models/diffusion.py
index a1f74ea..0c8f1a1 100644
--- a/code/diffuser/models/diffusion.py
+++ b/code/diffuser/models/diffusion.py
@@ -3,6 +3,8 @@ import torch
 from torch import nn
 import torch.nn.functional as F
 import pdb
+import clip
+from config.locomotion_config import Config
 
 import diffuser.utils as utils
 from .helpers import (
@@ -295,12 +297,22 @@ class GaussianInvDynDiffusion(nn.Module):
         condition_guidance_w=0.1, ar_inv=False, train_only_inv=False):
         super().__init__()
         self.horizon = horizon
-        self.observation_dim = observation_dim
-        self.action_dim = action_dim
+        self.observation_dim = observation_dim # 11
+        self.action_dim = action_dim # 3
         self.transition_dim = observation_dim + action_dim
         self.model = model
         self.ar_inv = ar_inv
         self.train_only_inv = train_only_inv
+
+        # Text embedding
+        device = Config.device
+        self.clip_model, _ = clip.load("ViT-B/32", device=device)
+        for p in self.clip_model.parameters():
+            p.requires_grad = False
+        with torch.no_grad():
+            self.text_cond = self.clip_model.encode_text(clip.tokenize('the man is hopping').to(device))
+        self.proj_layer = nn.Linear(512, self.observation_dim)
+
         if self.ar_inv:
             self.inv_model = ARInvModel(hidden_dim=hidden_dim, observation_dim=observation_dim, action_dim=action_dim)
         else:
diff --git a/code/diffuser/models/helpers.py b/code/diffuser/models/helpers.py
index 53859d4..be91af3 100644
--- a/code/diffuser/models/helpers.py
+++ b/code/diffuser/models/helpers.py
@@ -92,7 +92,8 @@ def cosine_beta_schedule(timesteps, s=0.008, dtype=torch.float32):
 
 def apply_conditioning(x, conditions, action_dim):
     for t, val in conditions.items():
-        x[:, t, action_dim:] = val.clone()
+        x[:, t, action_dim:] = val.clone() # batch size, time step, trajectory (all observation)
+    # x[:, 0, :] = conditions.clone()
     return x
 
 #-----------------------------------------------------------------------------#
diff --git a/code/diffuser/models/temporal.py b/code/diffuser/models/temporal.py
index 63155c2..1ac05f4 100644
--- a/code/diffuser/models/temporal.py
+++ b/code/diffuser/models/temporal.py
@@ -89,10 +89,16 @@ class ResidualTemporalBlock(nn.Module):
             Rearrange('batch t -> batch t 1'),
         )
 
+        self.cond_mlp = nn.Sequential(
+            act_fn,
+            nn.Linear(128, out_channels),
+            Rearrange('batch t -> batch t 1')
+        )
+
         self.residual_conv = nn.Conv1d(inp_channels, out_channels, 1) \
             if inp_channels != out_channels else nn.Identity()
 
-    def forward(self, x, t):
+    def forward(self, x, t, cond=None):
         '''
             x : [ batch_size x inp_channels x horizon ]
             t : [ batch_size x embed_dim ]
@@ -100,6 +106,8 @@ class ResidualTemporalBlock(nn.Module):
             out : [ batch_size x out_channels x horizon ]
         '''
         out = self.blocks[0](x) + self.time_mlp(t)
+        # + self.cond_mlp(cond)
+        # breakpoint()
         out = self.blocks[1](out)
 
         return out + self.residual_conv(x)
@@ -140,6 +148,12 @@ class TemporalUnet(nn.Module):
             act_fn,
             nn.Linear(dim * 4, dim),
         )
+        self.cond_mlp = nn.Sequential(
+            # SinusoidalPosEmb(dim),
+            nn.Linear(11, dim * 4),
+            act_fn,
+            nn.Linear(dim * 4, dim),
+        )
 
         self.returns_condition = returns_condition
         self.condition_dropout = condition_dropout
@@ -207,6 +221,7 @@ class TemporalUnet(nn.Module):
         x = einops.rearrange(x, 'b h t -> b t h')
 
         t = self.time_mlp(time)
+        c = self.cond_mlp(cond)
 
         if self.returns_condition:
             assert returns is not None
@@ -257,6 +272,7 @@ class TemporalUnet(nn.Module):
         x = einops.rearrange(x, 'b h t -> b t h')
 
         t = self.time_mlp(time)
+        c = self.cond_mlp(cond)
 
         if self.returns_condition:
             assert returns is not None
@@ -325,6 +341,7 @@ class MLPnet(nn.Module):
         self.calc_energy = calc_energy
         self.transition_dim = transition_dim
         self.action_dim = transition_dim - cond_dim
+        self.cond_dim = cond_dim
 
         if self.returns_condition:
             self.returns_mlp = nn.Sequential(
@@ -365,8 +382,8 @@ class MLPnet(nn.Module):
             if force_dropout:
                 returns_embed = 0*returns_embed
             t = torch.cat([t, returns_embed], dim=-1)
-
         inp = torch.cat([t, cond, x], dim=-1)
+        # inp = torch.cat([t.unsqueeze(1), cond.unsqueez(1), x], dim=-1)
         out  = self.mlp(inp)
 
         if self.calc_energy:
diff --git a/code/diffuser/utils/arrays.py b/code/diffuser/utils/arrays.py
index ae9d45c..1161318 100644
--- a/code/diffuser/utils/arrays.py
+++ b/code/diffuser/utils/arrays.py
@@ -68,6 +68,7 @@ def apply_dict(fn, d, *args, **kwargs):
 		k: fn(v, *args, **kwargs)
 		for k, v in d.items()
 	}
+	# return d
 
 def normalize(x):
 	"""
diff --git a/code/diffuser/utils/training.py b/code/diffuser/utils/training.py
index e8dcdb4..0b6e0bf 100644
--- a/code/diffuser/utils/training.py
+++ b/code/diffuser/utils/training.py
@@ -113,6 +113,7 @@ class Trainer(object):
             for i in range(self.gradient_accumulate_every):
                 batch = next(self.dataloader)
                 batch = batch_to_device(batch, device=self.device)
+                # breakpoint()
                 loss, infos = self.model.loss(*batch)
                 loss = loss / self.gradient_accumulate_every
                 loss.backward()
diff --git a/code/environment.yml b/code/environment.yml
index 31a3487..47a996c 100644
--- a/code/environment.yml
+++ b/code/environment.yml
@@ -8,9 +8,9 @@ dependencies:
 - patchelf
 - pip:
     - -f https://download.pytorch.org/whl/torch_stable.html
-    - numpy
+    - numpy==1.23.0
     - gym==0.18.0
-    - mujoco-py==2.0.2.13
+    - mujoco-py
     - matplotlib==3.3.4
     - torch==1.9.1+cu111
     - typed-argument-parser
diff --git a/code/scripts/train.py b/code/scripts/train.py
index db05e5a..53b8e2f 100644
--- a/code/scripts/train.py
+++ b/code/scripts/train.py
@@ -60,6 +60,7 @@ def main(**deps):
             savepath='model_config.pkl',
             horizon=Config.horizon,
             transition_dim=observation_dim,
+            # transition_dim=observation_dim + action_dim,
             cond_dim=observation_dim,
             dim_mults=Config.dim_mults,
             returns_condition=Config.returns_condition,
@@ -95,7 +96,7 @@ def main(**deps):
             Config.model,
             savepath='model_config.pkl',
             horizon=Config.horizon,
-            transition_dim=observation_dim + action_dim,
+            transition_dim=observation_dim,
             cond_dim=observation_dim,
             dim_mults=Config.dim_mults,
             returns_condition=Config.returns_condition,diff --git a/code/config/locomotion_config.py b/code/config/locomotion_config.py
index 90173b3..b79b7e6 100644
--- a/code/config/locomotion_config.py
+++ b/code/config/locomotion_config.py
@@ -6,7 +6,7 @@ class Config(ParamsProto):
     # misc
     seed = 100
     device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
-    bucket = '/home/aajay/weights/'
+    bucket = '/home/yanyangji/research/RL/final/decision-diffuser/code/output'
     dataset = 'hopper-medium-expert-v2'
 
     ## model
diff --git a/code/diffuser/datasets/buffer.py b/code/diffuser/datasets/buffer.py
index 42bc734..29edcd8 100644
--- a/code/diffuser/datasets/buffer.py
+++ b/code/diffuser/datasets/buffer.py
@@ -9,7 +9,7 @@ class ReplayBuffer:
 
     def __init__(self, max_n_episodes, max_path_length, termination_penalty):
         self._dict = {
-            'path_lengths': np.zeros(max_n_episodes, dtype=np.int),
+            'path_lengths': np.zeros(max_n_episodes, dtype=np.int32),
         }
         self._count = 0
         self.max_n_episodes = max_n_episodes
diff --git a/code/diffuser/datasets/sequence.py b/code/diffuser/datasets/sequence.py
index 31ac84f..ad977fc 100644
--- a/code/diffuser/datasets/sequence.py
+++ b/code/diffuser/datasets/sequence.py
@@ -77,6 +77,7 @@ class SequenceDataset(torch.utils.data.Dataset):
             condition on current observation for planning
         '''
         return {0: observations[0]}
+        # return observations[0]
 
     def __len__(self):
         return len(self.indices)
diff --git a/code/diffuser/models/diffusion.py b/code/diffuser/models/diffusion.py
index a1f74ea..0c8f1a1 100644
--- a/code/diffuser/models/diffusion.py
+++ b/code/diffuser/models/diffusion.py
@@ -3,6 +3,8 @@ import torch
 from torch import nn
 import torch.nn.functional as F
 import pdb
+import clip
+from config.locomotion_config import Config
 
 import diffuser.utils as utils
 from .helpers import (
@@ -295,12 +297,22 @@ class GaussianInvDynDiffusion(nn.Module):
         condition_guidance_w=0.1, ar_inv=False, train_only_inv=False):
         super().__init__()
         self.horizon = horizon
-        self.observation_dim = observation_dim
-        self.action_dim = action_dim
+        self.observation_dim = observation_dim # 11
+        self.action_dim = action_dim # 3
         self.transition_dim = observation_dim + action_dim
         self.model = model
         self.ar_inv = ar_inv
         self.train_only_inv = train_only_inv
+
+        # Text embedding
+        device = Config.device
+        self.clip_model, _ = clip.load("ViT-B/32", device=device)
+        for p in self.clip_model.parameters():
+            p.requires_grad = False
+        with torch.no_grad():
+            self.text_cond = self.clip_model.encode_text(clip.tokenize('the man is hopping').to(device))
+        self.proj_layer = nn.Linear(512, self.observation_dim)
+
         if self.ar_inv:
             self.inv_model = ARInvModel(hidden_dim=hidden_dim, observation_dim=observation_dim, action_dim=action_dim)
         else:
diff --git a/code/diffuser/models/helpers.py b/code/diffuser/models/helpers.py
index 53859d4..be91af3 100644
--- a/code/diffuser/models/helpers.py
+++ b/code/diffuser/models/helpers.py
@@ -92,7 +92,8 @@ def cosine_beta_schedule(timesteps, s=0.008, dtype=torch.float32):
 
 def apply_conditioning(x, conditions, action_dim):
     for t, val in conditions.items():
-        x[:, t, action_dim:] = val.clone()
+        x[:, t, action_dim:] = val.clone() # batch size, time step, trajectory (all observation)
+    # x[:, 0, :] = conditions.clone()
     return x
 
 #-----------------------------------------------------------------------------#
diff --git a/code/diffuser/models/temporal.py b/code/diffuser/models/temporal.py
index 63155c2..523a8a6 100644
--- a/code/diffuser/models/temporal.py
+++ b/code/diffuser/models/temporal.py
@@ -89,10 +89,16 @@ class ResidualTemporalBlock(nn.Module):
             Rearrange('batch t -> batch t 1'),
         )
 
+        self.cond_mlp = nn.Sequential(
+            act_fn,
+            nn.Linear(128, out_channels),
+            Rearrange('batch t -> batch t 1')
+        )
+
         self.residual_conv = nn.Conv1d(inp_channels, out_channels, 1) \
             if inp_channels != out_channels else nn.Identity()
 
-    def forward(self, x, t):
+    def forward(self, x, t, cond=None):
         '''
             x : [ batch_size x inp_channels x horizon ]
             t : [ batch_size x embed_dim ]
@@ -100,6 +106,8 @@ class ResidualTemporalBlock(nn.Module):
             out : [ batch_size x out_channels x horizon ]
         '''
         out = self.blocks[0](x) + self.time_mlp(t)
+        # + self.cond_mlp(cond)
+        # breakpoint()
         out = self.blocks[1](out)
 
         return out + self.residual_conv(x)
@@ -140,6 +148,12 @@ class TemporalUnet(nn.Module):
             act_fn,
             nn.Linear(dim * 4, dim),
         )
+        self.cond_mlp = nn.Sequential(
+            # SinusoidalPosEmb(dim),
+            nn.Linear(11, dim * 4),
+            act_fn,
+            nn.Linear(dim * 4, dim),
+        )
 
         self.returns_condition = returns_condition
         self.condition_dropout = condition_dropout
@@ -207,6 +221,7 @@ class TemporalUnet(nn.Module):
         x = einops.rearrange(x, 'b h t -> b t h')
 
         t = self.time_mlp(time)
+        # c = self.cond_mlp(cond)
 
         if self.returns_condition:
             assert returns is not None
@@ -257,6 +272,7 @@ class TemporalUnet(nn.Module):
         x = einops.rearrange(x, 'b h t -> b t h')
 
         t = self.time_mlp(time)
+        # c = self.cond_mlp(cond)
 
         if self.returns_condition:
             assert returns is not None
@@ -325,6 +341,7 @@ class MLPnet(nn.Module):
         self.calc_energy = calc_energy
         self.transition_dim = transition_dim
         self.action_dim = transition_dim - cond_dim
+        self.cond_dim = cond_dim
 
         if self.returns_condition:
             self.returns_mlp = nn.Sequential(
@@ -365,8 +382,8 @@ class MLPnet(nn.Module):
             if force_dropout:
                 returns_embed = 0*returns_embed
             t = torch.cat([t, returns_embed], dim=-1)
-
         inp = torch.cat([t, cond, x], dim=-1)
+        # inp = torch.cat([t.unsqueeze(1), cond.unsqueez(1), x], dim=-1)
         out  = self.mlp(inp)
 
         if self.calc_energy:
diff --git a/code/diffuser/utils/arrays.py b/code/diffuser/utils/arrays.py
index ae9d45c..1161318 100644
--- a/code/diffuser/utils/arrays.py
+++ b/code/diffuser/utils/arrays.py
@@ -68,6 +68,7 @@ def apply_dict(fn, d, *args, **kwargs):
 		k: fn(v, *args, **kwargs)
 		for k, v in d.items()
 	}
+	# return d
 
 def normalize(x):
 	"""
diff --git a/code/diffuser/utils/training.py b/code/diffuser/utils/training.py
index e8dcdb4..0b6e0bf 100644
--- a/code/diffuser/utils/training.py
+++ b/code/diffuser/utils/training.py
@@ -113,6 +113,7 @@ class Trainer(object):
             for i in range(self.gradient_accumulate_every):
                 batch = next(self.dataloader)
                 batch = batch_to_device(batch, device=self.device)
+                # breakpoint()
                 loss, infos = self.model.loss(*batch)
                 loss = loss / self.gradient_accumulate_every
                 loss.backward()
diff --git a/code/environment.yml b/code/environment.yml
index 31a3487..47a996c 100644
--- a/code/environment.yml
+++ b/code/environment.yml
@@ -8,9 +8,9 @@ dependencies:
 - patchelf
 - pip:
     - -f https://download.pytorch.org/whl/torch_stable.html
-    - numpy
+    - numpy==1.23.0
     - gym==0.18.0
-    - mujoco-py==2.0.2.13
+    - mujoco-py
     - matplotlib==3.3.4
     - torch==1.9.1+cu111
     - typed-argument-parser
diff --git a/code/scripts/train.py b/code/scripts/train.py
index db05e5a..53b8e2f 100644
--- a/code/scripts/train.py
+++ b/code/scripts/train.py
@@ -60,6 +60,7 @@ def main(**deps):
             savepath='model_config.pkl',
             horizon=Config.horizon,
             transition_dim=observation_dim,
+            # transition_dim=observation_dim + action_dim,
             cond_dim=observation_dim,
             dim_mults=Config.dim_mults,
             returns_condition=Config.returns_condition,
@@ -95,7 +96,7 @@ def main(**deps):
             Config.model,
             savepath='model_config.pkl',
             horizon=Config.horizon,
-            transition_dim=observation_dim + action_dim,
+            transition_dim=observation_dim,
             cond_dim=observation_dim,
             dim_mults=Config.dim_mults,
             returns_condition=Config.returns_condition,diff --git a/code/config/locomotion_config.py b/code/config/locomotion_config.py
index 90173b3..b79b7e6 100644
--- a/code/config/locomotion_config.py
+++ b/code/config/locomotion_config.py
@@ -6,7 +6,7 @@ class Config(ParamsProto):
     # misc
     seed = 100
     device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
-    bucket = '/home/aajay/weights/'
+    bucket = '/home/yanyangji/research/RL/final/decision-diffuser/code/output'
     dataset = 'hopper-medium-expert-v2'
 
     ## model
diff --git a/code/diffuser/datasets/buffer.py b/code/diffuser/datasets/buffer.py
index 42bc734..29edcd8 100644
--- a/code/diffuser/datasets/buffer.py
+++ b/code/diffuser/datasets/buffer.py
@@ -9,7 +9,7 @@ class ReplayBuffer:
 
     def __init__(self, max_n_episodes, max_path_length, termination_penalty):
         self._dict = {
-            'path_lengths': np.zeros(max_n_episodes, dtype=np.int),
+            'path_lengths': np.zeros(max_n_episodes, dtype=np.int32),
         }
         self._count = 0
         self.max_n_episodes = max_n_episodes
diff --git a/code/diffuser/datasets/sequence.py b/code/diffuser/datasets/sequence.py
index 31ac84f..ad977fc 100644
--- a/code/diffuser/datasets/sequence.py
+++ b/code/diffuser/datasets/sequence.py
@@ -77,6 +77,7 @@ class SequenceDataset(torch.utils.data.Dataset):
             condition on current observation for planning
         '''
         return {0: observations[0]}
+        # return observations[0]
 
     def __len__(self):
         return len(self.indices)
diff --git a/code/diffuser/models/diffusion.py b/code/diffuser/models/diffusion.py
index a1f74ea..0c8f1a1 100644
--- a/code/diffuser/models/diffusion.py
+++ b/code/diffuser/models/diffusion.py
@@ -3,6 +3,8 @@ import torch
 from torch import nn
 import torch.nn.functional as F
 import pdb
+import clip
+from config.locomotion_config import Config
 
 import diffuser.utils as utils
 from .helpers import (
@@ -295,12 +297,22 @@ class GaussianInvDynDiffusion(nn.Module):
         condition_guidance_w=0.1, ar_inv=False, train_only_inv=False):
         super().__init__()
         self.horizon = horizon
-        self.observation_dim = observation_dim
-        self.action_dim = action_dim
+        self.observation_dim = observation_dim # 11
+        self.action_dim = action_dim # 3
         self.transition_dim = observation_dim + action_dim
         self.model = model
         self.ar_inv = ar_inv
         self.train_only_inv = train_only_inv
+
+        # Text embedding
+        device = Config.device
+        self.clip_model, _ = clip.load("ViT-B/32", device=device)
+        for p in self.clip_model.parameters():
+            p.requires_grad = False
+        with torch.no_grad():
+            self.text_cond = self.clip_model.encode_text(clip.tokenize('the man is hopping').to(device))
+        self.proj_layer = nn.Linear(512, self.observation_dim)
+
         if self.ar_inv:
             self.inv_model = ARInvModel(hidden_dim=hidden_dim, observation_dim=observation_dim, action_dim=action_dim)
         else:
diff --git a/code/diffuser/models/helpers.py b/code/diffuser/models/helpers.py
index 53859d4..be91af3 100644
--- a/code/diffuser/models/helpers.py
+++ b/code/diffuser/models/helpers.py
@@ -92,7 +92,8 @@ def cosine_beta_schedule(timesteps, s=0.008, dtype=torch.float32):
 
 def apply_conditioning(x, conditions, action_dim):
     for t, val in conditions.items():
-        x[:, t, action_dim:] = val.clone()
+        x[:, t, action_dim:] = val.clone() # batch size, time step, trajectory (all observation)
+    # x[:, 0, :] = conditions.clone()
     return x
 
 #-----------------------------------------------------------------------------#
diff --git a/code/diffuser/models/temporal.py b/code/diffuser/models/temporal.py
index 63155c2..4965e29 100644
--- a/code/diffuser/models/temporal.py
+++ b/code/diffuser/models/temporal.py
@@ -89,10 +89,16 @@ class ResidualTemporalBlock(nn.Module):
             Rearrange('batch t -> batch t 1'),
         )
 
+        self.cond_mlp = nn.Sequential(
+            act_fn,
+            nn.Linear(128, out_channels),
+            Rearrange('batch t -> batch t 1')
+        )
+
         self.residual_conv = nn.Conv1d(inp_channels, out_channels, 1) \
             if inp_channels != out_channels else nn.Identity()
 
-    def forward(self, x, t):
+    def forward(self, x, t, cond=None):
         '''
             x : [ batch_size x inp_channels x horizon ]
             t : [ batch_size x embed_dim ]
@@ -100,6 +106,8 @@ class ResidualTemporalBlock(nn.Module):
             out : [ batch_size x out_channels x horizon ]
         '''
         out = self.blocks[0](x) + self.time_mlp(t)
+        # + self.cond_mlp(cond)
+        # breakpoint()
         out = self.blocks[1](out)
 
         return out + self.residual_conv(x)
@@ -140,6 +148,12 @@ class TemporalUnet(nn.Module):
             act_fn,
             nn.Linear(dim * 4, dim),
         )
+        self.cond_mlp = nn.Sequential(
+            # SinusoidalPosEmb(dim),
+            nn.Linear(11, dim * 4),
+            act_fn,
+            nn.Linear(dim * 4, dim),
+        )
 
         self.returns_condition = returns_condition
         self.condition_dropout = condition_dropout
@@ -207,6 +221,8 @@ class TemporalUnet(nn.Module):
         x = einops.rearrange(x, 'b h t -> b t h')
 
         t = self.time_mlp(time)
+        c = self.cond_mlp(cond[0])
+        breakpoint()
 
         if self.returns_condition:
             assert returns is not None
@@ -257,6 +273,7 @@ class TemporalUnet(nn.Module):
         x = einops.rearrange(x, 'b h t -> b t h')
 
         t = self.time_mlp(time)
+        # c = self.cond_mlp(cond)
 
         if self.returns_condition:
             assert returns is not None
@@ -325,6 +342,7 @@ class MLPnet(nn.Module):
         self.calc_energy = calc_energy
         self.transition_dim = transition_dim
         self.action_dim = transition_dim - cond_dim
+        self.cond_dim = cond_dim
 
         if self.returns_condition:
             self.returns_mlp = nn.Sequential(
@@ -365,8 +383,8 @@ class MLPnet(nn.Module):
             if force_dropout:
                 returns_embed = 0*returns_embed
             t = torch.cat([t, returns_embed], dim=-1)
-
         inp = torch.cat([t, cond, x], dim=-1)
+        # inp = torch.cat([t.unsqueeze(1), cond.unsqueez(1), x], dim=-1)
         out  = self.mlp(inp)
 
         if self.calc_energy:
diff --git a/code/diffuser/utils/arrays.py b/code/diffuser/utils/arrays.py
index ae9d45c..1161318 100644
--- a/code/diffuser/utils/arrays.py
+++ b/code/diffuser/utils/arrays.py
@@ -68,6 +68,7 @@ def apply_dict(fn, d, *args, **kwargs):
 		k: fn(v, *args, **kwargs)
 		for k, v in d.items()
 	}
+	# return d
 
 def normalize(x):
 	"""
diff --git a/code/diffuser/utils/training.py b/code/diffuser/utils/training.py
index e8dcdb4..0b6e0bf 100644
--- a/code/diffuser/utils/training.py
+++ b/code/diffuser/utils/training.py
@@ -113,6 +113,7 @@ class Trainer(object):
             for i in range(self.gradient_accumulate_every):
                 batch = next(self.dataloader)
                 batch = batch_to_device(batch, device=self.device)
+                # breakpoint()
                 loss, infos = self.model.loss(*batch)
                 loss = loss / self.gradient_accumulate_every
                 loss.backward()
diff --git a/code/environment.yml b/code/environment.yml
index 31a3487..47a996c 100644
--- a/code/environment.yml
+++ b/code/environment.yml
@@ -8,9 +8,9 @@ dependencies:
 - patchelf
 - pip:
     - -f https://download.pytorch.org/whl/torch_stable.html
-    - numpy
+    - numpy==1.23.0
     - gym==0.18.0
-    - mujoco-py==2.0.2.13
+    - mujoco-py
     - matplotlib==3.3.4
     - torch==1.9.1+cu111
     - typed-argument-parser
diff --git a/code/scripts/train.py b/code/scripts/train.py
index db05e5a..53b8e2f 100644
--- a/code/scripts/train.py
+++ b/code/scripts/train.py
@@ -60,6 +60,7 @@ def main(**deps):
             savepath='model_config.pkl',
             horizon=Config.horizon,
             transition_dim=observation_dim,
+            # transition_dim=observation_dim + action_dim,
             cond_dim=observation_dim,
             dim_mults=Config.dim_mults,
             returns_condition=Config.returns_condition,
@@ -95,7 +96,7 @@ def main(**deps):
             Config.model,
             savepath='model_config.pkl',
             horizon=Config.horizon,
-            transition_dim=observation_dim + action_dim,
+            transition_dim=observation_dim,
             cond_dim=observation_dim,
             dim_mults=Config.dim_mults,
             returns_condition=Config.returns_condition,diff --git a/code/config/locomotion_config.py b/code/config/locomotion_config.py
index 90173b3..b79b7e6 100644
--- a/code/config/locomotion_config.py
+++ b/code/config/locomotion_config.py
@@ -6,7 +6,7 @@ class Config(ParamsProto):
     # misc
     seed = 100
     device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
-    bucket = '/home/aajay/weights/'
+    bucket = '/home/yanyangji/research/RL/final/decision-diffuser/code/output'
     dataset = 'hopper-medium-expert-v2'
 
     ## model
diff --git a/code/diffuser/datasets/buffer.py b/code/diffuser/datasets/buffer.py
index 42bc734..29edcd8 100644
--- a/code/diffuser/datasets/buffer.py
+++ b/code/diffuser/datasets/buffer.py
@@ -9,7 +9,7 @@ class ReplayBuffer:
 
     def __init__(self, max_n_episodes, max_path_length, termination_penalty):
         self._dict = {
-            'path_lengths': np.zeros(max_n_episodes, dtype=np.int),
+            'path_lengths': np.zeros(max_n_episodes, dtype=np.int32),
         }
         self._count = 0
         self.max_n_episodes = max_n_episodes
diff --git a/code/diffuser/datasets/sequence.py b/code/diffuser/datasets/sequence.py
index 31ac84f..ad977fc 100644
--- a/code/diffuser/datasets/sequence.py
+++ b/code/diffuser/datasets/sequence.py
@@ -77,6 +77,7 @@ class SequenceDataset(torch.utils.data.Dataset):
             condition on current observation for planning
         '''
         return {0: observations[0]}
+        # return observations[0]
 
     def __len__(self):
         return len(self.indices)
diff --git a/code/diffuser/models/diffusion.py b/code/diffuser/models/diffusion.py
index a1f74ea..0c8f1a1 100644
--- a/code/diffuser/models/diffusion.py
+++ b/code/diffuser/models/diffusion.py
@@ -3,6 +3,8 @@ import torch
 from torch import nn
 import torch.nn.functional as F
 import pdb
+import clip
+from config.locomotion_config import Config
 
 import diffuser.utils as utils
 from .helpers import (
@@ -295,12 +297,22 @@ class GaussianInvDynDiffusion(nn.Module):
         condition_guidance_w=0.1, ar_inv=False, train_only_inv=False):
         super().__init__()
         self.horizon = horizon
-        self.observation_dim = observation_dim
-        self.action_dim = action_dim
+        self.observation_dim = observation_dim # 11
+        self.action_dim = action_dim # 3
         self.transition_dim = observation_dim + action_dim
         self.model = model
         self.ar_inv = ar_inv
         self.train_only_inv = train_only_inv
+
+        # Text embedding
+        device = Config.device
+        self.clip_model, _ = clip.load("ViT-B/32", device=device)
+        for p in self.clip_model.parameters():
+            p.requires_grad = False
+        with torch.no_grad():
+            self.text_cond = self.clip_model.encode_text(clip.tokenize('the man is hopping').to(device))
+        self.proj_layer = nn.Linear(512, self.observation_dim)
+
         if self.ar_inv:
             self.inv_model = ARInvModel(hidden_dim=hidden_dim, observation_dim=observation_dim, action_dim=action_dim)
         else:
diff --git a/code/diffuser/models/helpers.py b/code/diffuser/models/helpers.py
index 53859d4..be91af3 100644
--- a/code/diffuser/models/helpers.py
+++ b/code/diffuser/models/helpers.py
@@ -92,7 +92,8 @@ def cosine_beta_schedule(timesteps, s=0.008, dtype=torch.float32):
 
 def apply_conditioning(x, conditions, action_dim):
     for t, val in conditions.items():
-        x[:, t, action_dim:] = val.clone()
+        x[:, t, action_dim:] = val.clone() # batch size, time step, trajectory (all observation)
+    # x[:, 0, :] = conditions.clone()
     return x
 
 #-----------------------------------------------------------------------------#
diff --git a/code/diffuser/models/temporal.py b/code/diffuser/models/temporal.py
index 63155c2..8324795 100644
--- a/code/diffuser/models/temporal.py
+++ b/code/diffuser/models/temporal.py
@@ -89,10 +89,16 @@ class ResidualTemporalBlock(nn.Module):
             Rearrange('batch t -> batch t 1'),
         )
 
+        self.cond_mlp = nn.Sequential(
+            act_fn,
+            nn.Linear(128, out_channels),
+            Rearrange('batch t -> batch t 1')
+        )
+
         self.residual_conv = nn.Conv1d(inp_channels, out_channels, 1) \
             if inp_channels != out_channels else nn.Identity()
 
-    def forward(self, x, t):
+    def forward(self, x, t, cond=None):
         '''
             x : [ batch_size x inp_channels x horizon ]
             t : [ batch_size x embed_dim ]
@@ -100,6 +106,8 @@ class ResidualTemporalBlock(nn.Module):
             out : [ batch_size x out_channels x horizon ]
         '''
         out = self.blocks[0](x) + self.time_mlp(t)
+        # + self.cond_mlp(cond)
+        # breakpoint()
         out = self.blocks[1](out)
 
         return out + self.residual_conv(x)
@@ -140,6 +148,12 @@ class TemporalUnet(nn.Module):
             act_fn,
             nn.Linear(dim * 4, dim),
         )
+        self.cond_mlp = nn.Sequential(
+            # SinusoidalPosEmb(dim),
+            nn.Linear(11, dim * 4),
+            act_fn,
+            nn.Linear(dim * 4, dim),
+        )
 
         self.returns_condition = returns_condition
         self.condition_dropout = condition_dropout
@@ -207,6 +221,8 @@ class TemporalUnet(nn.Module):
         x = einops.rearrange(x, 'b h t -> b t h')
 
         t = self.time_mlp(time)
+        c = self.cond_mlp(cond[0])
+        breakpoint()
 
         if self.returns_condition:
             assert returns is not None
@@ -217,6 +233,7 @@ class TemporalUnet(nn.Module):
             if force_dropout:
                 returns_embed = 0*returns_embed
             t = torch.cat([t, returns_embed], dim=-1)
+        breakpoint()
 
         h = []
 
@@ -257,6 +274,7 @@ class TemporalUnet(nn.Module):
         x = einops.rearrange(x, 'b h t -> b t h')
 
         t = self.time_mlp(time)
+        # c = self.cond_mlp(cond)
 
         if self.returns_condition:
             assert returns is not None
@@ -325,6 +343,7 @@ class MLPnet(nn.Module):
         self.calc_energy = calc_energy
         self.transition_dim = transition_dim
         self.action_dim = transition_dim - cond_dim
+        self.cond_dim = cond_dim
 
         if self.returns_condition:
             self.returns_mlp = nn.Sequential(
@@ -365,8 +384,8 @@ class MLPnet(nn.Module):
             if force_dropout:
                 returns_embed = 0*returns_embed
             t = torch.cat([t, returns_embed], dim=-1)
-
         inp = torch.cat([t, cond, x], dim=-1)
+        # inp = torch.cat([t.unsqueeze(1), cond.unsqueez(1), x], dim=-1)
         out  = self.mlp(inp)
 
         if self.calc_energy:
diff --git a/code/diffuser/utils/arrays.py b/code/diffuser/utils/arrays.py
index ae9d45c..1161318 100644
--- a/code/diffuser/utils/arrays.py
+++ b/code/diffuser/utils/arrays.py
@@ -68,6 +68,7 @@ def apply_dict(fn, d, *args, **kwargs):
 		k: fn(v, *args, **kwargs)
 		for k, v in d.items()
 	}
+	# return d
 
 def normalize(x):
 	"""
diff --git a/code/diffuser/utils/training.py b/code/diffuser/utils/training.py
index e8dcdb4..0b6e0bf 100644
--- a/code/diffuser/utils/training.py
+++ b/code/diffuser/utils/training.py
@@ -113,6 +113,7 @@ class Trainer(object):
             for i in range(self.gradient_accumulate_every):
                 batch = next(self.dataloader)
                 batch = batch_to_device(batch, device=self.device)
+                # breakpoint()
                 loss, infos = self.model.loss(*batch)
                 loss = loss / self.gradient_accumulate_every
                 loss.backward()
diff --git a/code/environment.yml b/code/environment.yml
index 31a3487..47a996c 100644
--- a/code/environment.yml
+++ b/code/environment.yml
@@ -8,9 +8,9 @@ dependencies:
 - patchelf
 - pip:
     - -f https://download.pytorch.org/whl/torch_stable.html
-    - numpy
+    - numpy==1.23.0
     - gym==0.18.0
-    - mujoco-py==2.0.2.13
+    - mujoco-py
     - matplotlib==3.3.4
     - torch==1.9.1+cu111
     - typed-argument-parser
diff --git a/code/scripts/train.py b/code/scripts/train.py
index db05e5a..53b8e2f 100644
--- a/code/scripts/train.py
+++ b/code/scripts/train.py
@@ -60,6 +60,7 @@ def main(**deps):
             savepath='model_config.pkl',
             horizon=Config.horizon,
             transition_dim=observation_dim,
+            # transition_dim=observation_dim + action_dim,
             cond_dim=observation_dim,
             dim_mults=Config.dim_mults,
             returns_condition=Config.returns_condition,
@@ -95,7 +96,7 @@ def main(**deps):
             Config.model,
             savepath='model_config.pkl',
             horizon=Config.horizon,
-            transition_dim=observation_dim + action_dim,
+            transition_dim=observation_dim,
             cond_dim=observation_dim,
             dim_mults=Config.dim_mults,
             returns_condition=Config.returns_condition,diff --git a/code/config/locomotion_config.py b/code/config/locomotion_config.py
index 90173b3..b79b7e6 100644
--- a/code/config/locomotion_config.py
+++ b/code/config/locomotion_config.py
@@ -6,7 +6,7 @@ class Config(ParamsProto):
     # misc
     seed = 100
     device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
-    bucket = '/home/aajay/weights/'
+    bucket = '/home/yanyangji/research/RL/final/decision-diffuser/code/output'
     dataset = 'hopper-medium-expert-v2'
 
     ## model
diff --git a/code/diffuser/datasets/buffer.py b/code/diffuser/datasets/buffer.py
index 42bc734..29edcd8 100644
--- a/code/diffuser/datasets/buffer.py
+++ b/code/diffuser/datasets/buffer.py
@@ -9,7 +9,7 @@ class ReplayBuffer:
 
     def __init__(self, max_n_episodes, max_path_length, termination_penalty):
         self._dict = {
-            'path_lengths': np.zeros(max_n_episodes, dtype=np.int),
+            'path_lengths': np.zeros(max_n_episodes, dtype=np.int32),
         }
         self._count = 0
         self.max_n_episodes = max_n_episodes
diff --git a/code/diffuser/datasets/sequence.py b/code/diffuser/datasets/sequence.py
index 31ac84f..ad977fc 100644
--- a/code/diffuser/datasets/sequence.py
+++ b/code/diffuser/datasets/sequence.py
@@ -77,6 +77,7 @@ class SequenceDataset(torch.utils.data.Dataset):
             condition on current observation for planning
         '''
         return {0: observations[0]}
+        # return observations[0]
 
     def __len__(self):
         return len(self.indices)
diff --git a/code/diffuser/models/diffusion.py b/code/diffuser/models/diffusion.py
index a1f74ea..0c8f1a1 100644
--- a/code/diffuser/models/diffusion.py
+++ b/code/diffuser/models/diffusion.py
@@ -3,6 +3,8 @@ import torch
 from torch import nn
 import torch.nn.functional as F
 import pdb
+import clip
+from config.locomotion_config import Config
 
 import diffuser.utils as utils
 from .helpers import (
@@ -295,12 +297,22 @@ class GaussianInvDynDiffusion(nn.Module):
         condition_guidance_w=0.1, ar_inv=False, train_only_inv=False):
         super().__init__()
         self.horizon = horizon
-        self.observation_dim = observation_dim
-        self.action_dim = action_dim
+        self.observation_dim = observation_dim # 11
+        self.action_dim = action_dim # 3
         self.transition_dim = observation_dim + action_dim
         self.model = model
         self.ar_inv = ar_inv
         self.train_only_inv = train_only_inv
+
+        # Text embedding
+        device = Config.device
+        self.clip_model, _ = clip.load("ViT-B/32", device=device)
+        for p in self.clip_model.parameters():
+            p.requires_grad = False
+        with torch.no_grad():
+            self.text_cond = self.clip_model.encode_text(clip.tokenize('the man is hopping').to(device))
+        self.proj_layer = nn.Linear(512, self.observation_dim)
+
         if self.ar_inv:
             self.inv_model = ARInvModel(hidden_dim=hidden_dim, observation_dim=observation_dim, action_dim=action_dim)
         else:
diff --git a/code/diffuser/models/helpers.py b/code/diffuser/models/helpers.py
index 53859d4..be91af3 100644
--- a/code/diffuser/models/helpers.py
+++ b/code/diffuser/models/helpers.py
@@ -92,7 +92,8 @@ def cosine_beta_schedule(timesteps, s=0.008, dtype=torch.float32):
 
 def apply_conditioning(x, conditions, action_dim):
     for t, val in conditions.items():
-        x[:, t, action_dim:] = val.clone()
+        x[:, t, action_dim:] = val.clone() # batch size, time step, trajectory (all observation)
+    # x[:, 0, :] = conditions.clone()
     return x
 
 #-----------------------------------------------------------------------------#
diff --git a/code/diffuser/models/temporal.py b/code/diffuser/models/temporal.py
index 63155c2..6d5fb61 100644
--- a/code/diffuser/models/temporal.py
+++ b/code/diffuser/models/temporal.py
@@ -89,17 +89,24 @@ class ResidualTemporalBlock(nn.Module):
             Rearrange('batch t -> batch t 1'),
         )
 
+        self.cond_mlp = nn.Sequential(
+            act_fn,
+            nn.Linear(128, out_channels),
+            Rearrange('batch t -> batch t 1')
+        )
+
         self.residual_conv = nn.Conv1d(inp_channels, out_channels, 1) \
             if inp_channels != out_channels else nn.Identity()
 
-    def forward(self, x, t):
+    def forward(self, x, t, cond=None):
         '''
             x : [ batch_size x inp_channels x horizon ]
             t : [ batch_size x embed_dim ]
             returns:
             out : [ batch_size x out_channels x horizon ]
         '''
-        out = self.blocks[0](x) + self.time_mlp(t)
+        out = self.blocks[0](x) + self.time_mlp(t) + self.cond_mlp(cond)
+        # breakpoint()
         out = self.blocks[1](out)
 
         return out + self.residual_conv(x)
@@ -140,6 +147,21 @@ class TemporalUnet(nn.Module):
             act_fn,
             nn.Linear(dim * 4, dim),
         )
+        # add time condition
+        if self.returns_condition:
+            self.cond_mlp = nn.Sequential(
+                # SinusoidalPosEmb(dim),
+                nn.Linear(11, dim * 4),
+                act_fn,
+                nn.Linear(dim * 4, dim*2),
+            )
+        else:
+            self.cond_mlp = nn.Sequential(
+            # SinusoidalPosEmb(dim),
+            nn.Linear(11, dim * 4),
+            act_fn,
+            nn.Linear(dim * 4, dim),
+        )
 
         self.returns_condition = returns_condition
         self.condition_dropout = condition_dropout
@@ -207,6 +229,8 @@ class TemporalUnet(nn.Module):
         x = einops.rearrange(x, 'b h t -> b t h')
 
         t = self.time_mlp(time)
+        c = self.cond_mlp(cond[0])
+        # breakpoint()
 
         if self.returns_condition:
             assert returns is not None
@@ -217,24 +241,25 @@ class TemporalUnet(nn.Module):
             if force_dropout:
                 returns_embed = 0*returns_embed
             t = torch.cat([t, returns_embed], dim=-1)
+        # breakpoint()
 
         h = []
 
         for resnet, resnet2, downsample in self.downs:
-            x = resnet(x, t)
-            x = resnet2(x, t)
+            x = resnet(x, t, c)
+            x = resnet2(x, t, c)
             h.append(x)
             x = downsample(x)
 
-        x = self.mid_block1(x, t)
-        x = self.mid_block2(x, t)
+        x = self.mid_block1(x, t, c)
+        x = self.mid_block2(x, t, c)
 
         # import pdb; pdb.set_trace()
 
         for resnet, resnet2, upsample in self.ups:
             x = torch.cat((x, h.pop()), dim=1)
-            x = resnet(x, t)
-            x = resnet2(x, t)
+            x = resnet(x, t, c)
+            x = resnet2(x, t, c)
             x = upsample(x)
 
         x = self.final_conv(x)
@@ -257,6 +282,7 @@ class TemporalUnet(nn.Module):
         x = einops.rearrange(x, 'b h t -> b t h')
 
         t = self.time_mlp(time)
+        c = self.cond_mlp(cond[0])
 
         if self.returns_condition:
             assert returns is not None
@@ -271,18 +297,18 @@ class TemporalUnet(nn.Module):
         h = []
 
         for resnet, resnet2, downsample in self.downs:
-            x = resnet(x, t)
-            x = resnet2(x, t)
+            x = resnet(x, t, c)
+            x = resnet2(x, t, c)
             h.append(x)
             x = downsample(x)
 
-        x = self.mid_block1(x, t)
-        x = self.mid_block2(x, t)
+        x = self.mid_block1(x, t, c)
+        x = self.mid_block2(x, t, c)
 
         for resnet, resnet2, upsample in self.ups:
             x = torch.cat((x, h.pop()), dim=1)
-            x = resnet(x, t)
-            x = resnet2(x, t)
+            x = resnet(x, t, c)
+            x = resnet2(x, t, c)
             x = upsample(x)
 
         x = self.final_conv(x)
@@ -325,6 +351,7 @@ class MLPnet(nn.Module):
         self.calc_energy = calc_energy
         self.transition_dim = transition_dim
         self.action_dim = transition_dim - cond_dim
+        self.cond_dim = cond_dim
 
         if self.returns_condition:
             self.returns_mlp = nn.Sequential(
@@ -365,8 +392,8 @@ class MLPnet(nn.Module):
             if force_dropout:
                 returns_embed = 0*returns_embed
             t = torch.cat([t, returns_embed], dim=-1)
-
         inp = torch.cat([t, cond, x], dim=-1)
+        # inp = torch.cat([t.unsqueeze(1), cond.unsqueez(1), x], dim=-1)
         out  = self.mlp(inp)
 
         if self.calc_energy:
diff --git a/code/diffuser/utils/arrays.py b/code/diffuser/utils/arrays.py
index ae9d45c..1161318 100644
--- a/code/diffuser/utils/arrays.py
+++ b/code/diffuser/utils/arrays.py
@@ -68,6 +68,7 @@ def apply_dict(fn, d, *args, **kwargs):
 		k: fn(v, *args, **kwargs)
 		for k, v in d.items()
 	}
+	# return d
 
 def normalize(x):
 	"""
diff --git a/code/diffuser/utils/training.py b/code/diffuser/utils/training.py
index e8dcdb4..0b6e0bf 100644
--- a/code/diffuser/utils/training.py
+++ b/code/diffuser/utils/training.py
@@ -113,6 +113,7 @@ class Trainer(object):
             for i in range(self.gradient_accumulate_every):
                 batch = next(self.dataloader)
                 batch = batch_to_device(batch, device=self.device)
+                # breakpoint()
                 loss, infos = self.model.loss(*batch)
                 loss = loss / self.gradient_accumulate_every
                 loss.backward()
diff --git a/code/environment.yml b/code/environment.yml
index 31a3487..47a996c 100644
--- a/code/environment.yml
+++ b/code/environment.yml
@@ -8,9 +8,9 @@ dependencies:
 - patchelf
 - pip:
     - -f https://download.pytorch.org/whl/torch_stable.html
-    - numpy
+    - numpy==1.23.0
     - gym==0.18.0
-    - mujoco-py==2.0.2.13
+    - mujoco-py
     - matplotlib==3.3.4
     - torch==1.9.1+cu111
     - typed-argument-parser
diff --git a/code/scripts/train.py b/code/scripts/train.py
index db05e5a..53b8e2f 100644
--- a/code/scripts/train.py
+++ b/code/scripts/train.py
@@ -60,6 +60,7 @@ def main(**deps):
             savepath='model_config.pkl',
             horizon=Config.horizon,
             transition_dim=observation_dim,
+            # transition_dim=observation_dim + action_dim,
             cond_dim=observation_dim,
             dim_mults=Config.dim_mults,
             returns_condition=Config.returns_condition,
@@ -95,7 +96,7 @@ def main(**deps):
             Config.model,
             savepath='model_config.pkl',
             horizon=Config.horizon,
-            transition_dim=observation_dim + action_dim,
+            transition_dim=observation_dim,
             cond_dim=observation_dim,
             dim_mults=Config.dim_mults,
             returns_condition=Config.returns_condition,diff --git a/code/config/locomotion_config.py b/code/config/locomotion_config.py
index 90173b3..b79b7e6 100644
--- a/code/config/locomotion_config.py
+++ b/code/config/locomotion_config.py
@@ -6,7 +6,7 @@ class Config(ParamsProto):
     # misc
     seed = 100
     device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
-    bucket = '/home/aajay/weights/'
+    bucket = '/home/yanyangji/research/RL/final/decision-diffuser/code/output'
     dataset = 'hopper-medium-expert-v2'
 
     ## model
diff --git a/code/diffuser/datasets/buffer.py b/code/diffuser/datasets/buffer.py
index 42bc734..29edcd8 100644
--- a/code/diffuser/datasets/buffer.py
+++ b/code/diffuser/datasets/buffer.py
@@ -9,7 +9,7 @@ class ReplayBuffer:
 
     def __init__(self, max_n_episodes, max_path_length, termination_penalty):
         self._dict = {
-            'path_lengths': np.zeros(max_n_episodes, dtype=np.int),
+            'path_lengths': np.zeros(max_n_episodes, dtype=np.int32),
         }
         self._count = 0
         self.max_n_episodes = max_n_episodes
diff --git a/code/diffuser/datasets/sequence.py b/code/diffuser/datasets/sequence.py
index 31ac84f..ad977fc 100644
--- a/code/diffuser/datasets/sequence.py
+++ b/code/diffuser/datasets/sequence.py
@@ -77,6 +77,7 @@ class SequenceDataset(torch.utils.data.Dataset):
             condition on current observation for planning
         '''
         return {0: observations[0]}
+        # return observations[0]
 
     def __len__(self):
         return len(self.indices)
diff --git a/code/diffuser/models/diffusion.py b/code/diffuser/models/diffusion.py
index a1f74ea..0c8f1a1 100644
--- a/code/diffuser/models/diffusion.py
+++ b/code/diffuser/models/diffusion.py
@@ -3,6 +3,8 @@ import torch
 from torch import nn
 import torch.nn.functional as F
 import pdb
+import clip
+from config.locomotion_config import Config
 
 import diffuser.utils as utils
 from .helpers import (
@@ -295,12 +297,22 @@ class GaussianInvDynDiffusion(nn.Module):
         condition_guidance_w=0.1, ar_inv=False, train_only_inv=False):
         super().__init__()
         self.horizon = horizon
-        self.observation_dim = observation_dim
-        self.action_dim = action_dim
+        self.observation_dim = observation_dim # 11
+        self.action_dim = action_dim # 3
         self.transition_dim = observation_dim + action_dim
         self.model = model
         self.ar_inv = ar_inv
         self.train_only_inv = train_only_inv
+
+        # Text embedding
+        device = Config.device
+        self.clip_model, _ = clip.load("ViT-B/32", device=device)
+        for p in self.clip_model.parameters():
+            p.requires_grad = False
+        with torch.no_grad():
+            self.text_cond = self.clip_model.encode_text(clip.tokenize('the man is hopping').to(device))
+        self.proj_layer = nn.Linear(512, self.observation_dim)
+
         if self.ar_inv:
             self.inv_model = ARInvModel(hidden_dim=hidden_dim, observation_dim=observation_dim, action_dim=action_dim)
         else:
diff --git a/code/diffuser/models/helpers.py b/code/diffuser/models/helpers.py
index 53859d4..be91af3 100644
--- a/code/diffuser/models/helpers.py
+++ b/code/diffuser/models/helpers.py
@@ -92,7 +92,8 @@ def cosine_beta_schedule(timesteps, s=0.008, dtype=torch.float32):
 
 def apply_conditioning(x, conditions, action_dim):
     for t, val in conditions.items():
-        x[:, t, action_dim:] = val.clone()
+        x[:, t, action_dim:] = val.clone() # batch size, time step, trajectory (all observation)
+    # x[:, 0, :] = conditions.clone()
     return x
 
 #-----------------------------------------------------------------------------#
diff --git a/code/diffuser/models/temporal.py b/code/diffuser/models/temporal.py
index 63155c2..37946a3 100644
--- a/code/diffuser/models/temporal.py
+++ b/code/diffuser/models/temporal.py
@@ -89,17 +89,24 @@ class ResidualTemporalBlock(nn.Module):
             Rearrange('batch t -> batch t 1'),
         )
 
+        self.cond_mlp = nn.Sequential(
+            act_fn,
+            nn.Linear(128, out_channels),
+            Rearrange('batch t -> batch t 1')
+        )
+
         self.residual_conv = nn.Conv1d(inp_channels, out_channels, 1) \
             if inp_channels != out_channels else nn.Identity()
 
-    def forward(self, x, t):
+    def forward(self, x, t, cond=None):
         '''
             x : [ batch_size x inp_channels x horizon ]
             t : [ batch_size x embed_dim ]
             returns:
             out : [ batch_size x out_channels x horizon ]
         '''
-        out = self.blocks[0](x) + self.time_mlp(t)
+        out = self.blocks[0](x) + self.time_mlp(t) + self.cond_mlp(cond)
+        # breakpoint()
         out = self.blocks[1](out)
 
         return out + self.residual_conv(x)
@@ -140,8 +147,24 @@ class TemporalUnet(nn.Module):
             act_fn,
             nn.Linear(dim * 4, dim),
         )
-
         self.returns_condition = returns_condition
+
+        # add time condition
+        if self.returns_condition:
+            self.cond_mlp = nn.Sequential(
+                # SinusoidalPosEmb(dim),
+                nn.Linear(11, dim * 4),
+                act_fn,
+                nn.Linear(dim * 4, dim*2),
+            )
+        else:
+            self.cond_mlp = nn.Sequential(
+            # SinusoidalPosEmb(dim),
+            nn.Linear(11, dim * 4),
+            act_fn,
+            nn.Linear(dim * 4, dim),
+        )
+            
         self.condition_dropout = condition_dropout
         self.calc_energy = calc_energy
 
@@ -207,6 +230,8 @@ class TemporalUnet(nn.Module):
         x = einops.rearrange(x, 'b h t -> b t h')
 
         t = self.time_mlp(time)
+        c = self.cond_mlp(cond[0])
+        # breakpoint()
 
         if self.returns_condition:
             assert returns is not None
@@ -217,24 +242,25 @@ class TemporalUnet(nn.Module):
             if force_dropout:
                 returns_embed = 0*returns_embed
             t = torch.cat([t, returns_embed], dim=-1)
+        # breakpoint()
 
         h = []
 
         for resnet, resnet2, downsample in self.downs:
-            x = resnet(x, t)
-            x = resnet2(x, t)
+            x = resnet(x, t, c)
+            x = resnet2(x, t, c)
             h.append(x)
             x = downsample(x)
 
-        x = self.mid_block1(x, t)
-        x = self.mid_block2(x, t)
+        x = self.mid_block1(x, t, c)
+        x = self.mid_block2(x, t, c)
 
         # import pdb; pdb.set_trace()
 
         for resnet, resnet2, upsample in self.ups:
             x = torch.cat((x, h.pop()), dim=1)
-            x = resnet(x, t)
-            x = resnet2(x, t)
+            x = resnet(x, t, c)
+            x = resnet2(x, t, c)
             x = upsample(x)
 
         x = self.final_conv(x)
@@ -257,6 +283,7 @@ class TemporalUnet(nn.Module):
         x = einops.rearrange(x, 'b h t -> b t h')
 
         t = self.time_mlp(time)
+        c = self.cond_mlp(cond[0])
 
         if self.returns_condition:
             assert returns is not None
@@ -271,18 +298,18 @@ class TemporalUnet(nn.Module):
         h = []
 
         for resnet, resnet2, downsample in self.downs:
-            x = resnet(x, t)
-            x = resnet2(x, t)
+            x = resnet(x, t, c)
+            x = resnet2(x, t, c)
             h.append(x)
             x = downsample(x)
 
-        x = self.mid_block1(x, t)
-        x = self.mid_block2(x, t)
+        x = self.mid_block1(x, t, c)
+        x = self.mid_block2(x, t, c)
 
         for resnet, resnet2, upsample in self.ups:
             x = torch.cat((x, h.pop()), dim=1)
-            x = resnet(x, t)
-            x = resnet2(x, t)
+            x = resnet(x, t, c)
+            x = resnet2(x, t, c)
             x = upsample(x)
 
         x = self.final_conv(x)
@@ -325,6 +352,7 @@ class MLPnet(nn.Module):
         self.calc_energy = calc_energy
         self.transition_dim = transition_dim
         self.action_dim = transition_dim - cond_dim
+        self.cond_dim = cond_dim
 
         if self.returns_condition:
             self.returns_mlp = nn.Sequential(
@@ -365,8 +393,8 @@ class MLPnet(nn.Module):
             if force_dropout:
                 returns_embed = 0*returns_embed
             t = torch.cat([t, returns_embed], dim=-1)
-
         inp = torch.cat([t, cond, x], dim=-1)
+        # inp = torch.cat([t.unsqueeze(1), cond.unsqueez(1), x], dim=-1)
         out  = self.mlp(inp)
 
         if self.calc_energy:
diff --git a/code/diffuser/utils/arrays.py b/code/diffuser/utils/arrays.py
index ae9d45c..1161318 100644
--- a/code/diffuser/utils/arrays.py
+++ b/code/diffuser/utils/arrays.py
@@ -68,6 +68,7 @@ def apply_dict(fn, d, *args, **kwargs):
 		k: fn(v, *args, **kwargs)
 		for k, v in d.items()
 	}
+	# return d
 
 def normalize(x):
 	"""
diff --git a/code/diffuser/utils/training.py b/code/diffuser/utils/training.py
index e8dcdb4..0b6e0bf 100644
--- a/code/diffuser/utils/training.py
+++ b/code/diffuser/utils/training.py
@@ -113,6 +113,7 @@ class Trainer(object):
             for i in range(self.gradient_accumulate_every):
                 batch = next(self.dataloader)
                 batch = batch_to_device(batch, device=self.device)
+                # breakpoint()
                 loss, infos = self.model.loss(*batch)
                 loss = loss / self.gradient_accumulate_every
                 loss.backward()
diff --git a/code/environment.yml b/code/environment.yml
index 31a3487..47a996c 100644
--- a/code/environment.yml
+++ b/code/environment.yml
@@ -8,9 +8,9 @@ dependencies:
 - patchelf
 - pip:
     - -f https://download.pytorch.org/whl/torch_stable.html
-    - numpy
+    - numpy==1.23.0
     - gym==0.18.0
-    - mujoco-py==2.0.2.13
+    - mujoco-py
     - matplotlib==3.3.4
     - torch==1.9.1+cu111
     - typed-argument-parser
diff --git a/code/scripts/train.py b/code/scripts/train.py
index db05e5a..53b8e2f 100644
--- a/code/scripts/train.py
+++ b/code/scripts/train.py
@@ -60,6 +60,7 @@ def main(**deps):
             savepath='model_config.pkl',
             horizon=Config.horizon,
             transition_dim=observation_dim,
+            # transition_dim=observation_dim + action_dim,
             cond_dim=observation_dim,
             dim_mults=Config.dim_mults,
             returns_condition=Config.returns_condition,
@@ -95,7 +96,7 @@ def main(**deps):
             Config.model,
             savepath='model_config.pkl',
             horizon=Config.horizon,
-            transition_dim=observation_dim + action_dim,
+            transition_dim=observation_dim,
             cond_dim=observation_dim,
             dim_mults=Config.dim_mults,
             returns_condition=Config.returns_condition,diff --git a/code/config/locomotion_config.py b/code/config/locomotion_config.py
index 90173b3..b79b7e6 100644
--- a/code/config/locomotion_config.py
+++ b/code/config/locomotion_config.py
@@ -6,7 +6,7 @@ class Config(ParamsProto):
     # misc
     seed = 100
     device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
-    bucket = '/home/aajay/weights/'
+    bucket = '/home/yanyangji/research/RL/final/decision-diffuser/code/output'
     dataset = 'hopper-medium-expert-v2'
 
     ## model
diff --git a/code/diffuser/datasets/buffer.py b/code/diffuser/datasets/buffer.py
index 42bc734..29edcd8 100644
--- a/code/diffuser/datasets/buffer.py
+++ b/code/diffuser/datasets/buffer.py
@@ -9,7 +9,7 @@ class ReplayBuffer:
 
     def __init__(self, max_n_episodes, max_path_length, termination_penalty):
         self._dict = {
-            'path_lengths': np.zeros(max_n_episodes, dtype=np.int),
+            'path_lengths': np.zeros(max_n_episodes, dtype=np.int32),
         }
         self._count = 0
         self.max_n_episodes = max_n_episodes
diff --git a/code/diffuser/datasets/sequence.py b/code/diffuser/datasets/sequence.py
index 31ac84f..ad977fc 100644
--- a/code/diffuser/datasets/sequence.py
+++ b/code/diffuser/datasets/sequence.py
@@ -77,6 +77,7 @@ class SequenceDataset(torch.utils.data.Dataset):
             condition on current observation for planning
         '''
         return {0: observations[0]}
+        # return observations[0]
 
     def __len__(self):
         return len(self.indices)
diff --git a/code/diffuser/models/diffusion.py b/code/diffuser/models/diffusion.py
index a1f74ea..0c8f1a1 100644
--- a/code/diffuser/models/diffusion.py
+++ b/code/diffuser/models/diffusion.py
@@ -3,6 +3,8 @@ import torch
 from torch import nn
 import torch.nn.functional as F
 import pdb
+import clip
+from config.locomotion_config import Config
 
 import diffuser.utils as utils
 from .helpers import (
@@ -295,12 +297,22 @@ class GaussianInvDynDiffusion(nn.Module):
         condition_guidance_w=0.1, ar_inv=False, train_only_inv=False):
         super().__init__()
         self.horizon = horizon
-        self.observation_dim = observation_dim
-        self.action_dim = action_dim
+        self.observation_dim = observation_dim # 11
+        self.action_dim = action_dim # 3
         self.transition_dim = observation_dim + action_dim
         self.model = model
         self.ar_inv = ar_inv
         self.train_only_inv = train_only_inv
+
+        # Text embedding
+        device = Config.device
+        self.clip_model, _ = clip.load("ViT-B/32", device=device)
+        for p in self.clip_model.parameters():
+            p.requires_grad = False
+        with torch.no_grad():
+            self.text_cond = self.clip_model.encode_text(clip.tokenize('the man is hopping').to(device))
+        self.proj_layer = nn.Linear(512, self.observation_dim)
+
         if self.ar_inv:
             self.inv_model = ARInvModel(hidden_dim=hidden_dim, observation_dim=observation_dim, action_dim=action_dim)
         else:
diff --git a/code/diffuser/models/helpers.py b/code/diffuser/models/helpers.py
index 53859d4..be91af3 100644
--- a/code/diffuser/models/helpers.py
+++ b/code/diffuser/models/helpers.py
@@ -92,7 +92,8 @@ def cosine_beta_schedule(timesteps, s=0.008, dtype=torch.float32):
 
 def apply_conditioning(x, conditions, action_dim):
     for t, val in conditions.items():
-        x[:, t, action_dim:] = val.clone()
+        x[:, t, action_dim:] = val.clone() # batch size, time step, trajectory (all observation)
+    # x[:, 0, :] = conditions.clone()
     return x
 
 #-----------------------------------------------------------------------------#
diff --git a/code/diffuser/models/temporal.py b/code/diffuser/models/temporal.py
index 63155c2..8101fd7 100644
--- a/code/diffuser/models/temporal.py
+++ b/code/diffuser/models/temporal.py
@@ -89,17 +89,24 @@ class ResidualTemporalBlock(nn.Module):
             Rearrange('batch t -> batch t 1'),
         )
 
+        self.cond_mlp = nn.Sequential(
+            act_fn,
+            nn.Linear(embed_dim, out_channels),
+            Rearrange('batch t -> batch t 1')
+        )
+
         self.residual_conv = nn.Conv1d(inp_channels, out_channels, 1) \
             if inp_channels != out_channels else nn.Identity()
 
-    def forward(self, x, t):
+    def forward(self, x, t, cond=None):
         '''
             x : [ batch_size x inp_channels x horizon ]
             t : [ batch_size x embed_dim ]
             returns:
             out : [ batch_size x out_channels x horizon ]
         '''
-        out = self.blocks[0](x) + self.time_mlp(t)
+        out = self.blocks[0](x) + self.time_mlp(t) + self.cond_mlp(cond)
+        # breakpoint()
         out = self.blocks[1](out)
 
         return out + self.residual_conv(x)
@@ -140,8 +147,24 @@ class TemporalUnet(nn.Module):
             act_fn,
             nn.Linear(dim * 4, dim),
         )
-
         self.returns_condition = returns_condition
+
+        # add time condition
+        if self.returns_condition:
+            self.cond_mlp = nn.Sequential(
+                # SinusoidalPosEmb(dim),
+                nn.Linear(11, dim * 4),
+                act_fn,
+                nn.Linear(dim * 4, dim*2),
+            )
+        else:
+            self.cond_mlp = nn.Sequential(
+            # SinusoidalPosEmb(dim),
+            nn.Linear(11, dim * 4),
+            act_fn,
+            nn.Linear(dim * 4, dim),
+        )
+            
         self.condition_dropout = condition_dropout
         self.calc_energy = calc_energy
 
@@ -207,6 +230,8 @@ class TemporalUnet(nn.Module):
         x = einops.rearrange(x, 'b h t -> b t h')
 
         t = self.time_mlp(time)
+        c = self.cond_mlp(cond[0])
+        # breakpoint()
 
         if self.returns_condition:
             assert returns is not None
@@ -217,24 +242,25 @@ class TemporalUnet(nn.Module):
             if force_dropout:
                 returns_embed = 0*returns_embed
             t = torch.cat([t, returns_embed], dim=-1)
+        # breakpoint()
 
         h = []
 
         for resnet, resnet2, downsample in self.downs:
-            x = resnet(x, t)
-            x = resnet2(x, t)
+            x = resnet(x, t, c)
+            x = resnet2(x, t, c)
             h.append(x)
             x = downsample(x)
 
-        x = self.mid_block1(x, t)
-        x = self.mid_block2(x, t)
+        x = self.mid_block1(x, t, c)
+        x = self.mid_block2(x, t, c)
 
         # import pdb; pdb.set_trace()
 
         for resnet, resnet2, upsample in self.ups:
             x = torch.cat((x, h.pop()), dim=1)
-            x = resnet(x, t)
-            x = resnet2(x, t)
+            x = resnet(x, t, c)
+            x = resnet2(x, t, c)
             x = upsample(x)
 
         x = self.final_conv(x)
@@ -257,6 +283,7 @@ class TemporalUnet(nn.Module):
         x = einops.rearrange(x, 'b h t -> b t h')
 
         t = self.time_mlp(time)
+        c = self.cond_mlp(cond[0])
 
         if self.returns_condition:
             assert returns is not None
@@ -271,18 +298,18 @@ class TemporalUnet(nn.Module):
         h = []
 
         for resnet, resnet2, downsample in self.downs:
-            x = resnet(x, t)
-            x = resnet2(x, t)
+            x = resnet(x, t, c)
+            x = resnet2(x, t, c)
             h.append(x)
             x = downsample(x)
 
-        x = self.mid_block1(x, t)
-        x = self.mid_block2(x, t)
+        x = self.mid_block1(x, t, c)
+        x = self.mid_block2(x, t, c)
 
         for resnet, resnet2, upsample in self.ups:
             x = torch.cat((x, h.pop()), dim=1)
-            x = resnet(x, t)
-            x = resnet2(x, t)
+            x = resnet(x, t, c)
+            x = resnet2(x, t, c)
             x = upsample(x)
 
         x = self.final_conv(x)
@@ -325,6 +352,7 @@ class MLPnet(nn.Module):
         self.calc_energy = calc_energy
         self.transition_dim = transition_dim
         self.action_dim = transition_dim - cond_dim
+        self.cond_dim = cond_dim
 
         if self.returns_condition:
             self.returns_mlp = nn.Sequential(
@@ -365,8 +393,8 @@ class MLPnet(nn.Module):
             if force_dropout:
                 returns_embed = 0*returns_embed
             t = torch.cat([t, returns_embed], dim=-1)
-
         inp = torch.cat([t, cond, x], dim=-1)
+        # inp = torch.cat([t.unsqueeze(1), cond.unsqueez(1), x], dim=-1)
         out  = self.mlp(inp)
 
         if self.calc_energy:
diff --git a/code/diffuser/utils/arrays.py b/code/diffuser/utils/arrays.py
index ae9d45c..1161318 100644
--- a/code/diffuser/utils/arrays.py
+++ b/code/diffuser/utils/arrays.py
@@ -68,6 +68,7 @@ def apply_dict(fn, d, *args, **kwargs):
 		k: fn(v, *args, **kwargs)
 		for k, v in d.items()
 	}
+	# return d
 
 def normalize(x):
 	"""
diff --git a/code/diffuser/utils/training.py b/code/diffuser/utils/training.py
index e8dcdb4..0b6e0bf 100644
--- a/code/diffuser/utils/training.py
+++ b/code/diffuser/utils/training.py
@@ -113,6 +113,7 @@ class Trainer(object):
             for i in range(self.gradient_accumulate_every):
                 batch = next(self.dataloader)
                 batch = batch_to_device(batch, device=self.device)
+                # breakpoint()
                 loss, infos = self.model.loss(*batch)
                 loss = loss / self.gradient_accumulate_every
                 loss.backward()
diff --git a/code/environment.yml b/code/environment.yml
index 31a3487..47a996c 100644
--- a/code/environment.yml
+++ b/code/environment.yml
@@ -8,9 +8,9 @@ dependencies:
 - patchelf
 - pip:
     - -f https://download.pytorch.org/whl/torch_stable.html
-    - numpy
+    - numpy==1.23.0
     - gym==0.18.0
-    - mujoco-py==2.0.2.13
+    - mujoco-py
     - matplotlib==3.3.4
     - torch==1.9.1+cu111
     - typed-argument-parser
diff --git a/code/scripts/train.py b/code/scripts/train.py
index db05e5a..53b8e2f 100644
--- a/code/scripts/train.py
+++ b/code/scripts/train.py
@@ -60,6 +60,7 @@ def main(**deps):
             savepath='model_config.pkl',
             horizon=Config.horizon,
             transition_dim=observation_dim,
+            # transition_dim=observation_dim + action_dim,
             cond_dim=observation_dim,
             dim_mults=Config.dim_mults,
             returns_condition=Config.returns_condition,
@@ -95,7 +96,7 @@ def main(**deps):
             Config.model,
             savepath='model_config.pkl',
             horizon=Config.horizon,
-            transition_dim=observation_dim + action_dim,
+            transition_dim=observation_dim,
             cond_dim=observation_dim,
             dim_mults=Config.dim_mults,
             returns_condition=Config.returns_condition,diff --git a/code/config/locomotion_config.py b/code/config/locomotion_config.py
index 90173b3..b79b7e6 100644
--- a/code/config/locomotion_config.py
+++ b/code/config/locomotion_config.py
@@ -6,7 +6,7 @@ class Config(ParamsProto):
     # misc
     seed = 100
     device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
-    bucket = '/home/aajay/weights/'
+    bucket = '/home/yanyangji/research/RL/final/decision-diffuser/code/output'
     dataset = 'hopper-medium-expert-v2'
 
     ## model
diff --git a/code/diffuser/datasets/buffer.py b/code/diffuser/datasets/buffer.py
index 42bc734..29edcd8 100644
--- a/code/diffuser/datasets/buffer.py
+++ b/code/diffuser/datasets/buffer.py
@@ -9,7 +9,7 @@ class ReplayBuffer:
 
     def __init__(self, max_n_episodes, max_path_length, termination_penalty):
         self._dict = {
-            'path_lengths': np.zeros(max_n_episodes, dtype=np.int),
+            'path_lengths': np.zeros(max_n_episodes, dtype=np.int32),
         }
         self._count = 0
         self.max_n_episodes = max_n_episodes
diff --git a/code/diffuser/datasets/sequence.py b/code/diffuser/datasets/sequence.py
index 31ac84f..ad977fc 100644
--- a/code/diffuser/datasets/sequence.py
+++ b/code/diffuser/datasets/sequence.py
@@ -77,6 +77,7 @@ class SequenceDataset(torch.utils.data.Dataset):
             condition on current observation for planning
         '''
         return {0: observations[0]}
+        # return observations[0]
 
     def __len__(self):
         return len(self.indices)
diff --git a/code/diffuser/models/diffusion.py b/code/diffuser/models/diffusion.py
index a1f74ea..43dfdbf 100644
--- a/code/diffuser/models/diffusion.py
+++ b/code/diffuser/models/diffusion.py
@@ -3,6 +3,7 @@ import torch
 from torch import nn
 import torch.nn.functional as F
 import pdb
+from config.locomotion_config import Config
 
 import diffuser.utils as utils
 from .helpers import (
@@ -295,12 +296,17 @@ class GaussianInvDynDiffusion(nn.Module):
         condition_guidance_w=0.1, ar_inv=False, train_only_inv=False):
         super().__init__()
         self.horizon = horizon
-        self.observation_dim = observation_dim
-        self.action_dim = action_dim
+        self.observation_dim = observation_dim # 11
+        self.action_dim = action_dim # 3
         self.transition_dim = observation_dim + action_dim
         self.model = model
         self.ar_inv = ar_inv
         self.train_only_inv = train_only_inv
+
+        # Text embedding
+        device = Config.device
+        self.proj_layer = nn.Linear(512, self.observation_dim)
+
         if self.ar_inv:
             self.inv_model = ARInvModel(hidden_dim=hidden_dim, observation_dim=observation_dim, action_dim=action_dim)
         else:
diff --git a/code/diffuser/models/helpers.py b/code/diffuser/models/helpers.py
index 53859d4..be91af3 100644
--- a/code/diffuser/models/helpers.py
+++ b/code/diffuser/models/helpers.py
@@ -92,7 +92,8 @@ def cosine_beta_schedule(timesteps, s=0.008, dtype=torch.float32):
 
 def apply_conditioning(x, conditions, action_dim):
     for t, val in conditions.items():
-        x[:, t, action_dim:] = val.clone()
+        x[:, t, action_dim:] = val.clone() # batch size, time step, trajectory (all observation)
+    # x[:, 0, :] = conditions.clone()
     return x
 
 #-----------------------------------------------------------------------------#
diff --git a/code/diffuser/models/temporal.py b/code/diffuser/models/temporal.py
index 63155c2..f40400f 100644
--- a/code/diffuser/models/temporal.py
+++ b/code/diffuser/models/temporal.py
@@ -5,6 +5,8 @@ from einops.layers.torch import Rearrange
 from einops import rearrange
 import pdb
 from torch.distributions import Bernoulli
+import clip
+from config.locomotion_config import Config
 
 from .helpers import (
     SinusoidalPosEmb,
@@ -89,17 +91,24 @@ class ResidualTemporalBlock(nn.Module):
             Rearrange('batch t -> batch t 1'),
         )
 
+        self.cond_mlp = nn.Sequential(
+            act_fn,
+            nn.Linear(embed_dim, out_channels),
+            Rearrange('batch t -> batch t 1')
+        )
+
         self.residual_conv = nn.Conv1d(inp_channels, out_channels, 1) \
             if inp_channels != out_channels else nn.Identity()
 
-    def forward(self, x, t):
+    def forward(self, x, t, cond=None):
         '''
             x : [ batch_size x inp_channels x horizon ]
             t : [ batch_size x embed_dim ]
             returns:
             out : [ batch_size x out_channels x horizon ]
         '''
-        out = self.blocks[0](x) + self.time_mlp(t)
+        out = self.blocks[0](x) + self.time_mlp(t) + self.cond_mlp(cond)
+        # breakpoint()
         out = self.blocks[1](out)
 
         return out + self.residual_conv(x)
@@ -145,7 +154,21 @@ class TemporalUnet(nn.Module):
         self.condition_dropout = condition_dropout
         self.calc_energy = calc_energy
 
+        # add text embedding
+        clip_model, _ = clip.load("ViT-B/32", device=Config.device)
+        with torch.no_grad():
+            self.text_feats = clip_model.encode_text(clip.tokenize(["a robot is hopping"]).to(Config.device))
+        
+        # add time condition
         if self.returns_condition:
+
+            self.cond_mlp = nn.Sequential(
+                # SinusoidalPosEmb(dim),
+                nn.Linear(512, dim * 4),
+                act_fn,
+                nn.Linear(dim * 4, dim*2),
+            )
+
             self.returns_mlp = nn.Sequential(
                         nn.Linear(1, dim),
                         act_fn,
@@ -156,8 +179,27 @@ class TemporalUnet(nn.Module):
             self.mask_dist = Bernoulli(probs=1-self.condition_dropout)
             embed_dim = 2*dim
         else:
+            self.cond_mlp = nn.Sequential(
+            # SinusoidalPosEmb(dim),
+            nn.Linear(512, dim * 4),
+            act_fn,
+            nn.Linear(dim * 4, dim),
+            )
             embed_dim = dim
 
+        # if self.returns_condition:
+        #     self.returns_mlp = nn.Sequential(
+        #                 nn.Linear(1, dim),
+        #                 act_fn,
+        #                 nn.Linear(dim, dim * 4),
+        #                 act_fn,
+        #                 nn.Linear(dim * 4, dim),
+        #             )
+        #     self.mask_dist = Bernoulli(probs=1-self.condition_dropout)
+        #     embed_dim = 2*dim
+        # else:
+        #     embed_dim = dim
+
         self.downs = nn.ModuleList([])
         self.ups = nn.ModuleList([])
         num_resolutions = len(in_out)
@@ -207,6 +249,8 @@ class TemporalUnet(nn.Module):
         x = einops.rearrange(x, 'b h t -> b t h')
 
         t = self.time_mlp(time)
+        c = self.cond_mlp(self.text_feats.expand(x.size(0), -1))
+        # breakpoint()
 
         if self.returns_condition:
             assert returns is not None
@@ -217,24 +261,25 @@ class TemporalUnet(nn.Module):
             if force_dropout:
                 returns_embed = 0*returns_embed
             t = torch.cat([t, returns_embed], dim=-1)
+        # breakpoint()
 
         h = []
 
         for resnet, resnet2, downsample in self.downs:
-            x = resnet(x, t)
-            x = resnet2(x, t)
+            x = resnet(x, t, c)
+            x = resnet2(x, t, c)
             h.append(x)
             x = downsample(x)
 
-        x = self.mid_block1(x, t)
-        x = self.mid_block2(x, t)
+        x = self.mid_block1(x, t, c)
+        x = self.mid_block2(x, t, c)
 
         # import pdb; pdb.set_trace()
 
         for resnet, resnet2, upsample in self.ups:
             x = torch.cat((x, h.pop()), dim=1)
-            x = resnet(x, t)
-            x = resnet2(x, t)
+            x = resnet(x, t, c)
+            x = resnet2(x, t, c)
             x = upsample(x)
 
         x = self.final_conv(x)
@@ -257,6 +302,7 @@ class TemporalUnet(nn.Module):
         x = einops.rearrange(x, 'b h t -> b t h')
 
         t = self.time_mlp(time)
+        c = self.cond_mlp(self.text_feats.expand(x.size(0), -1))
 
         if self.returns_condition:
             assert returns is not None
@@ -271,18 +317,18 @@ class TemporalUnet(nn.Module):
         h = []
 
         for resnet, resnet2, downsample in self.downs:
-            x = resnet(x, t)
-            x = resnet2(x, t)
+            x = resnet(x, t, c)
+            x = resnet2(x, t, c)
             h.append(x)
             x = downsample(x)
 
-        x = self.mid_block1(x, t)
-        x = self.mid_block2(x, t)
+        x = self.mid_block1(x, t, c)
+        x = self.mid_block2(x, t, c)
 
         for resnet, resnet2, upsample in self.ups:
             x = torch.cat((x, h.pop()), dim=1)
-            x = resnet(x, t)
-            x = resnet2(x, t)
+            x = resnet(x, t, c)
+            x = resnet2(x, t, c)
             x = upsample(x)
 
         x = self.final_conv(x)
@@ -325,6 +371,7 @@ class MLPnet(nn.Module):
         self.calc_energy = calc_energy
         self.transition_dim = transition_dim
         self.action_dim = transition_dim - cond_dim
+        self.cond_dim = cond_dim
 
         if self.returns_condition:
             self.returns_mlp = nn.Sequential(
@@ -365,8 +412,8 @@ class MLPnet(nn.Module):
             if force_dropout:
                 returns_embed = 0*returns_embed
             t = torch.cat([t, returns_embed], dim=-1)
-
         inp = torch.cat([t, cond, x], dim=-1)
+        # inp = torch.cat([t.unsqueeze(1), cond.unsqueez(1), x], dim=-1)
         out  = self.mlp(inp)
 
         if self.calc_energy:
diff --git a/code/diffuser/utils/arrays.py b/code/diffuser/utils/arrays.py
index ae9d45c..1161318 100644
--- a/code/diffuser/utils/arrays.py
+++ b/code/diffuser/utils/arrays.py
@@ -68,6 +68,7 @@ def apply_dict(fn, d, *args, **kwargs):
 		k: fn(v, *args, **kwargs)
 		for k, v in d.items()
 	}
+	# return d
 
 def normalize(x):
 	"""
diff --git a/code/diffuser/utils/training.py b/code/diffuser/utils/training.py
index e8dcdb4..0b6e0bf 100644
--- a/code/diffuser/utils/training.py
+++ b/code/diffuser/utils/training.py
@@ -113,6 +113,7 @@ class Trainer(object):
             for i in range(self.gradient_accumulate_every):
                 batch = next(self.dataloader)
                 batch = batch_to_device(batch, device=self.device)
+                # breakpoint()
                 loss, infos = self.model.loss(*batch)
                 loss = loss / self.gradient_accumulate_every
                 loss.backward()
diff --git a/code/environment.yml b/code/environment.yml
index 31a3487..47a996c 100644
--- a/code/environment.yml
+++ b/code/environment.yml
@@ -8,9 +8,9 @@ dependencies:
 - patchelf
 - pip:
     - -f https://download.pytorch.org/whl/torch_stable.html
-    - numpy
+    - numpy==1.23.0
     - gym==0.18.0
-    - mujoco-py==2.0.2.13
+    - mujoco-py
     - matplotlib==3.3.4
     - torch==1.9.1+cu111
     - typed-argument-parser
diff --git a/code/scripts/train.py b/code/scripts/train.py
index db05e5a..53b8e2f 100644
--- a/code/scripts/train.py
+++ b/code/scripts/train.py
@@ -60,6 +60,7 @@ def main(**deps):
             savepath='model_config.pkl',
             horizon=Config.horizon,
             transition_dim=observation_dim,
+            # transition_dim=observation_dim + action_dim,
             cond_dim=observation_dim,
             dim_mults=Config.dim_mults,
             returns_condition=Config.returns_condition,
@@ -95,7 +96,7 @@ def main(**deps):
             Config.model,
             savepath='model_config.pkl',
             horizon=Config.horizon,
-            transition_dim=observation_dim + action_dim,
+            transition_dim=observation_dim,
             cond_dim=observation_dim,
             dim_mults=Config.dim_mults,
             returns_condition=Config.returns_condition,diff --git a/code/config/locomotion_config.py b/code/config/locomotion_config.py
index 90173b3..b79b7e6 100644
--- a/code/config/locomotion_config.py
+++ b/code/config/locomotion_config.py
@@ -6,7 +6,7 @@ class Config(ParamsProto):
     # misc
     seed = 100
     device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
-    bucket = '/home/aajay/weights/'
+    bucket = '/home/yanyangji/research/RL/final/decision-diffuser/code/output'
     dataset = 'hopper-medium-expert-v2'
 
     ## model
diff --git a/code/diffuser/datasets/buffer.py b/code/diffuser/datasets/buffer.py
index 42bc734..29edcd8 100644
--- a/code/diffuser/datasets/buffer.py
+++ b/code/diffuser/datasets/buffer.py
@@ -9,7 +9,7 @@ class ReplayBuffer:
 
     def __init__(self, max_n_episodes, max_path_length, termination_penalty):
         self._dict = {
-            'path_lengths': np.zeros(max_n_episodes, dtype=np.int),
+            'path_lengths': np.zeros(max_n_episodes, dtype=np.int32),
         }
         self._count = 0
         self.max_n_episodes = max_n_episodes
diff --git a/code/diffuser/datasets/sequence.py b/code/diffuser/datasets/sequence.py
index 31ac84f..ad977fc 100644
--- a/code/diffuser/datasets/sequence.py
+++ b/code/diffuser/datasets/sequence.py
@@ -77,6 +77,7 @@ class SequenceDataset(torch.utils.data.Dataset):
             condition on current observation for planning
         '''
         return {0: observations[0]}
+        # return observations[0]
 
     def __len__(self):
         return len(self.indices)
diff --git a/code/diffuser/models/diffusion.py b/code/diffuser/models/diffusion.py
index a1f74ea..43dfdbf 100644
--- a/code/diffuser/models/diffusion.py
+++ b/code/diffuser/models/diffusion.py
@@ -3,6 +3,7 @@ import torch
 from torch import nn
 import torch.nn.functional as F
 import pdb
+from config.locomotion_config import Config
 
 import diffuser.utils as utils
 from .helpers import (
@@ -295,12 +296,17 @@ class GaussianInvDynDiffusion(nn.Module):
         condition_guidance_w=0.1, ar_inv=False, train_only_inv=False):
         super().__init__()
         self.horizon = horizon
-        self.observation_dim = observation_dim
-        self.action_dim = action_dim
+        self.observation_dim = observation_dim # 11
+        self.action_dim = action_dim # 3
         self.transition_dim = observation_dim + action_dim
         self.model = model
         self.ar_inv = ar_inv
         self.train_only_inv = train_only_inv
+
+        # Text embedding
+        device = Config.device
+        self.proj_layer = nn.Linear(512, self.observation_dim)
+
         if self.ar_inv:
             self.inv_model = ARInvModel(hidden_dim=hidden_dim, observation_dim=observation_dim, action_dim=action_dim)
         else:
diff --git a/code/diffuser/models/helpers.py b/code/diffuser/models/helpers.py
index 53859d4..be91af3 100644
--- a/code/diffuser/models/helpers.py
+++ b/code/diffuser/models/helpers.py
@@ -92,7 +92,8 @@ def cosine_beta_schedule(timesteps, s=0.008, dtype=torch.float32):
 
 def apply_conditioning(x, conditions, action_dim):
     for t, val in conditions.items():
-        x[:, t, action_dim:] = val.clone()
+        x[:, t, action_dim:] = val.clone() # batch size, time step, trajectory (all observation)
+    # x[:, 0, :] = conditions.clone()
     return x
 
 #-----------------------------------------------------------------------------#
diff --git a/code/diffuser/models/temporal.py b/code/diffuser/models/temporal.py
index 63155c2..f4563c7 100644
--- a/code/diffuser/models/temporal.py
+++ b/code/diffuser/models/temporal.py
@@ -5,6 +5,8 @@ from einops.layers.torch import Rearrange
 from einops import rearrange
 import pdb
 from torch.distributions import Bernoulli
+import clip
+from config.locomotion_config import Config
 
 from .helpers import (
     SinusoidalPosEmb,
@@ -89,17 +91,24 @@ class ResidualTemporalBlock(nn.Module):
             Rearrange('batch t -> batch t 1'),
         )
 
+        self.cond_mlp = nn.Sequential(
+            act_fn,
+            nn.Linear(embed_dim, out_channels),
+            Rearrange('batch t -> batch t 1')
+        )
+
         self.residual_conv = nn.Conv1d(inp_channels, out_channels, 1) \
             if inp_channels != out_channels else nn.Identity()
 
-    def forward(self, x, t):
+    def forward(self, x, t, cond=None):
         '''
             x : [ batch_size x inp_channels x horizon ]
             t : [ batch_size x embed_dim ]
             returns:
             out : [ batch_size x out_channels x horizon ]
         '''
-        out = self.blocks[0](x) + self.time_mlp(t)
+        out = self.blocks[0](x) + self.time_mlp(t) + self.cond_mlp(cond)
+        # breakpoint()
         out = self.blocks[1](out)
 
         return out + self.residual_conv(x)
@@ -145,7 +154,21 @@ class TemporalUnet(nn.Module):
         self.condition_dropout = condition_dropout
         self.calc_energy = calc_energy
 
+        # add text embedding
+        clip_model, _ = clip.load("ViT-B/32", device=Config.device).float()
+        with torch.no_grad():
+            self.text_feats = clip_model.encode_text(clip.tokenize(["a robot is hopping"]).to(Config.device))
+        
+        # add time condition
         if self.returns_condition:
+
+            self.cond_mlp = nn.Sequential(
+                # SinusoidalPosEmb(dim),
+                nn.Linear(512, dim * 4),
+                act_fn,
+                nn.Linear(dim * 4, dim*2),
+            )
+
             self.returns_mlp = nn.Sequential(
                         nn.Linear(1, dim),
                         act_fn,
@@ -156,8 +179,27 @@ class TemporalUnet(nn.Module):
             self.mask_dist = Bernoulli(probs=1-self.condition_dropout)
             embed_dim = 2*dim
         else:
+            self.cond_mlp = nn.Sequential(
+            # SinusoidalPosEmb(dim),
+            nn.Linear(512, dim * 4),
+            act_fn,
+            nn.Linear(dim * 4, dim),
+            )
             embed_dim = dim
 
+        # if self.returns_condition:
+        #     self.returns_mlp = nn.Sequential(
+        #                 nn.Linear(1, dim),
+        #                 act_fn,
+        #                 nn.Linear(dim, dim * 4),
+        #                 act_fn,
+        #                 nn.Linear(dim * 4, dim),
+        #             )
+        #     self.mask_dist = Bernoulli(probs=1-self.condition_dropout)
+        #     embed_dim = 2*dim
+        # else:
+        #     embed_dim = dim
+
         self.downs = nn.ModuleList([])
         self.ups = nn.ModuleList([])
         num_resolutions = len(in_out)
@@ -207,6 +249,8 @@ class TemporalUnet(nn.Module):
         x = einops.rearrange(x, 'b h t -> b t h')
 
         t = self.time_mlp(time)
+        c = self.cond_mlp(self.text_feats.expand(x.size(0), -1))
+        # breakpoint()
 
         if self.returns_condition:
             assert returns is not None
@@ -217,24 +261,25 @@ class TemporalUnet(nn.Module):
             if force_dropout:
                 returns_embed = 0*returns_embed
             t = torch.cat([t, returns_embed], dim=-1)
+        # breakpoint()
 
         h = []
 
         for resnet, resnet2, downsample in self.downs:
-            x = resnet(x, t)
-            x = resnet2(x, t)
+            x = resnet(x, t, c)
+            x = resnet2(x, t, c)
             h.append(x)
             x = downsample(x)
 
-        x = self.mid_block1(x, t)
-        x = self.mid_block2(x, t)
+        x = self.mid_block1(x, t, c)
+        x = self.mid_block2(x, t, c)
 
         # import pdb; pdb.set_trace()
 
         for resnet, resnet2, upsample in self.ups:
             x = torch.cat((x, h.pop()), dim=1)
-            x = resnet(x, t)
-            x = resnet2(x, t)
+            x = resnet(x, t, c)
+            x = resnet2(x, t, c)
             x = upsample(x)
 
         x = self.final_conv(x)
@@ -257,6 +302,7 @@ class TemporalUnet(nn.Module):
         x = einops.rearrange(x, 'b h t -> b t h')
 
         t = self.time_mlp(time)
+        c = self.cond_mlp(self.text_feats.expand(x.size(0), -1))
 
         if self.returns_condition:
             assert returns is not None
@@ -271,18 +317,18 @@ class TemporalUnet(nn.Module):
         h = []
 
         for resnet, resnet2, downsample in self.downs:
-            x = resnet(x, t)
-            x = resnet2(x, t)
+            x = resnet(x, t, c)
+            x = resnet2(x, t, c)
             h.append(x)
             x = downsample(x)
 
-        x = self.mid_block1(x, t)
-        x = self.mid_block2(x, t)
+        x = self.mid_block1(x, t, c)
+        x = self.mid_block2(x, t, c)
 
         for resnet, resnet2, upsample in self.ups:
             x = torch.cat((x, h.pop()), dim=1)
-            x = resnet(x, t)
-            x = resnet2(x, t)
+            x = resnet(x, t, c)
+            x = resnet2(x, t, c)
             x = upsample(x)
 
         x = self.final_conv(x)
@@ -325,6 +371,7 @@ class MLPnet(nn.Module):
         self.calc_energy = calc_energy
         self.transition_dim = transition_dim
         self.action_dim = transition_dim - cond_dim
+        self.cond_dim = cond_dim
 
         if self.returns_condition:
             self.returns_mlp = nn.Sequential(
@@ -365,8 +412,8 @@ class MLPnet(nn.Module):
             if force_dropout:
                 returns_embed = 0*returns_embed
             t = torch.cat([t, returns_embed], dim=-1)
-
         inp = torch.cat([t, cond, x], dim=-1)
+        # inp = torch.cat([t.unsqueeze(1), cond.unsqueez(1), x], dim=-1)
         out  = self.mlp(inp)
 
         if self.calc_energy:
diff --git a/code/diffuser/utils/arrays.py b/code/diffuser/utils/arrays.py
index ae9d45c..1161318 100644
--- a/code/diffuser/utils/arrays.py
+++ b/code/diffuser/utils/arrays.py
@@ -68,6 +68,7 @@ def apply_dict(fn, d, *args, **kwargs):
 		k: fn(v, *args, **kwargs)
 		for k, v in d.items()
 	}
+	# return d
 
 def normalize(x):
 	"""
diff --git a/code/diffuser/utils/training.py b/code/diffuser/utils/training.py
index e8dcdb4..0b6e0bf 100644
--- a/code/diffuser/utils/training.py
+++ b/code/diffuser/utils/training.py
@@ -113,6 +113,7 @@ class Trainer(object):
             for i in range(self.gradient_accumulate_every):
                 batch = next(self.dataloader)
                 batch = batch_to_device(batch, device=self.device)
+                # breakpoint()
                 loss, infos = self.model.loss(*batch)
                 loss = loss / self.gradient_accumulate_every
                 loss.backward()
diff --git a/code/environment.yml b/code/environment.yml
index 31a3487..47a996c 100644
--- a/code/environment.yml
+++ b/code/environment.yml
@@ -8,9 +8,9 @@ dependencies:
 - patchelf
 - pip:
     - -f https://download.pytorch.org/whl/torch_stable.html
-    - numpy
+    - numpy==1.23.0
     - gym==0.18.0
-    - mujoco-py==2.0.2.13
+    - mujoco-py
     - matplotlib==3.3.4
     - torch==1.9.1+cu111
     - typed-argument-parser
diff --git a/code/scripts/train.py b/code/scripts/train.py
index db05e5a..53b8e2f 100644
--- a/code/scripts/train.py
+++ b/code/scripts/train.py
@@ -60,6 +60,7 @@ def main(**deps):
             savepath='model_config.pkl',
             horizon=Config.horizon,
             transition_dim=observation_dim,
+            # transition_dim=observation_dim + action_dim,
             cond_dim=observation_dim,
             dim_mults=Config.dim_mults,
             returns_condition=Config.returns_condition,
@@ -95,7 +96,7 @@ def main(**deps):
             Config.model,
             savepath='model_config.pkl',
             horizon=Config.horizon,
-            transition_dim=observation_dim + action_dim,
+            transition_dim=observation_dim,
             cond_dim=observation_dim,
             dim_mults=Config.dim_mults,
             returns_condition=Config.returns_condition,diff --git a/code/config/locomotion_config.py b/code/config/locomotion_config.py
index 90173b3..b79b7e6 100644
--- a/code/config/locomotion_config.py
+++ b/code/config/locomotion_config.py
@@ -6,7 +6,7 @@ class Config(ParamsProto):
     # misc
     seed = 100
     device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
-    bucket = '/home/aajay/weights/'
+    bucket = '/home/yanyangji/research/RL/final/decision-diffuser/code/output'
     dataset = 'hopper-medium-expert-v2'
 
     ## model
diff --git a/code/diffuser/datasets/buffer.py b/code/diffuser/datasets/buffer.py
index 42bc734..29edcd8 100644
--- a/code/diffuser/datasets/buffer.py
+++ b/code/diffuser/datasets/buffer.py
@@ -9,7 +9,7 @@ class ReplayBuffer:
 
     def __init__(self, max_n_episodes, max_path_length, termination_penalty):
         self._dict = {
-            'path_lengths': np.zeros(max_n_episodes, dtype=np.int),
+            'path_lengths': np.zeros(max_n_episodes, dtype=np.int32),
         }
         self._count = 0
         self.max_n_episodes = max_n_episodes
diff --git a/code/diffuser/datasets/sequence.py b/code/diffuser/datasets/sequence.py
index 31ac84f..ad977fc 100644
--- a/code/diffuser/datasets/sequence.py
+++ b/code/diffuser/datasets/sequence.py
@@ -77,6 +77,7 @@ class SequenceDataset(torch.utils.data.Dataset):
             condition on current observation for planning
         '''
         return {0: observations[0]}
+        # return observations[0]
 
     def __len__(self):
         return len(self.indices)
diff --git a/code/diffuser/models/diffusion.py b/code/diffuser/models/diffusion.py
index a1f74ea..43dfdbf 100644
--- a/code/diffuser/models/diffusion.py
+++ b/code/diffuser/models/diffusion.py
@@ -3,6 +3,7 @@ import torch
 from torch import nn
 import torch.nn.functional as F
 import pdb
+from config.locomotion_config import Config
 
 import diffuser.utils as utils
 from .helpers import (
@@ -295,12 +296,17 @@ class GaussianInvDynDiffusion(nn.Module):
         condition_guidance_w=0.1, ar_inv=False, train_only_inv=False):
         super().__init__()
         self.horizon = horizon
-        self.observation_dim = observation_dim
-        self.action_dim = action_dim
+        self.observation_dim = observation_dim # 11
+        self.action_dim = action_dim # 3
         self.transition_dim = observation_dim + action_dim
         self.model = model
         self.ar_inv = ar_inv
         self.train_only_inv = train_only_inv
+
+        # Text embedding
+        device = Config.device
+        self.proj_layer = nn.Linear(512, self.observation_dim)
+
         if self.ar_inv:
             self.inv_model = ARInvModel(hidden_dim=hidden_dim, observation_dim=observation_dim, action_dim=action_dim)
         else:
diff --git a/code/diffuser/models/helpers.py b/code/diffuser/models/helpers.py
index 53859d4..be91af3 100644
--- a/code/diffuser/models/helpers.py
+++ b/code/diffuser/models/helpers.py
@@ -92,7 +92,8 @@ def cosine_beta_schedule(timesteps, s=0.008, dtype=torch.float32):
 
 def apply_conditioning(x, conditions, action_dim):
     for t, val in conditions.items():
-        x[:, t, action_dim:] = val.clone()
+        x[:, t, action_dim:] = val.clone() # batch size, time step, trajectory (all observation)
+    # x[:, 0, :] = conditions.clone()
     return x
 
 #-----------------------------------------------------------------------------#
diff --git a/code/diffuser/models/temporal.py b/code/diffuser/models/temporal.py
index 63155c2..acc12dc 100644
--- a/code/diffuser/models/temporal.py
+++ b/code/diffuser/models/temporal.py
@@ -5,6 +5,8 @@ from einops.layers.torch import Rearrange
 from einops import rearrange
 import pdb
 from torch.distributions import Bernoulli
+import clip
+from config.locomotion_config import Config
 
 from .helpers import (
     SinusoidalPosEmb,
@@ -89,17 +91,24 @@ class ResidualTemporalBlock(nn.Module):
             Rearrange('batch t -> batch t 1'),
         )
 
+        self.cond_mlp = nn.Sequential(
+            act_fn,
+            nn.Linear(embed_dim, out_channels),
+            Rearrange('batch t -> batch t 1')
+        )
+
         self.residual_conv = nn.Conv1d(inp_channels, out_channels, 1) \
             if inp_channels != out_channels else nn.Identity()
 
-    def forward(self, x, t):
+    def forward(self, x, t, cond=None):
         '''
             x : [ batch_size x inp_channels x horizon ]
             t : [ batch_size x embed_dim ]
             returns:
             out : [ batch_size x out_channels x horizon ]
         '''
-        out = self.blocks[0](x) + self.time_mlp(t)
+        out = self.blocks[0](x) + self.time_mlp(t) + self.cond_mlp(cond)
+        # breakpoint()
         out = self.blocks[1](out)
 
         return out + self.residual_conv(x)
@@ -145,7 +154,22 @@ class TemporalUnet(nn.Module):
         self.condition_dropout = condition_dropout
         self.calc_energy = calc_energy
 
+        # add text embedding
+        clip_model, _ = clip.load("ViT-B/32", device=Config.device)
+        clip_model.float()
+        with torch.no_grad():
+            self.text_feats = clip_model.encode_text(clip.tokenize("a robot is hopping").to(Config.device))
+        
+        # add time condition
         if self.returns_condition:
+
+            self.cond_mlp = nn.Sequential(
+                # SinusoidalPosEmb(dim),
+                nn.Linear(512, dim * 4),
+                act_fn,
+                nn.Linear(dim * 4, dim*2),
+            )
+
             self.returns_mlp = nn.Sequential(
                         nn.Linear(1, dim),
                         act_fn,
@@ -156,8 +180,27 @@ class TemporalUnet(nn.Module):
             self.mask_dist = Bernoulli(probs=1-self.condition_dropout)
             embed_dim = 2*dim
         else:
+            self.cond_mlp = nn.Sequential(
+            # SinusoidalPosEmb(dim),
+            nn.Linear(512, dim * 4),
+            act_fn,
+            nn.Linear(dim * 4, dim),
+            )
             embed_dim = dim
 
+        # if self.returns_condition:
+        #     self.returns_mlp = nn.Sequential(
+        #                 nn.Linear(1, dim),
+        #                 act_fn,
+        #                 nn.Linear(dim, dim * 4),
+        #                 act_fn,
+        #                 nn.Linear(dim * 4, dim),
+        #             )
+        #     self.mask_dist = Bernoulli(probs=1-self.condition_dropout)
+        #     embed_dim = 2*dim
+        # else:
+        #     embed_dim = dim
+
         self.downs = nn.ModuleList([])
         self.ups = nn.ModuleList([])
         num_resolutions = len(in_out)
@@ -207,6 +250,8 @@ class TemporalUnet(nn.Module):
         x = einops.rearrange(x, 'b h t -> b t h')
 
         t = self.time_mlp(time)
+        c = self.cond_mlp(self.text_feats.expand(x.size(0), -1))
+        # breakpoint()
 
         if self.returns_condition:
             assert returns is not None
@@ -217,24 +262,25 @@ class TemporalUnet(nn.Module):
             if force_dropout:
                 returns_embed = 0*returns_embed
             t = torch.cat([t, returns_embed], dim=-1)
+        # breakpoint()
 
         h = []
 
         for resnet, resnet2, downsample in self.downs:
-            x = resnet(x, t)
-            x = resnet2(x, t)
+            x = resnet(x, t, c)
+            x = resnet2(x, t, c)
             h.append(x)
             x = downsample(x)
 
-        x = self.mid_block1(x, t)
-        x = self.mid_block2(x, t)
+        x = self.mid_block1(x, t, c)
+        x = self.mid_block2(x, t, c)
 
         # import pdb; pdb.set_trace()
 
         for resnet, resnet2, upsample in self.ups:
             x = torch.cat((x, h.pop()), dim=1)
-            x = resnet(x, t)
-            x = resnet2(x, t)
+            x = resnet(x, t, c)
+            x = resnet2(x, t, c)
             x = upsample(x)
 
         x = self.final_conv(x)
@@ -257,6 +303,7 @@ class TemporalUnet(nn.Module):
         x = einops.rearrange(x, 'b h t -> b t h')
 
         t = self.time_mlp(time)
+        c = self.cond_mlp(self.text_feats.expand(x.size(0), -1))
 
         if self.returns_condition:
             assert returns is not None
@@ -271,18 +318,18 @@ class TemporalUnet(nn.Module):
         h = []
 
         for resnet, resnet2, downsample in self.downs:
-            x = resnet(x, t)
-            x = resnet2(x, t)
+            x = resnet(x, t, c)
+            x = resnet2(x, t, c)
             h.append(x)
             x = downsample(x)
 
-        x = self.mid_block1(x, t)
-        x = self.mid_block2(x, t)
+        x = self.mid_block1(x, t, c)
+        x = self.mid_block2(x, t, c)
 
         for resnet, resnet2, upsample in self.ups:
             x = torch.cat((x, h.pop()), dim=1)
-            x = resnet(x, t)
-            x = resnet2(x, t)
+            x = resnet(x, t, c)
+            x = resnet2(x, t, c)
             x = upsample(x)
 
         x = self.final_conv(x)
@@ -325,6 +372,7 @@ class MLPnet(nn.Module):
         self.calc_energy = calc_energy
         self.transition_dim = transition_dim
         self.action_dim = transition_dim - cond_dim
+        self.cond_dim = cond_dim
 
         if self.returns_condition:
             self.returns_mlp = nn.Sequential(
@@ -365,8 +413,8 @@ class MLPnet(nn.Module):
             if force_dropout:
                 returns_embed = 0*returns_embed
             t = torch.cat([t, returns_embed], dim=-1)
-
         inp = torch.cat([t, cond, x], dim=-1)
+        # inp = torch.cat([t.unsqueeze(1), cond.unsqueez(1), x], dim=-1)
         out  = self.mlp(inp)
 
         if self.calc_energy:
diff --git a/code/diffuser/utils/arrays.py b/code/diffuser/utils/arrays.py
index ae9d45c..1161318 100644
--- a/code/diffuser/utils/arrays.py
+++ b/code/diffuser/utils/arrays.py
@@ -68,6 +68,7 @@ def apply_dict(fn, d, *args, **kwargs):
 		k: fn(v, *args, **kwargs)
 		for k, v in d.items()
 	}
+	# return d
 
 def normalize(x):
 	"""
diff --git a/code/diffuser/utils/training.py b/code/diffuser/utils/training.py
index e8dcdb4..0b6e0bf 100644
--- a/code/diffuser/utils/training.py
+++ b/code/diffuser/utils/training.py
@@ -113,6 +113,7 @@ class Trainer(object):
             for i in range(self.gradient_accumulate_every):
                 batch = next(self.dataloader)
                 batch = batch_to_device(batch, device=self.device)
+                # breakpoint()
                 loss, infos = self.model.loss(*batch)
                 loss = loss / self.gradient_accumulate_every
                 loss.backward()
diff --git a/code/environment.yml b/code/environment.yml
index 31a3487..47a996c 100644
--- a/code/environment.yml
+++ b/code/environment.yml
@@ -8,9 +8,9 @@ dependencies:
 - patchelf
 - pip:
     - -f https://download.pytorch.org/whl/torch_stable.html
-    - numpy
+    - numpy==1.23.0
     - gym==0.18.0
-    - mujoco-py==2.0.2.13
+    - mujoco-py
     - matplotlib==3.3.4
     - torch==1.9.1+cu111
     - typed-argument-parser
diff --git a/code/scripts/train.py b/code/scripts/train.py
index db05e5a..53b8e2f 100644
--- a/code/scripts/train.py
+++ b/code/scripts/train.py
@@ -60,6 +60,7 @@ def main(**deps):
             savepath='model_config.pkl',
             horizon=Config.horizon,
             transition_dim=observation_dim,
+            # transition_dim=observation_dim + action_dim,
             cond_dim=observation_dim,
             dim_mults=Config.dim_mults,
             returns_condition=Config.returns_condition,
@@ -95,7 +96,7 @@ def main(**deps):
             Config.model,
             savepath='model_config.pkl',
             horizon=Config.horizon,
-            transition_dim=observation_dim + action_dim,
+            transition_dim=observation_dim,
             cond_dim=observation_dim,
             dim_mults=Config.dim_mults,
             returns_condition=Config.returns_condition,diff --git a/code/config/locomotion_config.py b/code/config/locomotion_config.py
index 90173b3..b79b7e6 100644
--- a/code/config/locomotion_config.py
+++ b/code/config/locomotion_config.py
@@ -6,7 +6,7 @@ class Config(ParamsProto):
     # misc
     seed = 100
     device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
-    bucket = '/home/aajay/weights/'
+    bucket = '/home/yanyangji/research/RL/final/decision-diffuser/code/output'
     dataset = 'hopper-medium-expert-v2'
 
     ## model
diff --git a/code/diffuser/datasets/buffer.py b/code/diffuser/datasets/buffer.py
index 42bc734..29edcd8 100644
--- a/code/diffuser/datasets/buffer.py
+++ b/code/diffuser/datasets/buffer.py
@@ -9,7 +9,7 @@ class ReplayBuffer:
 
     def __init__(self, max_n_episodes, max_path_length, termination_penalty):
         self._dict = {
-            'path_lengths': np.zeros(max_n_episodes, dtype=np.int),
+            'path_lengths': np.zeros(max_n_episodes, dtype=np.int32),
         }
         self._count = 0
         self.max_n_episodes = max_n_episodes
diff --git a/code/diffuser/datasets/sequence.py b/code/diffuser/datasets/sequence.py
index 31ac84f..ad977fc 100644
--- a/code/diffuser/datasets/sequence.py
+++ b/code/diffuser/datasets/sequence.py
@@ -77,6 +77,7 @@ class SequenceDataset(torch.utils.data.Dataset):
             condition on current observation for planning
         '''
         return {0: observations[0]}
+        # return observations[0]
 
     def __len__(self):
         return len(self.indices)
diff --git a/code/diffuser/models/diffusion.py b/code/diffuser/models/diffusion.py
index a1f74ea..43dfdbf 100644
--- a/code/diffuser/models/diffusion.py
+++ b/code/diffuser/models/diffusion.py
@@ -3,6 +3,7 @@ import torch
 from torch import nn
 import torch.nn.functional as F
 import pdb
+from config.locomotion_config import Config
 
 import diffuser.utils as utils
 from .helpers import (
@@ -295,12 +296,17 @@ class GaussianInvDynDiffusion(nn.Module):
         condition_guidance_w=0.1, ar_inv=False, train_only_inv=False):
         super().__init__()
         self.horizon = horizon
-        self.observation_dim = observation_dim
-        self.action_dim = action_dim
+        self.observation_dim = observation_dim # 11
+        self.action_dim = action_dim # 3
         self.transition_dim = observation_dim + action_dim
         self.model = model
         self.ar_inv = ar_inv
         self.train_only_inv = train_only_inv
+
+        # Text embedding
+        device = Config.device
+        self.proj_layer = nn.Linear(512, self.observation_dim)
+
         if self.ar_inv:
             self.inv_model = ARInvModel(hidden_dim=hidden_dim, observation_dim=observation_dim, action_dim=action_dim)
         else:
diff --git a/code/diffuser/models/helpers.py b/code/diffuser/models/helpers.py
index 53859d4..be91af3 100644
--- a/code/diffuser/models/helpers.py
+++ b/code/diffuser/models/helpers.py
@@ -92,7 +92,8 @@ def cosine_beta_schedule(timesteps, s=0.008, dtype=torch.float32):
 
 def apply_conditioning(x, conditions, action_dim):
     for t, val in conditions.items():
-        x[:, t, action_dim:] = val.clone()
+        x[:, t, action_dim:] = val.clone() # batch size, time step, trajectory (all observation)
+    # x[:, 0, :] = conditions.clone()
     return x
 
 #-----------------------------------------------------------------------------#
diff --git a/code/diffuser/models/temporal.py b/code/diffuser/models/temporal.py
index 63155c2..acc12dc 100644
--- a/code/diffuser/models/temporal.py
+++ b/code/diffuser/models/temporal.py
@@ -5,6 +5,8 @@ from einops.layers.torch import Rearrange
 from einops import rearrange
 import pdb
 from torch.distributions import Bernoulli
+import clip
+from config.locomotion_config import Config
 
 from .helpers import (
     SinusoidalPosEmb,
@@ -89,17 +91,24 @@ class ResidualTemporalBlock(nn.Module):
             Rearrange('batch t -> batch t 1'),
         )
 
+        self.cond_mlp = nn.Sequential(
+            act_fn,
+            nn.Linear(embed_dim, out_channels),
+            Rearrange('batch t -> batch t 1')
+        )
+
         self.residual_conv = nn.Conv1d(inp_channels, out_channels, 1) \
             if inp_channels != out_channels else nn.Identity()
 
-    def forward(self, x, t):
+    def forward(self, x, t, cond=None):
         '''
             x : [ batch_size x inp_channels x horizon ]
             t : [ batch_size x embed_dim ]
             returns:
             out : [ batch_size x out_channels x horizon ]
         '''
-        out = self.blocks[0](x) + self.time_mlp(t)
+        out = self.blocks[0](x) + self.time_mlp(t) + self.cond_mlp(cond)
+        # breakpoint()
         out = self.blocks[1](out)
 
         return out + self.residual_conv(x)
@@ -145,7 +154,22 @@ class TemporalUnet(nn.Module):
         self.condition_dropout = condition_dropout
         self.calc_energy = calc_energy
 
+        # add text embedding
+        clip_model, _ = clip.load("ViT-B/32", device=Config.device)
+        clip_model.float()
+        with torch.no_grad():
+            self.text_feats = clip_model.encode_text(clip.tokenize("a robot is hopping").to(Config.device))
+        
+        # add time condition
         if self.returns_condition:
+
+            self.cond_mlp = nn.Sequential(
+                # SinusoidalPosEmb(dim),
+                nn.Linear(512, dim * 4),
+                act_fn,
+                nn.Linear(dim * 4, dim*2),
+            )
+
             self.returns_mlp = nn.Sequential(
                         nn.Linear(1, dim),
                         act_fn,
@@ -156,8 +180,27 @@ class TemporalUnet(nn.Module):
             self.mask_dist = Bernoulli(probs=1-self.condition_dropout)
             embed_dim = 2*dim
         else:
+            self.cond_mlp = nn.Sequential(
+            # SinusoidalPosEmb(dim),
+            nn.Linear(512, dim * 4),
+            act_fn,
+            nn.Linear(dim * 4, dim),
+            )
             embed_dim = dim
 
+        # if self.returns_condition:
+        #     self.returns_mlp = nn.Sequential(
+        #                 nn.Linear(1, dim),
+        #                 act_fn,
+        #                 nn.Linear(dim, dim * 4),
+        #                 act_fn,
+        #                 nn.Linear(dim * 4, dim),
+        #             )
+        #     self.mask_dist = Bernoulli(probs=1-self.condition_dropout)
+        #     embed_dim = 2*dim
+        # else:
+        #     embed_dim = dim
+
         self.downs = nn.ModuleList([])
         self.ups = nn.ModuleList([])
         num_resolutions = len(in_out)
@@ -207,6 +250,8 @@ class TemporalUnet(nn.Module):
         x = einops.rearrange(x, 'b h t -> b t h')
 
         t = self.time_mlp(time)
+        c = self.cond_mlp(self.text_feats.expand(x.size(0), -1))
+        # breakpoint()
 
         if self.returns_condition:
             assert returns is not None
@@ -217,24 +262,25 @@ class TemporalUnet(nn.Module):
             if force_dropout:
                 returns_embed = 0*returns_embed
             t = torch.cat([t, returns_embed], dim=-1)
+        # breakpoint()
 
         h = []
 
         for resnet, resnet2, downsample in self.downs:
-            x = resnet(x, t)
-            x = resnet2(x, t)
+            x = resnet(x, t, c)
+            x = resnet2(x, t, c)
             h.append(x)
             x = downsample(x)
 
-        x = self.mid_block1(x, t)
-        x = self.mid_block2(x, t)
+        x = self.mid_block1(x, t, c)
+        x = self.mid_block2(x, t, c)
 
         # import pdb; pdb.set_trace()
 
         for resnet, resnet2, upsample in self.ups:
             x = torch.cat((x, h.pop()), dim=1)
-            x = resnet(x, t)
-            x = resnet2(x, t)
+            x = resnet(x, t, c)
+            x = resnet2(x, t, c)
             x = upsample(x)
 
         x = self.final_conv(x)
@@ -257,6 +303,7 @@ class TemporalUnet(nn.Module):
         x = einops.rearrange(x, 'b h t -> b t h')
 
         t = self.time_mlp(time)
+        c = self.cond_mlp(self.text_feats.expand(x.size(0), -1))
 
         if self.returns_condition:
             assert returns is not None
@@ -271,18 +318,18 @@ class TemporalUnet(nn.Module):
         h = []
 
         for resnet, resnet2, downsample in self.downs:
-            x = resnet(x, t)
-            x = resnet2(x, t)
+            x = resnet(x, t, c)
+            x = resnet2(x, t, c)
             h.append(x)
             x = downsample(x)
 
-        x = self.mid_block1(x, t)
-        x = self.mid_block2(x, t)
+        x = self.mid_block1(x, t, c)
+        x = self.mid_block2(x, t, c)
 
         for resnet, resnet2, upsample in self.ups:
             x = torch.cat((x, h.pop()), dim=1)
-            x = resnet(x, t)
-            x = resnet2(x, t)
+            x = resnet(x, t, c)
+            x = resnet2(x, t, c)
             x = upsample(x)
 
         x = self.final_conv(x)
@@ -325,6 +372,7 @@ class MLPnet(nn.Module):
         self.calc_energy = calc_energy
         self.transition_dim = transition_dim
         self.action_dim = transition_dim - cond_dim
+        self.cond_dim = cond_dim
 
         if self.returns_condition:
             self.returns_mlp = nn.Sequential(
@@ -365,8 +413,8 @@ class MLPnet(nn.Module):
             if force_dropout:
                 returns_embed = 0*returns_embed
             t = torch.cat([t, returns_embed], dim=-1)
-
         inp = torch.cat([t, cond, x], dim=-1)
+        # inp = torch.cat([t.unsqueeze(1), cond.unsqueez(1), x], dim=-1)
         out  = self.mlp(inp)
 
         if self.calc_energy:
diff --git a/code/diffuser/utils/arrays.py b/code/diffuser/utils/arrays.py
index ae9d45c..1161318 100644
--- a/code/diffuser/utils/arrays.py
+++ b/code/diffuser/utils/arrays.py
@@ -68,6 +68,7 @@ def apply_dict(fn, d, *args, **kwargs):
 		k: fn(v, *args, **kwargs)
 		for k, v in d.items()
 	}
+	# return d
 
 def normalize(x):
 	"""
diff --git a/code/diffuser/utils/training.py b/code/diffuser/utils/training.py
index e8dcdb4..0b6e0bf 100644
--- a/code/diffuser/utils/training.py
+++ b/code/diffuser/utils/training.py
@@ -113,6 +113,7 @@ class Trainer(object):
             for i in range(self.gradient_accumulate_every):
                 batch = next(self.dataloader)
                 batch = batch_to_device(batch, device=self.device)
+                # breakpoint()
                 loss, infos = self.model.loss(*batch)
                 loss = loss / self.gradient_accumulate_every
                 loss.backward()
diff --git a/code/environment.yml b/code/environment.yml
index 31a3487..47a996c 100644
--- a/code/environment.yml
+++ b/code/environment.yml
@@ -8,9 +8,9 @@ dependencies:
 - patchelf
 - pip:
     - -f https://download.pytorch.org/whl/torch_stable.html
-    - numpy
+    - numpy==1.23.0
     - gym==0.18.0
-    - mujoco-py==2.0.2.13
+    - mujoco-py
     - matplotlib==3.3.4
     - torch==1.9.1+cu111
     - typed-argument-parser
diff --git a/code/scripts/train.py b/code/scripts/train.py
index db05e5a..53b8e2f 100644
--- a/code/scripts/train.py
+++ b/code/scripts/train.py
@@ -60,6 +60,7 @@ def main(**deps):
             savepath='model_config.pkl',
             horizon=Config.horizon,
             transition_dim=observation_dim,
+            # transition_dim=observation_dim + action_dim,
             cond_dim=observation_dim,
             dim_mults=Config.dim_mults,
             returns_condition=Config.returns_condition,
@@ -95,7 +96,7 @@ def main(**deps):
             Config.model,
             savepath='model_config.pkl',
             horizon=Config.horizon,
-            transition_dim=observation_dim + action_dim,
+            transition_dim=observation_dim,
             cond_dim=observation_dim,
             dim_mults=Config.dim_mults,
             returns_condition=Config.returns_condition,diff --git a/code/config/locomotion_config.py b/code/config/locomotion_config.py
index 90173b3..b79b7e6 100644
--- a/code/config/locomotion_config.py
+++ b/code/config/locomotion_config.py
@@ -6,7 +6,7 @@ class Config(ParamsProto):
     # misc
     seed = 100
     device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
-    bucket = '/home/aajay/weights/'
+    bucket = '/home/yanyangji/research/RL/final/decision-diffuser/code/output'
     dataset = 'hopper-medium-expert-v2'
 
     ## model
diff --git a/code/diffuser/datasets/buffer.py b/code/diffuser/datasets/buffer.py
index 42bc734..29edcd8 100644
--- a/code/diffuser/datasets/buffer.py
+++ b/code/diffuser/datasets/buffer.py
@@ -9,7 +9,7 @@ class ReplayBuffer:
 
     def __init__(self, max_n_episodes, max_path_length, termination_penalty):
         self._dict = {
-            'path_lengths': np.zeros(max_n_episodes, dtype=np.int),
+            'path_lengths': np.zeros(max_n_episodes, dtype=np.int32),
         }
         self._count = 0
         self.max_n_episodes = max_n_episodes
diff --git a/code/diffuser/datasets/sequence.py b/code/diffuser/datasets/sequence.py
index 31ac84f..ad977fc 100644
--- a/code/diffuser/datasets/sequence.py
+++ b/code/diffuser/datasets/sequence.py
@@ -77,6 +77,7 @@ class SequenceDataset(torch.utils.data.Dataset):
             condition on current observation for planning
         '''
         return {0: observations[0]}
+        # return observations[0]
 
     def __len__(self):
         return len(self.indices)
diff --git a/code/diffuser/models/diffusion.py b/code/diffuser/models/diffusion.py
index a1f74ea..43dfdbf 100644
--- a/code/diffuser/models/diffusion.py
+++ b/code/diffuser/models/diffusion.py
@@ -3,6 +3,7 @@ import torch
 from torch import nn
 import torch.nn.functional as F
 import pdb
+from config.locomotion_config import Config
 
 import diffuser.utils as utils
 from .helpers import (
@@ -295,12 +296,17 @@ class GaussianInvDynDiffusion(nn.Module):
         condition_guidance_w=0.1, ar_inv=False, train_only_inv=False):
         super().__init__()
         self.horizon = horizon
-        self.observation_dim = observation_dim
-        self.action_dim = action_dim
+        self.observation_dim = observation_dim # 11
+        self.action_dim = action_dim # 3
         self.transition_dim = observation_dim + action_dim
         self.model = model
         self.ar_inv = ar_inv
         self.train_only_inv = train_only_inv
+
+        # Text embedding
+        device = Config.device
+        self.proj_layer = nn.Linear(512, self.observation_dim)
+
         if self.ar_inv:
             self.inv_model = ARInvModel(hidden_dim=hidden_dim, observation_dim=observation_dim, action_dim=action_dim)
         else:
diff --git a/code/diffuser/models/helpers.py b/code/diffuser/models/helpers.py
index 53859d4..be91af3 100644
--- a/code/diffuser/models/helpers.py
+++ b/code/diffuser/models/helpers.py
@@ -92,7 +92,8 @@ def cosine_beta_schedule(timesteps, s=0.008, dtype=torch.float32):
 
 def apply_conditioning(x, conditions, action_dim):
     for t, val in conditions.items():
-        x[:, t, action_dim:] = val.clone()
+        x[:, t, action_dim:] = val.clone() # batch size, time step, trajectory (all observation)
+    # x[:, 0, :] = conditions.clone()
     return x
 
 #-----------------------------------------------------------------------------#
diff --git a/code/diffuser/models/temporal.py b/code/diffuser/models/temporal.py
index 63155c2..acc12dc 100644
--- a/code/diffuser/models/temporal.py
+++ b/code/diffuser/models/temporal.py
@@ -5,6 +5,8 @@ from einops.layers.torch import Rearrange
 from einops import rearrange
 import pdb
 from torch.distributions import Bernoulli
+import clip
+from config.locomotion_config import Config
 
 from .helpers import (
     SinusoidalPosEmb,
@@ -89,17 +91,24 @@ class ResidualTemporalBlock(nn.Module):
             Rearrange('batch t -> batch t 1'),
         )
 
+        self.cond_mlp = nn.Sequential(
+            act_fn,
+            nn.Linear(embed_dim, out_channels),
+            Rearrange('batch t -> batch t 1')
+        )
+
         self.residual_conv = nn.Conv1d(inp_channels, out_channels, 1) \
             if inp_channels != out_channels else nn.Identity()
 
-    def forward(self, x, t):
+    def forward(self, x, t, cond=None):
         '''
             x : [ batch_size x inp_channels x horizon ]
             t : [ batch_size x embed_dim ]
             returns:
             out : [ batch_size x out_channels x horizon ]
         '''
-        out = self.blocks[0](x) + self.time_mlp(t)
+        out = self.blocks[0](x) + self.time_mlp(t) + self.cond_mlp(cond)
+        # breakpoint()
         out = self.blocks[1](out)
 
         return out + self.residual_conv(x)
@@ -145,7 +154,22 @@ class TemporalUnet(nn.Module):
         self.condition_dropout = condition_dropout
         self.calc_energy = calc_energy
 
+        # add text embedding
+        clip_model, _ = clip.load("ViT-B/32", device=Config.device)
+        clip_model.float()
+        with torch.no_grad():
+            self.text_feats = clip_model.encode_text(clip.tokenize("a robot is hopping").to(Config.device))
+        
+        # add time condition
         if self.returns_condition:
+
+            self.cond_mlp = nn.Sequential(
+                # SinusoidalPosEmb(dim),
+                nn.Linear(512, dim * 4),
+                act_fn,
+                nn.Linear(dim * 4, dim*2),
+            )
+
             self.returns_mlp = nn.Sequential(
                         nn.Linear(1, dim),
                         act_fn,
@@ -156,8 +180,27 @@ class TemporalUnet(nn.Module):
             self.mask_dist = Bernoulli(probs=1-self.condition_dropout)
             embed_dim = 2*dim
         else:
+            self.cond_mlp = nn.Sequential(
+            # SinusoidalPosEmb(dim),
+            nn.Linear(512, dim * 4),
+            act_fn,
+            nn.Linear(dim * 4, dim),
+            )
             embed_dim = dim
 
+        # if self.returns_condition:
+        #     self.returns_mlp = nn.Sequential(
+        #                 nn.Linear(1, dim),
+        #                 act_fn,
+        #                 nn.Linear(dim, dim * 4),
+        #                 act_fn,
+        #                 nn.Linear(dim * 4, dim),
+        #             )
+        #     self.mask_dist = Bernoulli(probs=1-self.condition_dropout)
+        #     embed_dim = 2*dim
+        # else:
+        #     embed_dim = dim
+
         self.downs = nn.ModuleList([])
         self.ups = nn.ModuleList([])
         num_resolutions = len(in_out)
@@ -207,6 +250,8 @@ class TemporalUnet(nn.Module):
         x = einops.rearrange(x, 'b h t -> b t h')
 
         t = self.time_mlp(time)
+        c = self.cond_mlp(self.text_feats.expand(x.size(0), -1))
+        # breakpoint()
 
         if self.returns_condition:
             assert returns is not None
@@ -217,24 +262,25 @@ class TemporalUnet(nn.Module):
             if force_dropout:
                 returns_embed = 0*returns_embed
             t = torch.cat([t, returns_embed], dim=-1)
+        # breakpoint()
 
         h = []
 
         for resnet, resnet2, downsample in self.downs:
-            x = resnet(x, t)
-            x = resnet2(x, t)
+            x = resnet(x, t, c)
+            x = resnet2(x, t, c)
             h.append(x)
             x = downsample(x)
 
-        x = self.mid_block1(x, t)
-        x = self.mid_block2(x, t)
+        x = self.mid_block1(x, t, c)
+        x = self.mid_block2(x, t, c)
 
         # import pdb; pdb.set_trace()
 
         for resnet, resnet2, upsample in self.ups:
             x = torch.cat((x, h.pop()), dim=1)
-            x = resnet(x, t)
-            x = resnet2(x, t)
+            x = resnet(x, t, c)
+            x = resnet2(x, t, c)
             x = upsample(x)
 
         x = self.final_conv(x)
@@ -257,6 +303,7 @@ class TemporalUnet(nn.Module):
         x = einops.rearrange(x, 'b h t -> b t h')
 
         t = self.time_mlp(time)
+        c = self.cond_mlp(self.text_feats.expand(x.size(0), -1))
 
         if self.returns_condition:
             assert returns is not None
@@ -271,18 +318,18 @@ class TemporalUnet(nn.Module):
         h = []
 
         for resnet, resnet2, downsample in self.downs:
-            x = resnet(x, t)
-            x = resnet2(x, t)
+            x = resnet(x, t, c)
+            x = resnet2(x, t, c)
             h.append(x)
             x = downsample(x)
 
-        x = self.mid_block1(x, t)
-        x = self.mid_block2(x, t)
+        x = self.mid_block1(x, t, c)
+        x = self.mid_block2(x, t, c)
 
         for resnet, resnet2, upsample in self.ups:
             x = torch.cat((x, h.pop()), dim=1)
-            x = resnet(x, t)
-            x = resnet2(x, t)
+            x = resnet(x, t, c)
+            x = resnet2(x, t, c)
             x = upsample(x)
 
         x = self.final_conv(x)
@@ -325,6 +372,7 @@ class MLPnet(nn.Module):
         self.calc_energy = calc_energy
         self.transition_dim = transition_dim
         self.action_dim = transition_dim - cond_dim
+        self.cond_dim = cond_dim
 
         if self.returns_condition:
             self.returns_mlp = nn.Sequential(
@@ -365,8 +413,8 @@ class MLPnet(nn.Module):
             if force_dropout:
                 returns_embed = 0*returns_embed
             t = torch.cat([t, returns_embed], dim=-1)
-
         inp = torch.cat([t, cond, x], dim=-1)
+        # inp = torch.cat([t.unsqueeze(1), cond.unsqueez(1), x], dim=-1)
         out  = self.mlp(inp)
 
         if self.calc_energy:
diff --git a/code/diffuser/utils/arrays.py b/code/diffuser/utils/arrays.py
index ae9d45c..1161318 100644
--- a/code/diffuser/utils/arrays.py
+++ b/code/diffuser/utils/arrays.py
@@ -68,6 +68,7 @@ def apply_dict(fn, d, *args, **kwargs):
 		k: fn(v, *args, **kwargs)
 		for k, v in d.items()
 	}
+	# return d
 
 def normalize(x):
 	"""
diff --git a/code/diffuser/utils/training.py b/code/diffuser/utils/training.py
index e8dcdb4..0b6e0bf 100644
--- a/code/diffuser/utils/training.py
+++ b/code/diffuser/utils/training.py
@@ -113,6 +113,7 @@ class Trainer(object):
             for i in range(self.gradient_accumulate_every):
                 batch = next(self.dataloader)
                 batch = batch_to_device(batch, device=self.device)
+                # breakpoint()
                 loss, infos = self.model.loss(*batch)
                 loss = loss / self.gradient_accumulate_every
                 loss.backward()
diff --git a/code/environment.yml b/code/environment.yml
index 31a3487..47a996c 100644
--- a/code/environment.yml
+++ b/code/environment.yml
@@ -8,9 +8,9 @@ dependencies:
 - patchelf
 - pip:
     - -f https://download.pytorch.org/whl/torch_stable.html
-    - numpy
+    - numpy==1.23.0
     - gym==0.18.0
-    - mujoco-py==2.0.2.13
+    - mujoco-py
     - matplotlib==3.3.4
     - torch==1.9.1+cu111
     - typed-argument-parser
diff --git a/code/scripts/train.py b/code/scripts/train.py
index db05e5a..53b8e2f 100644
--- a/code/scripts/train.py
+++ b/code/scripts/train.py
@@ -60,6 +60,7 @@ def main(**deps):
             savepath='model_config.pkl',
             horizon=Config.horizon,
             transition_dim=observation_dim,
+            # transition_dim=observation_dim + action_dim,
             cond_dim=observation_dim,
             dim_mults=Config.dim_mults,
             returns_condition=Config.returns_condition,
@@ -95,7 +96,7 @@ def main(**deps):
             Config.model,
             savepath='model_config.pkl',
             horizon=Config.horizon,
-            transition_dim=observation_dim + action_dim,
+            transition_dim=observation_dim,
             cond_dim=observation_dim,
             dim_mults=Config.dim_mults,
             returns_condition=Config.returns_condition,